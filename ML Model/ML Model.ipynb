{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Importing Libraries"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Read and Display data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "   Unnamed: 0                                               Text  \\\n0           2  aspnet site maps has anyone got experience cre...   \n1           4  adding scripting functionality to net applicat...   \n2           5  should i use nested classes in this case i am ...   \n3           6  homegrown consumption of web services i have b...   \n4           8  automatically update version number i would li...   \n\n                 Tags  \n0  ['sql', 'asp.net']  \n1      ['c#', '.net']  \n2             ['c++']  \n3            ['.net']  \n4              ['c#']  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>Text</th>\n      <th>Tags</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2</td>\n      <td>aspnet site maps has anyone got experience cre...</td>\n      <td>['sql', 'asp.net']</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>adding scripting functionality to net applicat...</td>\n      <td>['c#', '.net']</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>should i use nested classes in this case i am ...</td>\n      <td>['c++']</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>homegrown consumption of web services i have b...</td>\n      <td>['.net']</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>8</td>\n      <td>automatically update version number i would li...</td>\n      <td>['c#']</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"stackoverflow.csv\")\n",
    "train_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "['java']                            5894\n['c#']                              4409\n['android']                         4270\n['python']                          4176\n['c++']                             3689\n                                    ... \n['c#', '.net', 'mysql']                1\n['c#', 'java', 'python', 'c++']        1\n['php', 'jquery', 'html', 'css']       1\n['php', 'mysql', 'c']                  1\n['jquery', 'ios', 'iphone']            1\nName: Tags, Length: 438, dtype: int64"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['Tags'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "train_df['Tags'] = train_df['Tags'].apply(lambda x: ast.literal_eval(x))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:\n",
      "what is the use of an ioc framework in an mvc application i am trying to understand the use of an ioc framework like structuremap but i cannot help thinking that these design patterns are just nonsense making code just more complexlet me start with an example where i think an ioc is somewhat usefulli think an ioc can be usefull when dealing with the instantiation of controller classes in an mvc framework in this case i am thinking about the net mvc frameworknormally the instantiation of the controller class is handled by the framework so that means you cannot really pass any parameters to the constructor of your controller classthis is where an ioc framework can come in handy somewhere in an ioc container you specify what class should be instantiated and passed to your controllers constructor when the controller class is invokedthis is also handy when you want to unit test the controller because you can mock the object that is passed to itbut like i said i can somewhat understand why people want to use it for their controller classes but not outside of that from there on you can simply do normal dependency injectionbut why not simply do it like thispublic class somecontroller public somecontroller this new someobj publiv somecontrollersomeobj obj thisobj obj now you do not have to use any 3rd party ioc framework which also means a lower learning curve since you do not have to go into the specs of that framework aswellyou can still mock the object in your unit tests so no problem there eitherthe only thing you can say is but now your class is tightly coupled to someobjthis is true but who cares it is a controller class i am not going to reuse that class ever so why on earth should i worry about that tight coupling i can mock the object that is passed to it that is the only important thingso why should i bother using an ioc am i really missing the point to me the ioc pattern is just some overrated pattern adding more complex layers to your application\n",
      "\n",
      "Labels: ['c#', '.net']\n",
      "---\n",
      "\n",
      "Text:\n",
      "knockoutjs setting an empty selection i am trying to use knockoutjs to populate and manage a select box i would like the initial value to be emptyhowever i am having trouble trying to force the managed value to be null at any time let alone initiallyfor example consider this fiddlehtml select databindoptions myoptions value myvalueselect akaspan databindtext myvaluespandiv button databindclick setmyvaluenullclear the selectionbutton button databindclick setmyvalueoneselect onebutton button databindclick setmyvaluefourselect fourbuttondivul databindforeach log limessage span databindtext messagespanliuljsfunction model var self this selfmyoptions one two three four selfmyvalue koobservable selfsetmyvalue function val return function thislogpush message ok trying to set value as val selfmyvalueval selflog koobservablearrayvar model new modelkoapplybindingsmodelthe select one and select four buttons work to change the selection by forcing an update of myvalue however the clear the selection button does not work the selection is not cleared by myvaluenull which is what i thought was the proper way to do itwhat am i doing wrong\n",
      "\n",
      "Labels: ['javascript']\n",
      "---\n",
      "\n",
      "Text:\n",
      "how java implement the access to the enclosing class from an inner inner class i have created an inner class in an inner class public class enclosingclass public class innerclass private enclosingclass getenclosing return enclosingclassthis public class innerinnerclass private innerclass getenclosing return innerclassthis private enclosingclass getenclosingofenclosing return enclosingclassthis i have been surprised that java allows the innerinnerclass to access directly the enclosingclass how is this code implemented internally by javathe innerinnerclass keeps two pointers one on the innerclass and the other on the enclosingclass or the innerinnerclass access the enclosingclass through the innerclass\n",
      "\n",
      "Labels: ['java']\n",
      "---\n",
      "\n",
      "Text:\n",
      "why am i getting typeerror objaddeventlistener is not a function heres my codefunction addevent obj type fn if objattachevent objetypefn fn objtypefn functionobjetypefn windowevent objattachevent ontype objtypefn else objaddeventlistenertype fn falsefunction alertwinner alertyou may be a winnerfunction showwinner var atag documentgetelementsbytagnamea addeventatag click alertwinnershowwinnerbasically i am working in the firebug console and trying to get an alert to pop up when any a tag is clickedi cannot see the problem that results in this not working and giving me the error stated in my questions title viewed in firebug anybody\n",
      "\n",
      "Labels: ['javascript']\n",
      "---\n",
      "\n",
      "Text:\n",
      "threadpoolexecutor with unbounded queue not creating new threads my threadpoolexecutor is failing to create new threads in fact i wrote a somewhat hacky linkedblockingqueue that will accept any task ie it is unbounded but call an additional handler which in my application spews warning trace that the pool is behind which gives me very explicit information that the tpe is refusing to create new threads even though the queue has thousands of entries in it my constructor is as follows private final executorservice s3uploadpool new threadpoolexecutor1 40 1 timeunithours unboundedloggingqueuewhy is it not creating new threads\n",
      "\n",
      "Labels: ['java']\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's visualize some random training examples\n",
    "import random\n",
    "\n",
    "random_index = random.randint(0, len(train_df) - 5)  # create random indexes not higher than the total number of samples\n",
    "for row in train_df[[\"Text\", \"Tags\"]][random_index:random_index + 5].itertuples():\n",
    "    _, text, labels = row\n",
    "    print(f\"Text:\\n{text}\\n\")\n",
    "    print(f\"Labels: {labels}\")\n",
    "\n",
    "    print(\"---\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## One hot encoding of labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0, 0, 1, ..., 0, 0, 1],\n       [1, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       ...,\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0]])"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multilabel = MultiLabelBinarizer()\n",
    "labels = multilabel.fit_transform(train_df['Tags'])\n",
    "labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Split data into train and validation set"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "train_text, val_text, train_labels, val_labels = train_test_split(train_df[\"Text\"].to_numpy(),\n",
    "                                                                  labels,\n",
    "                                                                  test_size=0.1,\n",
    "                                                                  # dedicate 10% of samples to validation set\n",
    "                                                                  random_state=42)  # random state for reproducibility"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "(44078, 44078, 4898, 4898)"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_text), len(train_labels), len(val_text), len(val_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Converting text into numbers (Tokenization)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "average_sentence_len = round(sum([len(i.split()) for i in train_text]) / len(train_text))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "max_vocab_length = 10000\n",
    "max_sentence_length = average_sentence_len\n",
    "text_vectorizer = TextVectorization(max_tokens=max_vocab_length,  # how many words in the vocabulary\n",
    "                                    standardize=\"lower_and_strip_punctuation\",  # how to process text\n",
    "                                    split=\"whitespace\",  # how to split tokens\n",
    "                                    ngrams=None,  # create groups of n-words?\n",
    "                                    output_mode=\"int\",  # how to map tokens to numbers\n",
    "                                    output_sequence_length=max_sentence_length,\n",
    "                                    # how long should the output sequence of tokens be?\n",
    "                                    pad_to_max_tokens=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "text_vectorizer.adapt(train_text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "long static strings in shortlived objects this might be a stupid question or just make me look stupid however i would be interested in how to work with long string objects in the context of shortlived objectsthink about long sql queries in cron job or anonymous command or functionlike classes these are very shortlived classes and even will use these long strings once in their lifetime for most of the time what is better to construct a string inline and let it be collected with the instance or make it static final anyway and let them sit in the memory useless until the classes next instantiation      \n",
      "\n",
      "Vectorized version:\n"
     ]
    },
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(1, 139), dtype=int64, numpy=\narray([[ 260,  113,  564,    7,    1,  268,   12,  377,   23,    5, 2916,\n         129,   31,   67,  108,   73,  363, 2916,  172,    3,   38,   23,\n         964,    7,   27,    4,   92,   16,  260,   58,  268,    7,    2,\n         333,    9,    1,    1,  105,  260,  278, 1003,    7, 5031, 1022,\n          31, 1752,  342,   31,    1,  321,  152,   35,  196,    1,  321,\n           8,  197,   59,   40,  152,  260,  564,  489,    7,  334, 3538,\n          14,  306,    9,    2,   96,   42,    6,  291,    4, 1633,    5,\n          58, 1088,    8,  353,   10,   23, 3591,   16,    2,  277,   31,\n         108,   10,  113,  324, 1293,    8,  353,  153, 2237,    7,    2,\n         229, 3235,  613,    2,  321,  455, 2906,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0]], dtype=int64)>"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_sentence = random.choice(train_text)\n",
    "print(f\"Original text:\\n{random_sentence}\\\n",
    "      \\n\\nVectorized version:\")\n",
    "text_vectorizer([random_sentence])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in vocab: 10000\n",
      "Top 5 most common words: ['', '[UNK]', 'the', 'i', 'to']\n",
      "Bottom 5 least common words: ['fibonacci', 'falsebut', 'fades', 'externally', 'explore']\n"
     ]
    }
   ],
   "source": [
    "# Get the unique words in the vocabulary\n",
    "words_in_vocab = text_vectorizer.get_vocabulary()\n",
    "top_5_words = words_in_vocab[:5]  # most common tokens (notice the [UNK] token for \"unknown\" words)\n",
    "bottom_5_words = words_in_vocab[-5:]  # least common tokens\n",
    "print(f\"Number of words in vocab: {len(words_in_vocab)}\")\n",
    "print(f\"Top 5 most common words: {top_5_words}\")\n",
    "print(f\"Bottom 5 least common words: {bottom_5_words}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Embedding"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "data": {
      "text/plain": "<keras.layers.embeddings.Embedding at 0x1bfa8203490>"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "embedding = layers.Embedding(input_dim=max_vocab_length,  # set input shape\n",
    "                             output_dim=128,  # set size of embedding vector\n",
    "                             embeddings_initializer=\"uniform\",  # default, intialize randomly\n",
    "                             input_length=max_sentence_length)  # how long is each input\n",
    "\n",
    "embedding"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "how to define and work with an array of bits in c i want to create a very large array on which i write 0s and 1s i am trying to simulate a physical process called random sequential adsorption where units of length 2 dimers are deposited onto an ndimensional lattice at a random location without overlapping each other the process stops when there is no more room left on the lattice for depositing more dimers lattice is jammedinitially i start with a lattice of zeroes and the dimers are represented by a pair of 1s as each dimer is deposited the site on the left of the dimer is blocked due to the fact that the dimers cannot overlap so i simulate this process by depositing a triple of 1s on the lattice i need to repeat the entire simulation a large number of times and then work out the average coverage i have already done this using an array of chars for 1d and 2d lattices at the moment i am trying to make the code as efficient as possible before working on the 3d problem and more complicated generalisations this is basically what the code looks like in 1d simplifiedint main define lattice array charmallocn sizeofchar total c 0 carry out rsa multiple times for i 0 i 10 i rand seq ads calculate average coverage efficiency at jamming printfcoverage efficiency lf total c10 return 0void rand seq ads initialise array initial conditions memseta 0 and sizeofchar available sites n count 0 while the lattice still has enough room whileavailable sites 0 generate random site location x rand deposit dimer if site is available ifarrayx 0 arrayx 1 arrayx1 1 count 1 available sites 2 mark site left of dimer as unavailable if its empty ifarrayx1 0 arrayx1 1 available sites 1 calculate coverage and add to total c countn total c cfor the actual project i am doing it involves not just dimers but trimers quadrimers and all sorts of shapes and sizes for 2d and 3d i was hoping that i would be able to work with individual bits instead of bytes but i have been reading around and as far as i can tell you can only change 1 byte at a time so either i need to do some complicated indexing or there is a simpler way to do itthanks for your answers      \n",
      "\n",
      "Embedded version:\n"
     ]
    },
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(1, 139, 128), dtype=float32, numpy=\narray([[[ 0.03810194, -0.00194151,  0.02151889, ..., -0.0097611 ,\n         -0.03590286,  0.03955403],\n        [ 0.01416263, -0.03613079,  0.02529489, ...,  0.04558947,\n          0.0273128 , -0.01165075],\n        [-0.02942413,  0.00273955, -0.02120998, ...,  0.0380934 ,\n          0.02103237, -0.01704551],\n        ...,\n        [ 0.01416263, -0.03613079,  0.02529489, ...,  0.04558947,\n          0.0273128 , -0.01165075],\n        [ 0.01301611,  0.04941064, -0.00040913, ..., -0.02778605,\n          0.02211258,  0.04483546],\n        [-0.0367678 , -0.0084657 , -0.03834645, ...,  0.01718051,\n          0.03707523,  0.00561762]]], dtype=float32)>"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a random sentence from training set\n",
    "random_sentence = random.choice(train_text)\n",
    "print(f\"Original text:\\n{random_sentence}\\\n",
    "      \\n\\nEmbedded version:\")\n",
    "\n",
    "# Embed the random sentence (turn it into numerical representation)\n",
    "sample_embed = embedding(text_vectorizer([random_sentence]))\n",
    "sample_embed"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating Baseline Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# Create tokenization and modelling\n",
    "# pipeline => To Sequentially apply a list of transforms\n",
    "model_0 = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer()),  # convert words to numbers using tfidf\n",
    "    ('clf', OneVsRestClassifier(LinearSVC(), n_jobs=1))\n",
    "])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "data": {
      "text/plain": "Pipeline(steps=[('tfidf', TfidfVectorizer()),\n                ('clf', OneVsRestClassifier(estimator=LinearSVC(), n_jobs=1))])"
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_0.fit(train_text, train_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "model_0_predictions = model_0.predict(val_text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [],
   "source": [
    "# Function to evaluate: accuracy, precision, recall, f1-score\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "\n",
    "def calculate_results(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates model accuracy, precision, recall and f1 score of a binary classification model.\n",
    "\n",
    "    Args:\n",
    "    -----\n",
    "    y_true = true labels in the form of a 1D array\n",
    "    y_pred = predicted labels in the form of a 1D array\n",
    "\n",
    "    Returns a dictionary of accuracy, precision, recall, f1-score.\n",
    "    \"\"\"\n",
    "\n",
    "    true_labels = 0\n",
    "    count = 0\n",
    "    for i in range(len(y_true[0])):\n",
    "        true_labels += np.count_nonzero((model_0_predictions == val_labels)[:, i] == True)\n",
    "        count += len(model_0_predictions)\n",
    "\n",
    "    element_wise_accuracy = true_labels * 100 / count\n",
    "\n",
    "    # Calculate model accuracy\n",
    "    model_accuracy = accuracy_score(y_true, y_pred) * 100\n",
    "    # Calculate model precision, recall and f1 score using \"weighted\" average\n",
    "    model_precision, model_recall, model_f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")\n",
    "    model_results = {\n",
    "        \"elementwise_accuracy\": element_wise_accuracy,\n",
    "        \"accuracy\": model_accuracy,\n",
    "        \"precision\": model_precision,\n",
    "        \"recall\": model_recall,\n",
    "        \"f1\": model_f1}\n",
    "    return model_results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "outputs": [
    {
     "data": {
      "text/plain": "{'elementwise_accuracy': 97.09473254389546,\n 'accuracy': 54.98162515312372,\n 'precision': 0.8371819082860844,\n 'recall': 0.6455717118307998,\n 'f1': 0.7261454279536945}"
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_results(val_labels, model_0_predictions)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Simple Dense Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dense Layer is simple layer of neurons in which each neuron receives input from all the neurons of previous layer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "outputs": [],
   "source": [
    "# Build model with the Functional API\n",
    "from tensorflow.keras import layers\n",
    "inputs = layers.Input(shape=(1,), dtype=\"string\") # inputs are 1-dimensional strings\n",
    "x = text_vectorizer(inputs) # turn the input text into numbers\n",
    "x = embedding(x) # create an embedding of the numerized numbers\n",
    "x = layers.GlobalAveragePooling1D()(x) # lower the dimensionality of the embedding (try running the model without this layer and see what happens)\n",
    "\n",
    "// decide whether the neuron should be activated or not\n",
    "outputs = layers.Dense(20, activation=\"sigmoid\")(x) # create the output layer, want binary outputs so use sigmoid activation\n",
    "model_1 = tf.keras.Model(inputs, outputs, name=\"model_1_dense\") # construct the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model_1.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1_dense\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_5 (TextV  (None, 139)              0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     (None, 139, 128)          1280000   \n",
      "                                                                 \n",
      " global_average_pooling1d_2   (None, 128)              0         \n",
      " (GlobalAveragePooling1D)                                        \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 20)                2580      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,282,580\n",
      "Trainable params: 1,282,580\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1378/1378 [==============================] - 30s 21ms/step - loss: 0.2388 - accuracy: 0.1572 - val_loss: 0.2037 - val_accuracy: 0.2750\n",
      "Epoch 2/5\n",
      "1378/1378 [==============================] - 29s 21ms/step - loss: 0.1787 - accuracy: 0.4313 - val_loss: 0.1535 - val_accuracy: 0.5304\n",
      "Epoch 3/5\n",
      "1378/1378 [==============================] - 29s 21ms/step - loss: 0.1357 - accuracy: 0.5914 - val_loss: 0.1241 - val_accuracy: 0.6162\n",
      "Epoch 4/5\n",
      "1378/1378 [==============================] - 30s 22ms/step - loss: 0.1121 - accuracy: 0.6510 - val_loss: 0.1094 - val_accuracy: 0.6566\n",
      "Epoch 5/5\n",
      "1378/1378 [==============================] - 31s 22ms/step - loss: 0.0986 - accuracy: 0.6833 - val_loss: 0.1012 - val_accuracy: 0.6786\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "model_1_history = model_1.fit(train_text, # input sentences can be a list of strings due to text preprocessing layer built-in model\n",
    "                              train_labels,\n",
    "                              epochs=5,\n",
    "                              validation_data=(val_text, val_labels))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.07564071, 0.00131911, 0.01794747, ..., 0.01041836, 0.00801349,\n        0.12972194],\n       [0.00993046, 0.07407355, 0.00124362, ..., 0.00667396, 0.00120899,\n        0.00145915],\n       [0.01438844, 0.00039795, 0.001531  , ..., 0.00373155, 0.00045359,\n        0.0022527 ],\n       ...,\n       [0.01449877, 0.00262266, 0.01206872, ..., 0.01118988, 0.00484034,\n        0.00296563],\n       [0.00289783, 0.01402417, 0.0007312 , ..., 0.02070445, 0.00832382,\n        0.0032801 ],\n       [0.12063766, 0.12050229, 0.07319388, ..., 0.03447977, 0.0268411 ,\n        0.02078125]], dtype=float32)"
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1_predictions = model_1.predict(val_text)\n",
    "model_1_predictions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(4898, 20), dtype=float32, numpy=\narray([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>"
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1_predictions = tf.squeeze(tf.round(model_1_predictions)) # squeeze removes single dimensions\n",
    "model_1_predictions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "outputs": [
    {
     "data": {
      "text/plain": "{'elementwise_accuracy': 97.09473254389546,\n 'accuracy': 41.2004899959167,\n 'precision': 0.7880815539187447,\n 'recall': 0.4897554527428949,\n 'f1': 0.5946402259705589}"
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_results(val_labels, model_1_predictions)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model 2 RNN (LSTM)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 139, 128)\n",
      "(None, 64)\n"
     ]
    }
   ],
   "source": [
    "# Create LSTM model\n",
    "from tensorflow.keras import layers\n",
    "inputs = layers.Input(shape=(1,), dtype=\"string\")\n",
    "x = text_vectorizer(inputs)\n",
    "x = embedding(x)\n",
    "print(x.shape)\n",
    "# x = layers.LSTM(64, return_sequences=True)(x) # return vector for each word in the Tweet (you can stack RNN cells as long as return_sequences=True)\n",
    "x = layers.LSTM(64)(x) # return vector for whole sequence\n",
    "print(x.shape)\n",
    "# x = layers.Dense(64, activation=\"relu\")(x) # optional dense layer on top of output of LSTM cell\n",
    "outputs = layers.Dense(20, activation=\"sigmoid\")(x)\n",
    "model_2 = tf.keras.Model(inputs, outputs, name=\"model_2_LSTM\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model_2.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2_LSTM\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_5 (TextV  (None, 139)              0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     (None, 139, 128)          1280000   \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 64)                49408     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 20)                1300      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,330,708\n",
      "Trainable params: 1,330,708\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_2.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1378/1378 [==============================] - 132s 91ms/step - loss: 0.1835 - accuracy: 0.3283 - val_loss: 0.1140 - val_accuracy: 0.5962\n",
      "Epoch 2/5\n",
      "1378/1378 [==============================] - 88s 64ms/step - loss: 0.0909 - accuracy: 0.6712 - val_loss: 0.0847 - val_accuracy: 0.6762\n",
      "Epoch 3/5\n",
      "1378/1378 [==============================] - 95s 69ms/step - loss: 0.0712 - accuracy: 0.7200 - val_loss: 0.0776 - val_accuracy: 0.7195\n",
      "Epoch 4/5\n",
      "1378/1378 [==============================] - 81s 59ms/step - loss: 0.0604 - accuracy: 0.7604 - val_loss: 0.0767 - val_accuracy: 0.7207\n",
      "Epoch 5/5\n",
      "1378/1378 [==============================] - 84s 61ms/step - loss: 0.0529 - accuracy: 0.7837 - val_loss: 0.0776 - val_accuracy: 0.7197\n"
     ]
    }
   ],
   "source": [
    "# Fit model\n",
    "model_2_history = model_2.fit(train_text,\n",
    "                              train_labels,\n",
    "                              epochs=5,\n",
    "                              validation_data=(val_text, val_labels))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(4898, 20), dtype=float32, numpy=\narray([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>"
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2_predictions = tf.squeeze(tf.round(model_2.predict(val_text)))\n",
    "model_1_predictions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "outputs": [
    {
     "data": {
      "text/plain": "{'elementwise_accuracy': 97.09473254389546,\n 'accuracy': 61.39240506329114,\n 'precision': 0.8211625200206847,\n 'recall': 0.6981163251817581,\n 'f1': 0.7490979114643821}"
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_results(val_labels, model_2_predictions)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## TensorFlow Hub Pretrained Sentence Encoder"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_hub'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_14236/1196409979.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;31m# Example of pretrained embedding with universal sentence encoder - https://tfhub.dev/google/universal-sentence-encoder/4\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m \u001B[1;32mimport\u001B[0m \u001B[0mtensorflow_hub\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mhub\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      3\u001B[0m \u001B[0membed\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mhub\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mload\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"https://tfhub.dev/google/universal-sentence-encoder/4\"\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;31m# load Universal Sentence Encoder\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m embed_samples = embed([sample_sentence,\n\u001B[0;32m      5\u001B[0m                       \"When you call the universal sentence encoder on a sentence, it turns it into numbers.\"])\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'tensorflow_hub'"
     ]
    }
   ],
   "source": [
    "# Example of pretrained embedding with universal sentence encoder - https://tfhub.dev/google/universal-sentence-encoder/4\n",
    "import tensorflow_hub as hub\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\") # load Universal Sentence Encoder\n",
    "embed_samples = embed([sample_sentence,\n",
    "                      \"When you call the universal sentence encoder on a sentence, it turns it into numbers.\"])\n",
    "\n",
    "print(embed_samples[0][:50])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tf\n",
    "# Compile model\n",
    "model_3.compile(loss=\"binary_crossentropy\",\n",
    "                loss=tf.metrics.F1Score,\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}