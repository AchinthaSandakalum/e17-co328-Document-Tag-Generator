,title,Scraped Data,description,batch,category,project_url,repo_url,page_url,data_url,api_url
0,A GUI for controlling and supervising multiple robots remotely,"A GUI for control & supervising multiple robots remotely
 Projects - Department of Computer Engineering
 University of Peradeniya, Sri Lanka
 A GUI for control & supervising multiple robots remotely
 Team
 E/15/140, Jaliyagoda A.J.N.M., nuwanjaliyagoda@eng.pdn.ac.lk
 E/15/173, Karunarathne S.D.D.D, dinelkadilshani95@gmail.com
 E/15/350, Tennakoon T.M.P.B., pasan96tennakoon@gmail.com
 Table of Contents
 Introduction
 Solution Architecture
 Hardware & Software Designs
 Links
 Introduction
 This idea came from a project that is related to swarm intelligence. According to the definition, “Swarm intelligence is the collective behavior of decentralized, self-organized systems”
 Swarm robotics is applying swarm intelligence to accomplish a bigger task. And it also similar to the behavior of animals like bees, ants, birds, etc.
 One of the greatest fallbacks of swarm intelligence-related research is that it is difficult to simulate the algorithms in the real world unless you have a large number of robots to test these algorithms. Building a group of robots takes a lot of time and it is very expensive. As a solution to this problem, we can design or buy general purpose robots which have hardware capabilities to run basic swarm intelligence related algorithms. But buying a set of pre-built robots doesn’t solve the whole problem since it is too expensive yet.
 The final goal of this project is not only to design a general-purpose swarm robot unit but designing the simulation platform too. This simulation platform will be able to control basic functionalities of the robots such as assign a robot into a defined place, recharge the robot’s battery when it is draining out, program the robots with giving algorithms, etc… This simulation arena can be accessed from a remote location and these remote users can upload their own algorithms into the robots which are placed in the arena. After upload, they can run it on robots and see the response of the robots using Data and Video feedbacks. Research teams who are working in the field of Swarm Robotics can test their algorithms without taking much effort into hardware. So it saves their time and money.
 Introduction Video: youtu.be/40D3IqbQy5A
 Solution Architecture
 Identifying the scope
 Since this whole project is beyond our scope for this unified project we are planning to do only a part of the main project. We hope to create a simulation platform for these robots and control and monitor a few parameters using a remote GUI (Graphical User Interface).
 Our goal is to develop a GUI that a user can select robots and specify locations for them in the simulation arena. Then robots will be moved to these specified locations within the smallest possible time by avoiding collisions with other robots.
 This project is not only limited to swarm intelligence based problems. We can use what we will develop to increase the efficiency of the real world problems like Search and Rescue Missions, Bomb detection, Planetary exploration, etc. Common facts of these problems are that there is a group or robots which are located in a geographical area and the person who needs controls them from a remote location.
 Identifying the requirements
 Since we need to remotely monitor and control robots, it is required to establish a communication link between the simulation arena and the end user in real time. This communication link should be able to send and receive messages in both directions
 Another requirement is that robots should be able to move into a specified location of the arena. In this case, the robot should be able to identify its current coordinates and the distance of the travel precisely.
 When a large number of robots moving in a limited area, there is a high probability to be collisions between robots. We need to find a suitable mechanism to avoid such kind of situations.
 We are allowing remote users (through the internet) to control and monitor the robots, so it is compulsory to think about the security of the network.
 Suggested Data Communication Flow
 Hardware and Software Designs
 Communication Protocol between Robot Server and End User
 When finding a solution for our project, we found a few challenging points. Since we hope to remotely monitor and control ‘multiple’ robots, we wanted to have a real-time communication method between our end nodes (in this case, robots) and the client (in this case, human users) The connection should be able to send control signals from user to robots as
 well as response messages from the robots.
 We first considered RESTful API based server-client architecture, but it was rejected due to it doesn’t support full duplex communication. Next, we looked into use a MQTT Broker between our robots and end user. MQTT (Message Queue Telemetry Transport) was originally developed for low power IoT devices. It is a topic based on communication. Nodes can subscribe to topics, and publish into topics. When a device published data into a topic, MQTT broker will inform it to all the devices/nodes which were subscribed into that topic.
 We found another communication protocol known as WebRTC. It is a free and open source project which is developed for communication between browsers and mobile applications in real time. WebRTC is a plugin-free API that most of the modern web browsers support. It has multiple standards and protocols, including STUN/TURN servers, signaling, JSEP, ICE, SIP, SDP, NAT, UDP/TCP, network sockets, and more.
 Generally, WebRTC is designed for stream Video and Audio, but there is a channel for data/media stream too. One of our requirements is to remotely monitor the robots and that includes both video and data. So we finally decided to use WebRTC as our communication method.
 You can learn more about WebRTC from following links.
 WebRTC : Official Page
 What Is WebRTC and How Does It Work?
 Data Communication between Robots and Robot Server
 We decided to use WiFi as the default communication method between our robots and the robot server. The main reason to choose WiFi is that we can implement two-way communication for multiple channels with the minimum hardware cost. And can be used in various communication protocols, which are developed on top of the basic WiFi protocols such as IEEE 802.11
 Robot Navigation Control
 Since our robots will be controlled remotely, robots should be able to handle incoming commands while navigating. Due to the practical hardware problems, robots can’t move on straight directions and take precise turns without feedback loop based control structures. Most of the available sensors that measure the distance have errors and when we continually taking measurements, it will add cumulative errors. So we decided to avoid this by doing a small modification to our platform.
 We decided to use a black color grid on the arena, so robots can follow the lines and correct the cumulating errors from the junctions when it is passing them. Not only that, it can use this grid system to take 90 degree turns using floor color sensors and a simple feedback loop.
 To identify the black lines from the white background, we decided to use IR Transmitter Receiver pairs. Principle of this sensor is that white color background reflects the IR beam emitted by the IR diode, but not by the black color background. The voltage output of the Photodiode will depend on the amount of reflection. Currently, we hope to feed this digital signal into our microcontroller as an external hardware interrupt, rather than polling digital inputs. So we decided to use an Op-amp circuit with Voltage comparator arrangement to convert this analog reading into a digital signal.
 The comparator is an electronic decision-making circuit that makes use of an operational amplifier very high gain in its open-loop state, that is, there is no feedback resistor. The Op-amp comparator compares one analog voltage level with another analog voltage level, or some preset reference voltage, VREF and produces an output signal based on this voltage comparison. In other words, the op-amp voltage comparator compares the magnitudes of two voltage inputs and determines which is the largest of the two.
 We have designed the comparator circuit using the IC named, LM324. The operational amplifier LM324 IC can work like a normal comparator, and it comprises four independent op-amps internally. This IC has designed with low-power, bandwidth and high stability for operating with single power supply over extensive voltage ranges. The range of operating voltages of this IC includes 3.0 V for low and 32 V for high. The range of common mode input mainly comprises the negative voltage supply, thus removing the requirement of outside biasing components in several applications. The range of output voltage also comprises the negative voltage supply.
 Voltage Reference for the comparator circuit is provided by a Multi-turn trimming resistor because it can be configured as a voltage divider more precisely than a usual Preset Resistor.
 Here are the Schematic Diagram and the PCB layout of the comparator circuit we have designed. We used an open-source software called Fritzing to design the PCB. Circuit board was fabricated using the PCB milling method, which engraves and isolate the circuit paths on copper plate.
 Overview of the PCB Design
 To design the PCB, we used free and open-source software named Fritzing. The PCB is a single layer design and we used jumper cables to avoid crossed signal paths. To fabricate the PCB, the design was exported as a Gerber file and processed with the software named dipTrace.
 After processing the Gerber file, it was imported into a software named, FlatCAM to convert the PCB design into a toolpath for machinery. The G-Code files which exported from the FlatCam were sent to the CNC milling machine, and the machine fabricated the PCB by few isolating milling and drilling cycles.
 After two and a half hours of soldering, we were able to completely assemble the PCB as shown below. (We used a ready-made power supply known as a
 DC to DC Buck converter to reduce the battery voltage of 8.4v to 5v and it is the green color module you can see in below image)
 Overview of the Robot Design
 We have used SolidWorks for designing the structure of the robot. After a few design revisions, we came up with the following design. We decided to use a round shape for the robot.
 We used a black 3mm Cladding Board as the raw material of the base. The design was fabricated by CNC milling, using a 1.5mm end milling bit with a contour milling operation. Wheels also manufactured using the same material and using the same method.
 We used 5mm x 60mm Hex bolts, M5 nuts, and M5 washers as the spacers between two base plates and the PCB layer.
 Links
 Pages
 Local Controller
 Control Panel
 Documents
 Technical Design
 User Manual
 Other Links
 Project Repository
 Project Page
 Department of Computer Engineering
 University of Peradeniya
 Back to top",The idea is to create a GUI platform where a user can give commands which is to be followed by some ground units in a remote location. The server can identify the starting locations of the robots and manipulate them to do the task in the most efficient way as possible.,E15,Cyber-Physical Systems Projects,https://projects.ce.pdn.ac.lk/3yp/e15/A-GUI-for-controlling-and-supervising-multiple-robots-remotely/,https://github.com/cepdnaclk/e15-3yp-A-GUI-for-controlling-and-supervising-multiple-robots-remotely,https://cepdnaclk.github.io/e15-3yp-A-GUI-for-controlling-and-supervising-multiple-robots-remotely,https://cepdnaclk.github.io/e15-3yp-A-GUI-for-controlling-and-supervising-multiple-robots-remotely/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/3yp/E15/A-GUI-for-controlling-and-supervising-multiple-robots-remotely/
1,An Efficient System For Waste Collection,"An Efficient System For Waste Collection
 Projects - Department of Computer Engineering
 University of Peradeniya, Sri Lanka
 An Efficient System For Waste Collection
 I.U. Sudasinghe	E/15/347
 K.A.R.L. Alwis	E/15/010
 U.L.R.R. Perera	E/15/265
 Team
 E/15/347, I.U. Sudasinghe, isuru.sudasinghe@eng.pdn.ac.lk
 E/15/010, K.A.R.L. Alwis, alwisruchika@gmail.com
 E/15/265, U.L.R.R. Perera, risithperera@eng.pdn.ac.lk
 Table of Contents
 Introduction
 Solution Architecture
 Links
 Introduction
 This Project aims to implement an Efficient Waste
 Collection System in urban areas. Its is a very well known fact to the public that the waste collection in our country is done in a very primitive manner. Since the waste collecting vehicles has no awareness regarding the content level of the garbage cans, everyday they follow the same routine even though sometimes they arrives at garbage cans which are basically empty. This waste a considerable amount of time, money and fuel regardless to the traffic created by those vehicles. Since the use of garbage cans separately for basic types of waste (food, paper, plastic & polythene) has already began in our country, the process of collection of waste could be make very effective if there’s a method to notify the shortest route that covers all the completely filled garbage cans to the driver of the waste collection vehicle. Our goal will be to implement the above solution using Embedded Systems, the knowledge of Networking and web application design.
 Solution Architecture
 Client Side
 Server Side
 Links
 Project Repository
 Project Page
 Department of Computer Engineering
 University of Peradeniya
 Back to top",This Project aims to implement an Efficient Waste  Collection System in urban areas. Its is a very well known fact to the public that the waste collection in our country is done in a very primitive manner. ,E15,Cyber-Physical Systems Projects,https://projects.ce.pdn.ac.lk/3yp/e15/An-Efficient-System-For-Waste-Collection/,https://github.com/cepdnaclk/e15-3yp-An-Efficient-System-For-Waste-Collection,https://cepdnaclk.github.io/e15-3yp-An-Efficient-System-For-Waste-Collection,https://cepdnaclk.github.io/e15-3yp-An-Efficient-System-For-Waste-Collection/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/3yp/E15/An-Efficient-System-For-Waste-Collection/
2,An automated system for monitoring and controlling the water supply to a large farmland,"An automated system for monitoring and controlling the water supply to a large farmland
 Projects - Department of Computer Engineering
 University of Peradeniya, Sri Lanka
 An automated system for monitoring and controlling the water supply to a large farmland
 Team
 E/15/138, M.M.M. Irfan, irfanmm96@gmail.com
 E/15/209, H.K. Madhushani, kithmamadushani1@gmail.com
 E/15/307, L. Rishikeshan, work@ris.fi
 Table of Contents
 Introduction
 Solution Architecture
 Hardware & Software Designs
 Testing
 Conclusion
 Links
 Introduction
 This is a system for automatically controlling the amount of water flow in a soil field for maintaining the correct amount of water over it. The idea is to sense the amount of water using a device with an embedded system. The device has sensing elements where it measures the electrical conductivity and those results are used to estimate the amount of water the soil has. The system could be controlled by a controller/supervisor or it can be done in an automated way. The device will automatically switch off the water supplier when the highest level is reached and switch on the motor when the lowest level is reached.
 Solution Architecture
 Our solution for the above mentioned problem is to develop a system for automatically controlling the amount of water flow in a soil field for maintaining the correct amount of water over it. The idea is to sense the amount of water using a device with an embedded system. The device has sensing elements where it measures the electrical conductivity and those results are used to estimate the amount of water the soil has. The system could be controlled by a controller/supervisor or it can be done in an automated way. The device will automatically switch off the water supplier when the highest level is reached and switch on the motor when the lowest level is reached.
 Hardware and Software Designs
 WiFi coverage area of a device
 Sprinkler
 The whole idea
 Testing
 Results for hardware load testing
 Conclusion
 You can see our final product and how the water supplying process works by this link.
 CLICK TO WATCH
 Links
 Project Repository
 Project Page
 Department of Computer Engineering
 University of Peradeniya
 Back to top",This is a system for automatically controlling the amount of water flow in a soil field for maintaining the correct amount of water over it. The idea is to sense the amount of water using a device with an embedded system. The device has sensing elements where it measures the electrical conductivity and those results are used to estimate the amount of water the soil has. The system could be controlled by a controller/supervisor or it can be done in an automated way. The device will automatically switch off the water supplier when the highest level is reached and switch on the motor when the lowest level is reached.,E15,Cyber-Physical Systems Projects,https://projects.ce.pdn.ac.lk/3yp/e15/An-automated-system-for-monitoring-and-controlling-the-water-supply-to-a-large-farmland/,https://github.com/cepdnaclk/e15-3yp-An-automated-system-for-monitoring-and-controlling-the-water-supply-to-a-large-farmland,https://cepdnaclk.github.io/e15-3yp-An-automated-system-for-monitoring-and-controlling-the-water-supply-to-a-large-farmland,https://cepdnaclk.github.io/e15-3yp-An-automated-system-for-monitoring-and-controlling-the-water-supply-to-a-large-farmland/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/3yp/E15/An-automated-system-for-monitoring-and-controlling-the-water-supply-to-a-large-farmland/
3,Automated Bike Sharing System,"Automated Bike Sharing System
 Projects - Department of Computer Engineering
 University of Peradeniya, Sri Lanka
 Automated Bike Sharing System
 Team
 E/14/154, JAYASUNDARA J.M.S.M., e14154@ce.pdn.ac.lk
 E/14/141, IHALAGEDARA I.P.S.B., e14141@ce.pdn.ac.lk
 E/14/194, LOKUGE S.D., e14194@ce.pdn.ac.lk
 Table of Contents
 Introduction
 Solution Architecture
 Hardware & Software Designs
 Links
 Introduction
 As the most of the universities have wide area of land, transportation within the university causes time waste, accidents, congestion because of using the private vehicles, parking problems and the energy consumption related to the mobility of workers and students of the universities. The bicycle sharing programs have received increasing attention in recent years with initiatives to increase bike usage,better meet the demand of a more mobile public and lessen the environmental impacts of our transportation activities. So the project aims to introduce automated bike sharing system to minimize above impacts while evaluating the mobility patterns of academic campuses and assessing the energy consumption and pollutant emissions produced by the universities. This system provides the users to unlock the
 chosen bicycle in the substations via a mobile app and start riding, check the availability of bicycles and authorized people to track the path of rides of all users.
 Due to the time limitation we are focusing only on smart locking system of bicycles and the mobile application.
 As this project is a Unified Project, the three aspects related to each subjects are as follows.
 CO321 : In the substations, the bicycles are locked using a smart lock and the rider have to unlock the chosen bicycle through a mobile app. Here we are using a QR code to open the each lock through the mobile app.
 CO324 : Each docks are considered as a node and they are connected to a another node placed in sub stations.Those nodes in the substation are connected to a centralized node. Each locks in the docks are controlled by the central server. The details of the each users are monitored at the central node. The locations of the bicycles are getting using the mobile app.
 CO325 : The details and data about each rider and the bicycle are sent through the system as encrypted data.
 Intro Video
 Solution Architecture
 Embedded System Designing
 Measuring and Controlling
 RFID reader and tags/stickers - To identify each bicycle is in the exact position and to identify the bicycle when returning to the dock station.
 Electric lock - To lock the bicycle
 Embedded Platform
 Arduino: Arduino
 is an open source computer hardware and software platform which is very easy to use. There are enough libraries and compatible modules which can connect to the arduino board. For serial communication we can have hardware serial ports or software serial ports. To control the locking mechanism there are digital I/O pins and ICSP pins.
 Connecting the system to the network
 Whole locking system will connect to internet using a GSM module
 Users will connect to the system using a Mobile App
 Peripheral devices
 RFID reader - used to read the RFID stickers in the bicycle. These stickers have a unique id which we use as the identification of bicycle. It is a 5V device, so you don’t need a external power source. ICSP pins are going to use for the communication between the reader and the arduino board.
 GSM module - used to connect with central server. TTL pins in GSM module will use to connect the module to Arduino board.
 A linear actuator - used in locking mechanism of the bikes. Digital I/O pins will be used to send control signals to the actuator.
 Limitations of peripheral devices
 There are various security problems with locking mechanism. Additional sensors have to use in order to make more secure.
 Web and Network Application Designing
 Protocols and Middleware
 HTTP - used to maintain the communication between dock stations and central server.
 I2C protocol - used to communicate between locks and the relay node.
 A central server - used to control the locks. A user scan the QR code in the lock and send the information with his login details to the server. Then the server will unlock the relevant lock and start to track the bicycle using the GPS system of the mobile using the given mobile app. It maintains a database of users and bicycles.
 Back End and Front End
 Back end
 Will use Node.js as the server side language
 Mongodb as database management system
 Heroku cloud application platform
 Front end
 Web interface for administrational usage
 HTML, CSS, Javascript
 Mobile Application
 Android studio
 Connecting components through APIs
 REST API
 Use to exchange information among components ( lock and mobile app)
 Google Maps API
 Will use to show to location of the bicycle
 Barcode API
 Will use to parse the QR code with different format
 Network Security
 Sensitive data
 Detail of users are stored in central server. Mobile app is used to login to the system. These login requests need to be secure.
 Passwords of users need to be stored in hash representation.
 Controlling responses from server should be secured
 Security features
 Encrypting the requests and responses.
 Encouraging users to use a strong password.
 Hardware and Software Designs
 Technologies used:
 React
 Redux
 Electron
 Material UI
 Overall system design
 Documents
 Project Report
 User Manual & Technical Note
 Links
 Project Repository
 Project Page
 Department of Computer Engineering
 University of Peradeniya
 Back to top",This project is about building an automated bike sharing system for university. This will be a convenient way for transportation inside the university premises.,E15,Cyber-Physical Systems Projects,https://projects.ce.pdn.ac.lk/3yp/e15/Automated-Bike-Sharing-System/,https://github.com/cepdnaclk/e15-3yp-Automated-Bike-Sharing-System,https://cepdnaclk.github.io/e15-3yp-Automated-Bike-Sharing-System,https://cepdnaclk.github.io/e15-3yp-Automated-Bike-Sharing-System/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/3yp/E15/Automated-Bike-Sharing-System/
4,Automated Book Management System Automated Book Carrying Robot,"Automated Book Management System - Automated Book Carrying Robot
 Projects - Department of Computer Engineering
 University of Peradeniya, Sri Lanka
 Automated Book Management System - Automated Book Carrying Robot
 Team
 E/15/016, ANOJAN S., e15016@eng.pdn.ac.lk
 E/15/171, KAPILRAJH R., svkapilvs@gmail.com
 E/15/351, THAKSHAJINI S., tsuhumar8@gmail.com
 Table of Contents
 Introduction
 Solution Architecture
 Hardware & Software Designs
 Links
 Introduction
 In libraries, We have planned to implement a book carrying robot to help the workers.Our embedded system will have Arduino,IR sensors,DC motors,power batteries and voltage regulator etc.It will work as a line following robot.Wi-fi module is used to communicate with a robot.We use RFID tag for books to use a web application,a database and a server to store the book details as well as the location details about the book shelfs for each and every book.
 Solution Architecture
 Making an automated book picking robot to help the workers in the library.
 The robot saves time and reduces human effort.
 It reduces human error and manages the books in an efficient and effective way.
 It is always available in the library.
 It is reliable.
 It ensures the security of the library and keeps the book safe.
 Hardware and Software Designs
 #### Basic Circuit Design for the Line Following Robot.
 #### Final Product
 Links
 Documents
 Project Report
 Project Proposal
 Other Links
 Project Repository
 Project Page
 Department of Computer Engineering
 University of Peradeniya
 Back to top","In libraries, We have planned to implement a book carrying robot to help the workers. Our embedded system will have Arduino, IR sensors, DC motors, power batteries and voltage regulator etc.",E15,Cyber-Physical Systems Projects,https://projects.ce.pdn.ac.lk/3yp/e15/Automated-Book-Management-System-Automated-Book-Carrying-Robot/,https://github.com/cepdnaclk/e15-3yp-Automated-Book-Management-System-Automated-Book-Carrying-Robot,https://cepdnaclk.github.io/e15-3yp-Automated-Book-Management-System-Automated-Book-Carrying-Robot,https://cepdnaclk.github.io/e15-3yp-Automated-Book-Management-System-Automated-Book-Carrying-Robot/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/3yp/E15/Automated-Book-Management-System-Automated-Book-Carrying-Robot/
5,Automated Vehicle Parking System,"Automated Vehicle Parking System
 Projects - Department of Computer Engineering
 University of Peradeniya, Sri Lanka
 Automated Vehicle Parking System
 Team
 E/15/076, DILEKA J.H.S., sandushidileka2@gmail.com
 E/15/065, DE SILVA K.G.P.M., prasadmadusankadasilva@gmail.com
 E/15/220, MALITHTHA K.H.H., maliththamax@gmail.com
 Table of Contents
 Introduction
 Solution Architecture
 Hardware & Software Designs
 Links
 Introduction
 Vehicle Parking areas usually have people who give printed tickets for parking. This consumes a lot of time and which causes a lot of traffic. Along with causing traffic and commotion, there is also a lot of paper litter outside the vehicle parking areas.As the number of vehicles are increasing, the problems faced by manual parking management system are also increasing. Such problems can be eliminated to some extent by implementing an intelligent parking system where the entry and exit of cars is monitored and payment is made easy with sensor technology.In order to avoid all of these, Automated Vehicle Parking System can be used. This project uses an RFID which can be swiped at the entrance.
 Solution Architecture
 RFID Card for registered users
 Tag for unregistered users
 No paper tickets
 No waitings and checkings at the gate
 Time informed by a message
 Automated payments
 Hardware and Software Designs
 Data Flow and Infrastructure
 Overall Process
 PCB Design for the Vehicle Parking System
 Links
 Document
 Project Proposal
 Testing
 Progress
 Other Links
 Project Repository
 Project Page
 Department of Computer Engineering
 University of Peradeniya
 Back to top","Vehicle Parking areas usually have people who give printed tickets for parking. This consumes a lot of time and which causes a lot of traffic. Along with causing traffic and commotion, there is also a lot of paper litter outside the vehicle parking areas.As the number of vehicles are increasing, the problems faced by manual parking management system are also increasing. Such problems can be eliminated to some extent by implementing an intelligent parking system where the entry and exit of cars is monitored and payment is made easy with sensor technology.In order to avoid all of these, Automated Vehicle Parking System can be used. This project uses an RFID which can be swiped at the entrance.",E15,Cyber-Physical Systems Projects,https://projects.ce.pdn.ac.lk/3yp/e15/Automated-Vehicle-Parking-System/,https://github.com/cepdnaclk/e15-3yp-Automated-Vehicle-Parking-System,https://cepdnaclk.github.io/e15-3yp-Automated-Vehicle-Parking-System,https://cepdnaclk.github.io/e15-3yp-Automated-Vehicle-Parking-System/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/3yp/E15/Automated-Vehicle-Parking-System/
6,Automated Water Quality Monitoring System,"Automated Water Quality Monitoring System
 Projects - Department of Computer Engineering
 University of Peradeniya, Sri Lanka
 Automated Water Quality Monitoring System
 Team
 E/15/077, K.P.W.A.K.K. Dilhani, kshithija.dilhani@gmail.com
 E/15/279, L.S.W.S. Premathilaka, wathsaripremathilaka@gmail.com
 E/15/211, S.A.I. Maduwanthi, ishmadhuwanthi@gmail.com
 Table of Contents
 Introduction
 Solution Architecture
 Hardware & Software Designs
 Testing
 Conclusion
 Links
 Introduction
 Fresh water is finite resource need for agriculture,industry and human existence. Therefore the quality of water is very important.The objective of this project is to develop automated water quality monitoring system by using continues measurements of pH and turbidity measurement.Normal process is water samples are normally collected at regular period and do the analysis and this ask for larger time consumption.But in this project we hope to offer fast and easy monitoring of pH and turbidity levels with IoT applications for continues maintenance of clean water.
 The process carried out water treatment plant.
 Solution Architecture
 Hardware and Software Designs
 Circuit Diagram
 Flow Chart
 Front-End Technologies
 UI Design
 Back-End Technologies
 ER Diagran
 Testing
 Test Plan
 Following are the test plan that is to be tested.
 Integration Testing
 Focus – pH sensor, web site
 Inputs and expected output are like below.
 PH <6.5 –> Alert
 PH
 6.5 – 7. 5 –> Normal
 PH >7.5 –> Alert
 Assumption – temperature 25(C)
 Testing Environment – wifi connection,pH sensor along with the embedded device and web interface
 Testing Process - In our system we are designing it to give alert when the variation of pH and turbidity values occurred. We can give some sort of boundary values to the system and check whether it gives expected output to the clients.
 Normally pH value of treated water should be in the range of 6.5 – 7.5.Then we are going to test our system using this case.
 By using soap water ,normal water and leman water it can be tested.
 Unit Testing
 Test whether sensors are working properly.Test both pH and Turbidity sensors with known solutions to verify whether it gives expected values.
 Focus – pH sensor
 Inputs and expected outputs –pH value of water is going to be changed by using pH known chemical and expecting their exact pH values using our pH sensor
 2.2 – Vinegar
 10.5 - Milk of Magnesia
 14.0 - Sodium Hydroxide (NaOH)
 Assumption – temperature 25(C)
 Testing Environment – pH sensor is needed
 Load Testing
 Focus –whole system
 Inputs – increase the number of nodes up to 30 nodes for the system using
 dummy values
 Expected output –system should operate properly as before
 Assumption - temperature 25(C)
 Testing Environment – pH sensor ,Turbidy sensors,wifi connection and web site
 Conclusion
 Now we have completed
 two nodes in the system. We can measure pH values and Turbidity values in real time of those two water treatment stages. But we still doing it in using two water samples(pure water and muddy water). And the Database also updating at the same time when sensor values are
 updated. And when considering the web application,
 the data can be retrieved easily. And tables in the web application also real-time updated with the database.
 Links
 Project Repository
 Project Page
 Department of Computer Engineering
 University of Peradeniya
 Back to top"," Fresh water is finite resource need for agriculture,industry and human existence. Therefore the quality of water is very important.The objective of this project is to develop automated water quality monitoring system by using continues measurements of pH and turbidity measurement.Normal process is water samples are normally collected at regular period and do the analysis and this ask for larger time consumption.But in this project we hope to offer fast and easy monitoring of pH and turbidity levels with IoT applications for continues maintenance of clean water",E15,Cyber-Physical Systems Projects,https://projects.ce.pdn.ac.lk/3yp/e15/Automated-Water-Quality-Monitoring-System/,https://github.com/cepdnaclk/e15-3yp-Automated-Water-Quality-Monitoring-System,https://cepdnaclk.github.io/e15-3yp-Automated-Water-Quality-Monitoring-System,https://cepdnaclk.github.io/e15-3yp-Automated-Water-Quality-Monitoring-System/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/3yp/E15/Automated-Water-Quality-Monitoring-System/
7,Automatic Door Lock System,"Automatic Door Lock System
 Projects - Department of Computer Engineering
 University of Peradeniya, Sri Lanka
 Automatic Door Lock System
 Team
 E/15/119, D.L. D. Hasanika, dinithiliyanage.95@gmail.com
 E/15/202, D.P. Liyanage, preethi.du1995@gmail.com
 E/15/208, G.G.R. D. Madhushani, roshanidilhara7@gmail.com
 Table of Contents
 Introduction
 Solution Architecture
 Hardware & Software Designs
 Testing
 Detailed budget
 Conclusion
 Links
 Introduction
 Today, security has become the most important thing to be considered. People need a security system to prohibit unauthorized access to their property. Our target is to implement an automatic door lock system which allows the access only for authorized people. It helps to safeguard property from unauthorized people.
 Solution Architecture
 Data Flow and Infrastructure
 RFID Door Lock System
 Mechanism of RFID Reader
 Fingerprint Sensor
 Face Recognition
 Hardware and Software Designs
 Node 1 : RFID Door Lock System PCB design
 Node 2 : Completed PCB Design
 Testing
 PDF document Group_4_Automatic_Door_Lock_System_test_plan.pdf
 Conclusion
 Special Note:
 We decided to use IR sensors instead of PIR sensors. Because PIR sensors detect all the motions in a wide range. So it was difficult to use in this project.
 We removed the face recognition part from our node 2. Because for the high security, recognizing only the fingerprint is sufficient.
 Links
 Project Repository
 Project Page
 Department of Computer Engineering
 University of Peradeniya
 Back to top"," Today, security has become the most important thing to be considered. People need a security system to prohibit unauthorized access to their property. Our target is to implement an automatic door lock system which allows the access only for authorized people. It helps to safeguard property from unauthorized people. ",E15,Cyber-Physical Systems Projects,https://projects.ce.pdn.ac.lk/3yp/e15/Automatic-Door-Lock-System/,https://github.com/cepdnaclk/e15-3yp-Automatic-Door-Lock-System,https://cepdnaclk.github.io/e15-3yp-Automatic-Door-Lock-System,https://cepdnaclk.github.io/e15-3yp-Automatic-Door-Lock-System/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/3yp/E15/Automatic-Door-Lock-System/
8,E Checkup,"E Checkup
 Projects - Department of Computer Engineering
 University of Peradeniya, Sri Lanka
 E Checkup
 Team
 E/15/366, THINESH S., sathathinesh@gmail.com
 E/15/373, VAHEESAN R., waga950924@gmail.com
 E/15/330, SATHURSAN K., wdeva22@gmail.lk
 Table of Contents
 Introduction
 Solution Architecture
 Hardware & Software Designs
 Links
 Introduction
 This project is about making an online interface for routine medical checkups. Usually people are not happy with standing in a queue or waiting a long time in the hospital to see the doctor as well as doctors also need a most efficient and effective way to examine their patients. Mostly in routine medical checkups doctor needs a data of several biometric parameters of patient’s body. These data can be measured by some sensors and stored through our system.
 Intro
 Solution Architecture
 This system allows you to measure biometric parameters such as pulse, breath rate, oxygen in blood, electrocardiogram signals, blood pressure, glucose levels. This information is used to monitor in real time the state of a user or to get sensitive data in order to be subsequently analyzed for medical diagnosis. Biometric information gathered can be wirelessly sent to the server and stored there , the data can be visualized in a tablet or smart phone by the patient thereafter the patient can send those data to doctor. The doctors can analyze those data and provide feedback to patients.
 Hardware and Software Designs
 Data Flow of System
 Web Application Demonstration
 Links
 Documents
 Project Report
 Project Proposal
 Other Links
 Project Repository
 Project Page
 Department of Computer Engineering
 University of Peradeniya
 Back to top",This project is about making an online interface for routine medical checkups. Usually people are not happy with standing in a queue or waiting a long time in the hospital to see the doctor as well as doctors also need a most efficient and effective way to examine their patients. Mostly in routine medical checkups doctor needs a data of several biometric parameters of patient’s body. These data can be measured by some sensors and stored through our system.,E15,Cyber-Physical Systems Projects,https://projects.ce.pdn.ac.lk/3yp/e15/E-Checkup/,https://github.com/cepdnaclk/e15-3yp-E-Checkup,https://cepdnaclk.github.io/e15-3yp-E-Checkup,https://cepdnaclk.github.io/e15-3yp-E-Checkup/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/3yp/E15/E-Checkup/
9,Embedded system for detecting adverse gases,"Embedded system for detecting adverse gases
 Projects - Department of Computer Engineering
 University of Peradeniya, Sri Lanka
 Embedded system for detecting adverse gases
 Team
 E/15/243, NISANSALA R.M.B.S., sewwanis@gmail.com
 E/15/271, PRASADIKA L.B.S., sonaliprasadika077@gmail.com
 E/15/180, KARUNATHILAKA V.M.B.S.S.V., supipivirajini@gmail.com
 Table of Contents
 Introduction
 Solution Architecture
 Hardware & Software Designs
 Testing
 Links
 Introduction
 Intro Video
 As we all know, air pollution is a growing problem in Sri Lanka. This is mainly due to motorization and industrialization. Among those two sections, we concentrate on industrialization with a hope of providing solution to that problem from our decided embedded system.
 Basically, authorities who are responsible for air pollution controlling our country,tend to regulate air pollution due to toxic gasses leaving from factories only at the beginning of them. But,with the time, that process is not longer continued by the authorities.Then, the factories are feel free to discharge air pollutants to the environment by exceeding the limit identified by the authorities. This is because regularly, authorities do not have proper system to detect whether a factory is discharging air pollutants with a control.
 Solution Architecture
 Therefore, our plan is to implement an embedded system for the use of government, from which authorities can anytime come and check whether factories are discharging toxic gasses exceeding the limitation. Actually this is just like a meter reading at our home. On the display of the system, the percentage of CO, SO2, NO2, Humidity, Temperature will be shown.
 Hardware and Software Designs
 In the server side we wish to analyze and filter the row of sensors and will display them on the web site graphically.
 For our project, we are going to use sensors; MQ9 (CO sensor), AM2301(Temperature and humidity sensor), SO2 alpha-sensors and NO2 sensor. Initially, sensor details were studied by referring datasheet of each sensor . After that, the sensors are connected to the arduino UNO board and arduino codes were written to all of those sensors in order to read voltage values of the sensors’ outputs. Calibration is done for measuring temperature and humidity. Meanwhile, php and mysql were studied in order to create a database to website.
 As we decided at very first of that project, CO, SO2, Temperature and Humidity can be measured through the developed device. Furthermore, to measure CH4 gasses which are discharged from the factories, another sensor also was added. For that, MQ-2 Gas sensor module smoke was used. So at Milestone 3, successfully,
 we could show that the device was taking data from that sensor.
 Up to now, data from sensors were retrieved using a WiFi-shield. But when WiFi is not available around corresponding factory, device will not work. Therefore, for the ability to use the device by any of the factories, we decided to add a GPRS. Up to now, performance of the project run under local server, but for the ability to work with GPRS, we need a public server. Then, after milestone 3, we expected to get a public server and do the coding for GPRS. From that, we hope to present our embedded system at next milestone with GPRS module.
 Furthermore, our Air Quality Monitor con be met with Two nodes
 Testing
 The Test plan of our project, Air Quality monitor will be done through three types of tests. They are;
 Unit Test
 Integration Test
 Load Test
 Unit Test
 This testing type will be used in-order to verify the behavior of the Air Quality monitor independently from other parts. From that it can ensure that every single unit in the system works correctly.
 Under Unit test, each sensor will be tested separately. How it is that output of each sensor will be compared with corresponding standard values or the values around our working environment. For an example, output from the temperature and humidity sensor will be compared with the values around our working area. Other values can be taken from Chemical department in our faculty. Inputs of sensors are in voltage, but after calibrating each sensor separately it will display output as temperature in Celsius, humidity as percentage and other gas sensors in ppm.
 Workability of MQTT server will be tested using publisher and subscriber which are implemented within a same computer, even though they should be at two separated computers. When subscriber sends a string as a topic to publisher and publisher responds it by sending relevant data, then it can say that the MQTT is running properly. Considering our project, if subscriber sends a message as ‘SO2’ and then publisher responds to it showing SO2 quantity in ppm, then it can say that the MQTT is working well.
 Integration Test
 Integration Test will be used in-order to demonstrate that different parts of a system work together in the real-life environment with the use of external resources. For our project, database and web servers will be used as external resources. By doing this testing. We hope to achieve a high level of confidence that the whole system works as expected.
 If our project is considered, it can be tested whether the data taken from sensors is sent to Main server using MQTT, while publisher and subscriber are in two devices. If we can get results as same as in unit test for MQTT, then it can realize that data transferring from sensors to Main Server happens properly.
 Furthermore, under this test, it can be checked whether data of every sensor is sent to database simultaneously by integrating each component of a node into a bread board. It can check whether the values shown at each unit test for every sensor can be seen in the LCD display. If it is, then it can finalize that all sensors are working correctly together.
 Load Test
 Under load test, it will be determined the speed, scalability of the system. From this test, the system behavior under both normal and peak load conditions (number of nodes) will be verified.
 Final system will come up with the ability to response many nodes other than a single node. So, doing this test, we will get an idea about whether the embedded system will work for multiple nodes or not as well as how many factories the Air Quality Monitor can handle successfully and how it avoids the potential problems in future such as increased number of factories for the use of server.
 This will be done by sending topics to publisher by subscriber. For each node that would be act as unique ones, when it handles different main topics for each node and the publisher gets separate response relevant to each node, then it can say the system can handle many of nodes. In here, past data will be deleted after those data are re-presented in graphs. Then it can save more space in database for the future use.
 Modified Test Plan
 Links
 Project Repository
 Project Page
 Department of Computer Engineering
 University of Peradeniya
 Back to top","As we all know Kandy is the most polluted city in Sri Lanka. It's air has been polluted three times than the air in Colombo. There are many solutions have been taken in order to reduce the air pollution in city Kandy. But the thing is that non of them could not show better performance in giving solution for that problem. So that from our project we hope to provide most important information which may be significant when finding solutions for that problem. We may implement an embedded system from which temperature,humidity, and toxic gases (CO,CO2,NO2) may be measured using relavant sensors and from these measurements it will show locations and time where air pollution is high in Kandy city.",E15,Cyber-Physical Systems Projects,https://projects.ce.pdn.ac.lk/3yp/e15/Embedded-system-for-detecting-adverse-gases/,https://github.com/cepdnaclk/e15-3yp-Embedded-system-for-detecting-adverse-gases,https://cepdnaclk.github.io/e15-3yp-Embedded-system-for-detecting-adverse-gases,https://cepdnaclk.github.io/e15-3yp-Embedded-system-for-detecting-adverse-gases/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/3yp/E15/Embedded-system-for-detecting-adverse-gases/
10,Fire Detection and Alert System,"Fire Detection and Alert System
 Projects - Department of Computer Engineering
 University of Peradeniya, Sri Lanka
 Fire Detection and Alert System
 Team
 E/15/154, JAYASOORIYA J.K.C.N., e15154@eng.pdn.ac.lk
 E/15/187, KULANJITH G.D., devingallage@gmail.com
 E/15/142, JAYALATH A.H.G.D., ganindudananja@gmail.com
 Table of Contents
 Introduction
 Solution Architecture
 Links
 Introduction
 According to National Fire Protection Association (NFPA), there were 1,319,500 fire cases were reported only in USA in 2017. Nearly 40% of them are structure fires. Moreover, following statements are highlights from NFPA report.
 In 2017, 22 fires in the United States resulted in losses of at least $10 million each, for a cumulative total of $12.5 billion in direct property losses. These fires resulted in the deaths of 52 civilians and one firefighter, and injuries to 213 civilians and 20 firefighters.
 The other 20 large-loss fires in 2017 involved structures and resulted in a total property loss of $747.7 million.
 Smoking materials were the leading cause of home fire deaths in 2012-2016.
 Intro
 Solution Architecture
 There are two ways a fire can be happened.
 Sudden fire
 Slowly growing fire
 We are going to implement a set of devices with the capability of detecting smoke and CO particles so that, the device will detect both the above mentioned fire types and the users will get warnings only if neccessary through an alarm on the device itself and an alert will be sent to the users of the mobile app and the web application. If the fire is massive or danger enough the intensity of the alarm will get increased and if the option for emergency is enabled, the emergency authorities will get notifications about the fire disaster.
 How the device works?
 There are two types of devices come with Ignio and they are “Ignio node” and the “Ignio relay”. The Ignio node is integrated with the sensors and an alarm so that it takes data from sensors and pass down to the Ignio relay through wi-fi which is connected to the ISP directly via a home network or an enterprise network. The sudden fires will get detected by the Ignio node itself through the sensors by detecting the passing of particular bandwidths for particle density and CO emission rate. And, the other type of fires will be predicted by analysing the periodic data on the microservice architectured API.
 Links
 Project Repository
 Project Page
 Department of Computer Engineering
 University of Peradeniya
 Back to top","In Sri Lanka also fire incidents have caused many life and property damages. In our project, we try to implement a practical solution to alert people before fire getting spread out so that people can minimize the damage caused by fire.",E15,Cyber-Physical Systems Projects,https://projects.ce.pdn.ac.lk/3yp/e15/Fire-Detection-and-Alert-System/,https://github.com/cepdnaclk/e15-3yp-Fire-Detection-and-Alert-System,https://cepdnaclk.github.io/e15-3yp-Fire-Detection-and-Alert-System,https://cepdnaclk.github.io/e15-3yp-Fire-Detection-and-Alert-System/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/3yp/E15/Fire-Detection-and-Alert-System/
11,Health Watch,"Page not found · GitHub Pages
 404
 File not found
 The site configured at this address does not
 contain the requested file.
 If this is your site, make sure that the filename case matches the URL.
 For root URLs (like http://example.com/) you must provide an
 index.html file.
 Read the full documentation
 for more information about using GitHub Pages.
 GitHub Status —
 @githubstatus","Health Watch is a wrist band which monitors certain variables in the body, such as the heart rate, blood oxygen level and temperature",E15,Cyber-Physical Systems Projects,https://projects.ce.pdn.ac.lk/3yp/e15/Health-Watch/,https://github.com/cepdnaclk/e15-3yp-Health-Watch,https://cepdnaclk.github.io/e15-3yp-Health-Watch,https://cepdnaclk.github.io/e15-3yp-Health-Watch/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/3yp/E15/Health-Watch/
12,Hydroponics Automation System,"Hydroponics Automation System
 Projects - Department of Computer Engineering
 University of Peradeniya, Sri Lanka
 Hydroponics Automation System
 Team
 E/15/299, L.M. Ranushka	, e15299@eng.pdn.ac.lk
 E/15/139, D.S. Ishanthi, dsajini.i@gmail.com
 E/15/249, W.A.D. Pamoda, dasunip2@gmail.com
 Table of Contents
 Introduction
 Solution Architecture
 Hardware & Software Designs
 Testing
 Conclusion
 Links
 Introduction
 Hydroponics is the method of growing plants in a nutrient-rich water-based environment which uses artificial lighting. This method is widely used in modern agriculture because of less space and conservation of water. But the main disadvantage of this system is the need of constant monitoring to get the maximum benefits from it.
 With this Hydroponics Automation System, the plants are supplied automatically with enough water, proper amount of light intensity and proper nutrients depending on the sensors’ feedback. And also the data obtained from the sensors is accessible through a web application.
 Solution Architecture
 Hardware and Software Designs
 The modified final data flow
 of the system
 Testing
 The data gathered from the sensors are sent to the NodeMCU and sent to a web server and stored in a database. User can access this data through a web application. The automatic control of the actuators is done by the webserver and the Node MCU. Also if the user wants to take the control decisions, he can control the system through the web application.
 Conclusion
 For the demonstration purpose for now we have implemented the structure of the hydroponics system with water circulation. We are going to further improve and develop it.
 Links
 Project Repository
 Project Page
 Department of Computer Engineering
 University of Peradeniya
 Back to top",Hydroponics is the method of growing plants in a nutrient-rich water-based environment which uses artificial lighting. This method is widely used in modern agriculture because of less space and conservation of water. But the main disadvantage of this system is the need of constant monitoring to get the maximum benefits from it.,E15,Cyber-Physical Systems Projects,https://projects.ce.pdn.ac.lk/3yp/e15/Hydroponics-Automation-System/,https://github.com/cepdnaclk/e15-3yp-Hydroponics-Automation-System,https://cepdnaclk.github.io/e15-3yp-Hydroponics-Automation-System,https://cepdnaclk.github.io/e15-3yp-Hydroponics-Automation-System/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/3yp/E15/Hydroponics-Automation-System/
13,Intelligent Road Traffic Control System,"Intelligent Road Traffic Control System
 Projects - Department of Computer Engineering
 University of Peradeniya, Sri Lanka
 Intelligent Road Traffic Control System
 Team
 E/15/280, PREMATHILAKA M.P.U., pubuduudara7@gmail.com
 E/15/316, Samarasinghe U.G.S.B., imsuneth@gmail.com
 E/15/123, HERATH H.M.M.E.W.L., wisheslakshan@gmail.com
 Table of Contents
 Introduction
 Solution Architecture
 Hardware & Software Designs
 Links
 Introduction
 Vehicle travel across the world is increasing, especially in larger urban areas. It is a serious problem in traffic congestions in many major cities around the world and in these cities, it has become a nightmare for travelers. Since Traditional systems have implemented to follow a preset time schedule, they do not control variable flows coming near junctions.
 There is a goal of customizing traffic flow vehicles in a junction. As the number of roads increases steadily, and the resources provided by the current infrastructure are limited, intelligent control of traffic in the future will become a very important issue. Therefore, to better accommodation of this increasing demand, traffic control algorithms need to be simulated and optimized.
 “One study done in Boston has proved the timings of 60 intersections in one district of the city could save $1.2 million per year”
 Intro Video
 Solution Architecture
 Our Approach:
 Hardware and Software Designs
 Communication Protocol
 Components
 ESP32
 Arduino Nano
 nrf24l01 Modules
 In our system, the relay node is the ESP32 board. Arduino Uno board works as the microcontroller for the sensor node. We needed to build up a communication system between the sensor node and relay nodes in order to send real-time vehicle count taken by sensor nodes. There are 4 sensor nodes per junction in the prototype.
 Overview
 In order to establish the communication, we used nrf24l01 radio modules both in the sensor node and relay nodes. We tried to implement a protocol for our system using RF as the media and it is succeded. In this protocol relay node uses different pipes to communicate with each of the sensor nodes. Through this implementation, we could overcome the broadcasting issue and we could establish one to one communication.
 References
 Vehicle Simulator
 As the demonstration has planned to do with a prototype of a junction which intersects two roads, other junctions will be demonstrated using the simulator software. Using a camera fixed from the top of the prototype, the video stream is taken to the simulator for the convenience of observing the behavior of the algorithm.
 Simulator
 We started developing this software from scratch and we were able to implement the following feature in it.
 Add vehicle defining a custom path using GUI
 Stepwise simulation
 Continuous simulation
 Add vehicles while the simulation is running
 Simulating multiple vehicles at the same time
 Pause and continue the simulation
 Camera View added and it can work while the simulation is running (Not showing in this video)
 Communication Between Sensor nodes and the Relay node
 Problem :
 The sensor node should keep listening to the magnetic reed sensors all the time, while it should be listening for the requests from the Relay node at the same time. Since we have an Arduino Nano microcontroller as the driver of sensor nodes, they can not handle two tasks at the same time.
 Solution:
 So, we came up with a solution for the above matter. We found that the availability of an interrupt pin on the NRF24l01 modules which we use for the radio communication. The interrupt pin on the module is called “IRQ” (Interrupt ReQuest) pin.
 The IRQ pin is normally HIGH and by default, it will send out a low pulse at three different events when
 It received data
 It transmitted data
 Transmission failed or no ack received
 The maskIRQ (tx_ok, tx_fail, tx_ready) function from the TRMh20 RF24 library can be used to enable the above three functionalities.
 We used this signal to trigger an external interrupt (we used digital pin 3) on the Arduino Nano module. Then, when the Relay Node makes a request to the Sensor node asking for the required information, the sensor node can sense the incoming request and it will only reply on that point of the time and it goes back to keep listening to the magnetic reed sensors.
 Results:
 We implemented the above-mentioned functionality and tested it. We measured the response time of the Sensor node, the time it takes to receive the request from the relay node and respond to it with the vehicle count. The average time was about 24ms.
 Also, we sent requests from the Relay node with very fewer time delays between two requests, to check whether the sensor node has time to respond to the request and go back to counting vehicles. We set the delay between requests to 10ms and verified that the sensor node can even handle requests at the amount of requesting frequency.
 Complete Network Diagram & Protocols
 Communication between the Relay node and the Server
 In the system, the database, server and the server program running on a standalone Raspberry Pi 3 devices. ESP32 board uploads real-time vehicle count in each lane of the junction to the server.
 To accomplish this we have used MQTT protocol since it is the most suitable protocol for real-time communication. In this network, the Raspberry pi device is functioning as the mosquito broker and esp32 working as an MQTT client. We will describe how the overall protocol works in the coming posts.
 Web Application
 Any host on the same network can access to the system website using the raspberry pi’s IP address. The server program, database and the front end application is running on the Raspberry Pi device. The HTTP protocol is used for this communication.
 Prototype Developments
 Sensor node module
 Relay Node module
 This module is the one that keeps polling the Sensor node modules in a separate thread in it. It will send a request to a Sensor node. That request message will be received by the Sensor node and it produces an interrupt from the Sensor node’s NRF module using its IRQ pin. The Relay Node waits for the replay and stores the replay in it.
 The other role of the Relay node is to keep communication with the Server.
 Color lights arrangement
 Website of the project
 Introduction
 Optimizing the current traffic controlling system is a project that can be undertaken as a mandatory requirement for society as the current human lifestyle as mentioned before. Build up the website that fulfills the user needs will be a great help for the users to maintain their day to day lifestyle in the scope of traffic congestions.
 The front-end
 Usability, or User Experience, is the art of making your website simple, user-friendly and easy to use. Understanding the customer’s online behavior gives you insight into what works and what doesn’t.
 Homepage
 Login window
 On this website, the responsiveness is considered
 Database
 As of now, we have implemented a database that includes 7 tabulations. All the details are provided below.
 Junction1
 There are 3 more junctions which have the same table structure. From above table, the vehicle count at the update time can be obtained. laneId is the primary key. This table is filled by the relay node of the junction.
 Feedback table
 Above table can be used to obtain the user’s feedback and their suggestions about the service and also the websites. ID is the primary key of the table.
 User_Accounts
 Above table is reserved for the administrators. The purpose of building up this table is to reserve space for the people who update the algorithm that runs at the back-end of the project.
 MQTT clients and broker full system
 MQTT is a lightweight publish/subscribe messaging protocol designed for M2M (machine to machine) telemetry in low bandwidth environments. MQTT stands for Message Queuing Telemetry Transport.
 The first concept is the publish and subscribe system. In a publish and subscribe system, a device can publish a message on a topic, or it can be subscribed to a particular topic to receive messages
 MQTT Topic
 Topics are the way you register interest for incoming messages or how you specify where you want to publish the message.
 MQTT Broker
 The broker is primarily responsible for receiving all messages, filtering the messages, decide who is interested in them and then publishing the message to all subscribed clients. There are several brokers we can use.
 In the project we have used
 Mosquitto broker which is installed on the raspberry pi device and it works as the server to the junction. Relay node works as the MQTT client to the Mosquitto server. Broker is programmed using python and broker program uploads data to the SQL database which is located on the raspberry pi device.
 Links
 Documents
 Project Report
 Project Proposal
 Other Links
 Project Repository
 Project Page
 Department of Computer Engineering
 University of Peradeniya
 Back to top","Vehicle travel across the world is increasing, especially in larger urban areas. It is a serious problem in traffic congestions in many major cities around the world and in these cities, it has become a nightmare for travelers. Intelligent control of traffic in the future will become a very important issue. Therefore, to better accommodation of this increasing demand, traffic control algorithms need to be simulated and optimized. ",E15,Cyber-Physical Systems Projects,https://projects.ce.pdn.ac.lk/3yp/e15/Intelligent-Road-Traffic-Control-System/,https://github.com/cepdnaclk/e15-3yp-Intelligent-Road-Traffic-Control-System,https://cepdnaclk.github.io/e15-3yp-Intelligent-Road-Traffic-Control-System,https://cepdnaclk.github.io/e15-3yp-Intelligent-Road-Traffic-Control-System/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/3yp/E15/Intelligent-Road-Traffic-Control-System/
14,Monitoring and Tracking System for Transportation of Pharmaceuticals,"Monitoring and Tracking System for Transportation of Pharmaceuticals
 Projects - Department of Computer Engineering
 University of Peradeniya, Sri Lanka
 Project Title
 Monitoring and Tracking System for Transportation of Pharmaceuticals
 E/15/021, M.M.M. Aslam, aslam.m9618@gmail.com
 E/15/131, M.H. Hisni Mohamed, hisnimohammed@eng.pdn.ac.lk
 E/15/348, S. Suhail, suhailsajahan@eng.pdn.ac.lk
 Table of Contents
 Introduction
 Solution Architecture
 Hardware & Software Designs
 Testing
 5 Links
 Introduction
 When Medicines or Drugs are transported or stored there are conditions that have to be satisfied, such as Temperature, Pressure, Moisture inside the box. It’s important for Medicine Distributors or Hospital management to ensure that medicines or medical samples are kept in a certain controlled environment. And also important to track these items when transported.
 This Project is a system that could monitor the variations in these parameters and track the locations of items being transported and let the relevant person know through a mobile app. This project will be helpful to Medicine Distributors and Hospital Management to transport Medicine, Medical Samples or Organs Safely.
 Solution Architecture
 An Embedded System which helps to:
 Measure Temperature, Pressure, Humidity, 3D Orientation of the product being transported and observe the variations through a mobile app and web interface.
 Get a warning alert when a parameter changes drastically or a condition is broken.
 Track the location of the product being transported.
 Maintain a database and analyze the variations in parameters being monitored
 Owner of the product, the person in charge of the distribution, the wholesale buyer can monitor the database.
 It can be used for all the products which are needed to be transported in controlled conditions.
 Hardware and Software Designs
 Overall Design
 Testing
 Test Plan for the Project
 We planed to do 4 tests for our project.
 1.Test for Backend Server
 2.Load Test for Relay Node (NodeMCU ESP8266 Access Point)
 3.Unit Test for Sensors (DHT22, BMP280)
 4.Unit Test for GPS module (uBlox Neo-6)
 Links
 Project Repository
 Project Page
 Department of Computer Engineering
 University of Peradeniya
 Back to top",,E15,Cyber-Physical Systems Projects,https://projects.ce.pdn.ac.lk/3yp/e15/Monitoring-and-Tracking-System-for-Transportation-of-Pharmaceuticals/,https://github.com/cepdnaclk/e15-3yp-Monitoring-and-Tracking-System-for-Transportation-of-Pharmaceuticals,https://cepdnaclk.github.io/e15-3yp-Monitoring-and-Tracking-System-for-Transportation-of-Pharmaceuticals,https://cepdnaclk.github.io/e15-3yp-Monitoring-and-Tracking-System-for-Transportation-of-Pharmaceuticals/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/3yp/E15/Monitoring-and-Tracking-System-for-Transportation-of-Pharmaceuticals/
15,Personal Physical Trainer PPT,"Personal Physical Trainer (PPT)
 Projects - Department of Computer Engineering
 University of Peradeniya, Sri Lanka
 Personal Physical Trainer (PPT)
 Team
 E/15/059, DASSANAYAKE P.S.B., prageethda@gmail.com
 E/15/023, ATHAPATTU A.D., avishka0303@gmail.com
 E/15/238, NANAYAKKARA G.S.C., sewwandiecn@gmail.com
 Table of Contents
 Introduction
 Solution Architecture
 Hardware & Software Designs
 Links
 Introduction
 1.What is physiotherapy?
 Physiotherapy is a treatment method for injuries or deformities using physical exercises.
 2.What is Personal Physical Trainer?
 Personal Physical Trainer is an innovation for physiotherapy. It creates an environment where the patients would be more willing to engage in physiotherapy
 as well as it makes sure that patient would do the correct movements. This also allows the therapist to monitor the progress of his/hers patient and whether the patient carries out the necessary exercises.
 3.Why should people use PPT?
 There are millions and millions of people who do physiotherapy but
 about 20% of them get cured in the relevant time period if we trace the causes for it we can see that most of the time people give up physiotherapy and they do not do the exercises they supposed to. One of the major draw back in traditional methods of physiotherapy is that therapist would examine the patient at the start of therapy and then again examine the patient after period of time. Within that time period the patient’s progress isn’t not monitored. So they tend not engage on exercises or some do not have a clue whether they are doing the correct movements.
 4.How does PPT work?
 We build a wearable to read the movements of the patient. A game is develop that would be controlled by the movements of the patients. In order to reach the goe als with in the game the patient has to engage in the correct movements themselves.
 We record data that is needed for the therapist such as speed, time period patient engaged in the activity as well as number movement. With all the data therapist can monitor the patient’s development.
 Intro Video
 Solution Architecture
 Physiotherapy is one of the leading medical tactics that uses physical therapy to cure injuries or deformities. However the effectiveness of this method is not up to a satisfactory level yet. The reason cited for the said is that
 only 20% out of the millions of people who undergo physiotherapy, gets cured within the expected time period. A close example could be found out from our team itself. One of our team members underwent physiotherapy in order to cure torn tissue; however as soon as he felt better to a certain extent he stopped the treatments and he continues to suffer from on and off knuckle pains to this date. In the prevailing system, therapist only examines the patient at the beginning and after a certain time period.
 Within this duration the patient is not monitored at all.
 Unless the patient is very obedient towards his treatment, the chances of him following this procedure regularly and accurately are very low. Reasons for this could be identified as follows:
 Doing the same exercise can be boring
 With the busy schedules they tend to forget about therapy sessions
 Sometimes patients think that they are better and give up on the therapy.
 Some don’t know the right way to do it
 This is where PPT comes in it allows its users to follow the correct treatment.
 Hardware and Software Designs
 High Level Design
 Statistics show that effectiveness of physiotherapy has decreased into the level of 20% cause of the unwilling of patience continue on therapy, lack of knowledge about what exercises patient has to do, lack of constant monitoring of a patient and the lack of patient and therapist interaction our intent is to develop a system where we can reduce these barriers and lift up the effectiveness of physiotherapy. Physiotherapy is one of the leading medical fields in the world and which shows promising results by following the treatments correctly. Following diagram shows how Personal Physio Therepy (PPT) lifts up these issues.
 Background Maths
 Euler’s angle
 Euler’s angle is used to get the most accurate angle from the accelerometer.
 Complementary Filter
 Complimentary filter is used to get the most accurate angle with the data from the accelerometer and the gyroscope. Data from Gyroscope precise but tend to drift. Data from accelerometer is bit unstable but not drift. So we filter them out to get the best angle.
 The Complementary Filter is actually a union of two different filters: a High-pass Filter for the gyroscope and a Low-pass Filter for the Accelerometer. The first lets only pass the values above a certain limit, unlike the Low-pass filter, which only allows those below
 Total Anglex = 0.98 × (Total Anglex + Gyrodatay.elapsedTime) + 0.02 × accelerometer angle
 98% - Gyro data
 2% - Accelometer data
 Links
 Project Repository
 Project Page
 Department of Computer Engineering
 University of Peradeniya
 Back to top",Personal Physical Trainer is an innovation for physiotherapy. It creates an environment where the patients would be more willing to engage in physiotherapy   as well as it makes sure that patient would do the correct movements. This also allows the therapist to monitor the progress of his/hers patient and whether the patient carries out the necessary exercises.,E15,Cyber-Physical Systems Projects,https://projects.ce.pdn.ac.lk/3yp/e15/Personal-Physical-Trainer-PPT/,https://github.com/cepdnaclk/e15-3yp-Personal-Physical-Trainer-PPT,https://cepdnaclk.github.io/e15-3yp-Personal-Physical-Trainer-PPT,https://cepdnaclk.github.io/e15-3yp-Personal-Physical-Trainer-PPT/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/3yp/E15/Personal-Physical-Trainer-PPT/
16,Safer Travel Utility SaVy,"Safer Travel Utility (SaVy)
 Projects - Department of Computer Engineering
 University of Peradeniya, Sri Lanka
 Safer Travel Utility (SaVy)
 Team
 E/15/363, THILAKARATHNE E.R.R.I., ireshe@outlook.com
 E/15/048, CHANDRASIRI S.A.G.L., laksaragayal1996@gmail.com
 E/15/043, BHAGYA T.P.Y., yasirubhagya@eng.pdn.ac.lk
 Table of Contents
 Introduction
 Solution Architecture
 Hardware & Software Designs
 Links
 Introduction
 According to the Wikipedia once CTB was the largest omnibus company in the world. Currently CTB has more than 6000+ buses operating in the road which is the same number of private buses operating in Sri Lanka. Sri Lanka has the second highest road density in Asia in 2011 (173.8 km/km2). Although there are so many roads and buses in the road still people don’t like to use public transportation very much. we feel, it’s because public transportation is very unpredictable and sometimes, we have to wait hours and hours in bus stations waiting for a bus. It’s so hard to find the right bus for the right location if you don’t know the right route number. So, we feel this all happens due to the inefficiency and mismanagement of resources. So, our plan is to make a tracker management system for bus owners. So CTB and private bus owners can install our trackers in their buses to track their buses current locations, distance bus traveled for a specific time period, take rough idea of fuel required for the buses and check whether there’s any difference between consumed fuel and predicted fuel usages. So, by providing this information to owners our goal is to install our system in most of the buses that we can. Afterwards we are going to use those real time generated date from our trackers to provide commute assistance to general public. So general public can easily navigate using our app. They don’t have to wait in bus stands for the buses. Because now they know where their bus is, thanks to our app. So, using the real time data set we can provide push notifications to users about the right bus and where it is, ETA (Estimate time of Arrival) for their journeys. So, they don’t have to waste their valuable time looking for buses. And we believe this will be useful for the CTB and private bus owners to manage their fleets as well. They can find the hotspots in transportation and provide more buses to those places. And by implementing this method we believe we can minimize usage of private vehicle as people will prefer public transportation. So, it will minimize the traffic jams in Sri Lanka too.
 Intro Video
 Solution Architecture
 In introduction we explained issues that we are facing in Sri Lanka, now will see how we are going to make SaVy Happen.
 But in order to do this thing, we are planning to build our architecture like image below
 Service layer (orange) is the fundamental unit of our design (will talk more about this in next blog post). Application layer (Red) is fully customizable solution. So, all our web, mobile APP runs in this layer. It seems complicated and you might wonder why we choose such an architecture.
 Although it was little bit hard to build a system like this, in long term this would be very beneficial to us. When upgrading our system, we can seamlessly upgrade each component of the system without affecting the all system.
 And we can reuse same service layer to provide different kinds of services if needed (like assetless logistic service)
 Hardware and Software Designs
 Application Layer
 This is the abstract view of the application layer
 Service Layer
 Service layer mainly consist of a TMS (Tracker Management System) and Trackers. TMS is like the queen of a beehive which control and manage all the trackers. Then TMS will send data to the upper level.
 Tracker
 Tracker consist of
 GPS module to track GPS
 A battery to provide backup power when needed
 IMU (inertia measurement unit), magnetometer, Barometric Pressure Sensor
 So, in our service layer, simply what happens is trackers send real time GPS coordinates, IMU and other data to TMS. So TMS will process it and provide it to application layer. Then application layer display relevant information in the web and mobile app. Simple that is what SaVy does in a nutshell.
 Links
 Project Report
 Project Repository
 Project Page
 Department of Computer Engineering
 University of Peradeniya
 Back to top","Sri Lanka has the second highest road density in Asia in 2011 (173.8 km/km2). Although there are so many roads and buses in the road still people don’t like to use public transportation very much. we feel, it’s because public transportation is very unpredictable and sometimes, we have to wait hours and hours in bus stations waiting for a bus. It’s so hard to find the right bus for the right location if you don’t know the right route number. So, we feel this all happens due to the inefficiency and mismanagement of resources. So, our plan is to make a tracker management system for bus owners.",E15,Cyber-Physical Systems Projects,https://projects.ce.pdn.ac.lk/3yp/e15/Safer-Travel-Utility-SaVy/,https://github.com/cepdnaclk/e15-3yp-Safer-Travel-Utility-SaVy,https://cepdnaclk.github.io/e15-3yp-Safer-Travel-Utility-SaVy,https://cepdnaclk.github.io/e15-3yp-Safer-Travel-Utility-SaVy/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/3yp/E15/Safer-Travel-Utility-SaVy/
17,Smart Educational Management System,"Smart Educational Management System
 Projects - Department of Computer Engineering
 University of Peradeniya, Sri Lanka
 Smart Educational Management System
 Team
 E/15/246, R.L. Opanayaka, rajithaopanayaka.ro@gmail.com
 E/15/385, S.P.A.P.E. Weerasinghe, amilaweerasinghe677@gmail.com
 E/15/233, P.N.N. Muthucumarana, nipunimuthucumarana@gmail
 Table of Contents
 Introduction
 Solution Architecture
 Hardware & Software Designs
 Testing
 Links
 Introduction
 Smart Educational Management System (SEMS) is specially designed for schools, colleges and institutes to automate the institutional processes. Use of SEMS makes your institution eco-friendly as all data and your reports are stored on the server, the use of paper and files are necessary only when needed. Attendance of students can be marked by taking the fingerprint using a device with an embedded system. SEMS is very affordable and comes with no limits on the number of students, teachers and parent logins. This system is highly secured and the automation helps in reducing costs and brings a lot of savings for you.
 Solution Architecture
 Smart Attendance Marking System (SAMS)
 Hardware and Software Designs
 Milestone 1-Project Evaluation one
 What we have completed up to now
 Finger print end node
 -Taking inputs from finger print sensor
 -Enter ID from each user
 -Enroll, Search, back function implementation
 We are capable of
 Enroll a new user
 Search for a enrolled user
 Web app
 -Implemented user login registration
 -Implemented retrieving and storing from Mysql DB using php backend
 Changes made
 -As with more relational connections occured in EER diagram we tend to use MySQL(realtional) instead of firebase(non-realtional)
 Testing
 Test plan PDF
 Links
 Project Repository
 Project Page
 Department of Computer Engineering
 University of Peradeniya
 Back to top","Smart Educational Management System (SEMS) is specially designed for schools, colleges and institutes to automate the institutional processes. Use of SEMS makes your institution eco-friendly as all data and your reports are stored on the server, the use of paper and files are necessary only when needed. Attendance of students can be marked by taking the fingerprint using a device with an embedded system. SEMS is very affordable and comes with no limits on the number of students, teachers and parent logins. This system is highly secured and the automation helps in reducing costs and brings a lot of savings for you.",E15,Cyber-Physical Systems Projects,https://projects.ce.pdn.ac.lk/3yp/e15/Smart-Educational-Management-System/,https://github.com/cepdnaclk/e15-3yp-Smart-Educational-Management-System,https://cepdnaclk.github.io/e15-3yp-Smart-Educational-Management-System,https://cepdnaclk.github.io/e15-3yp-Smart-Educational-Management-System/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/3yp/E15/Smart-Educational-Management-System/
18,Smart Mirror,"Smart Mirror
 Projects - Department of Computer Engineering
 University of Peradeniya, Sri Lanka
 Smart Mirror
 Team
 E/15/261, PERERA M.S.V., vidurangaperera1@gmail.com
 E/15/136, ILLANKOON H.M.A.U., ajuillankoon@gmail.com
 E/15/292, RANASOORIYA S.M., smadu1996@gmail.com
 Table of Contents
 Introduction
 Solution Architecture
 Hardware & Software Designs
 Links
 Introduction
 People today are so busy with their day to day activities, so most of us forget many stuff that we are supposed to do. And we are not used of using a note book or a diary to write our activities but we really need a method so that we can remember our activities. But what ever we miss due to our busyness we will never forget something specially in the mornings. That’s nothing but the mirror. Mirrors have been serving people from centuries. So can this tradition mirror remind us the things we have forgotten. Yes it can! That’s nothing but by a smart mirror. No matter what your age is, no matter what your profession is, no matter how busy you are, you will stand in front of a mirror with or without intentionally. If a mirror can remind our daily activities, how nice it would be. With this no need of looking into diaries for daily plans, just have to look at the mirror as usual. Mirror will simply display your to-do list for the day. This is the main function of the Smart Mirror.
 A smart mirror has these following features.
 It will remind us with our to-do list
 It will display the date and time
 News headlines - Today’s people have no idea of what is going on. So this can deliver the current situation while sorting the news feeds according to the interest of the user.
 Weather status - Knowing the weather forecast will help the user to manage his daily activities. So that will eliminate the embarrassing moments which the user faces due to
 the sudden changes in the weather.
 BMI Analysis - Nowadays people don’t pay much attention for their health. At least they don’t know whether they are healthy or not. This mirror will let us know if the BMI value changes.
 Intro Video
 Solution Architecture
 Project Layout
 Hardware and Software Designs
 Smart Mirror
 Project Proposal
 Links
 Project Repository
 Project Page
 Department of Computer Engineering
 University of Peradeniya
 Back to top",This is an automated mirror which will identify a user and the display the to-do list of that relevant user. Further more this mirror can display the weather and the news too....,E15,Cyber-Physical Systems Projects,https://projects.ce.pdn.ac.lk/3yp/e15/Smart-Mirror/,https://github.com/cepdnaclk/e15-3yp-Smart-Mirror,https://cepdnaclk.github.io/e15-3yp-Smart-Mirror,https://cepdnaclk.github.io/e15-3yp-Smart-Mirror/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/3yp/E15/Smart-Mirror/
19,Smart Monitoring and Automated Controlling System for an Aquarium,"Smart Monitoring and Automated Controlling System for an Aquarium
 Projects - Department of Computer Engineering
 University of Peradeniya, Sri Lanka
 Smart Monitoring and Automated Controlling System for an Aquarium
 Team
 E/15/362, K.A.H.P. Thilakarathna, hasinithilakarathna4@gmail.com
 E/15/081, S.L.I. Dinuwanthi, imalshadinu@gmail.com
 E/15/345, D.V. Sripadi, vidwasripadi@gmail.com
 Table of Contents
 Introduction
 Solution Architecture
 Hardware & Software Designs
 Conclusion
 Links
 Introduction
 In almost all the aquariums, the filtering of water is done manually by the operating officers. But by using this system, filtering of the aquarium’s water can be done automatically which would increase the efficiency of an aquarium’s working routine. Filtering of water can be done by measuring and analyzing the oxygen level of water so that there will be a sufficient amount of oxygen for the survival of aquatic animals and plants. Even though the system will be operated automatically, the user or the owner of the aquarium could check the aquarium’s filtering process anytime he/she want as there is a web page connected to this system which is updating automatically when all the filtering activities are happening.
 Solution Architecture
 Hardware and Software Designs
 DESIGN OVERVIEW
 Conclusion
 MODIFIED DESIGN OVERVIEW
 Links
 Project Repository
 Project Page
 Department of Computer Engineering
 University of Peradeniya
 Back to top"," In almost all the aquariums, the filtering of water is done manually by the operating officers. But by using this system, filtering of the aquarium's water can be done automatically which would increase the efficiency of an aquarium's working routine. Filtering of water can be done by measuring and analyzing the oxygen level of water so that there will be a sufficient amount of oxygen for the survival of aquatic animals and plants. Even though the system will be operated automatically, the user or the owner of the aquarium could check the aquarium's filtering process anytime he/she want as there is a web page connected to this system which is updating automatically when all the filtering activities are happening.",E15,Cyber-Physical Systems Projects,https://projects.ce.pdn.ac.lk/3yp/e15/Smart-Monitoring-and-Automated-Controlling-System-for-an-Aquarium/,https://github.com/cepdnaclk/e15-3yp-Smart-Monitoring-and-Automated-Controlling-System-for-an-Aquarium,https://cepdnaclk.github.io/e15-3yp-Smart-Monitoring-and-Automated-Controlling-System-for-an-Aquarium,https://cepdnaclk.github.io/e15-3yp-Smart-Monitoring-and-Automated-Controlling-System-for-an-Aquarium/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/3yp/E15/Smart-Monitoring-and-Automated-Controlling-System-for-an-Aquarium/
20,Air Quality Monitoring system,"Air Quality Monitoring system
 Projects - Department of Computer Engineering
 University of Peradeniya, Sri Lanka
 Air Quality Monitoring system
 Team
 E/14/049, KASUN VIMUKTHI, e140494@ce.pdn.ac.lk
 E/14/037, BANDARA M.K.G.E.S.P., e14037@ce.pdn.ac.lk
 Table of Contents
 Introduction
 Solution Architecture
 Hardware & Software Designs
 Links
 Introduction
 Monitoring and maintaining air quality and sound quality are most important aspects of city management. A significant proportion of the population lives in cities, where air quality index has exceeded limits for several air pollutants like - particulate matter (PM), ozone, nitrogen dioxide.The Noise pollution can cause hypertension, high stress levels, tinnitus, hearing loss, sleep disturbances, and other harmful effects.These pollution pose serious health and environment risks. Therefore air and sound quality monitoring system is an important component of any smart city.
 So in this unified project we develop a system to monitor , forecast and reducing air pollution in a city through actionable data. the data will be received from gas sensors and the sound sensor. we are trying measure the data using this module in urban areas.
 All those data taken from those sensors, are sent to a central server and processed there.if the permit levels are exceed Regulatory agencies or pollution control boards will be informed.and also person can see whether the city is suitable for living and what will be the future. they can get the information about Pollution Awareness.
 Solution Architecture
 Sense the quantities using
 5pcs LM393 sound detection sensor DC 3.3 -%v sound sensor module sound detector for sound quality measurements.
 CO2 Carbon Dioxide Sensor Module MG811 for mesuring the co2 content.
 2PCS Winsen ZE05 H2S/CO/NO2/SO2 Electrochemical Detection Module UART Output.
 MQ-136 H2S Hydrogen Sulfide MQ-137 NH3 Ammonia Gas Sensor Module Detection.
 1pc ORIGINAL New ZE03-O2 winsen Electrochemical Oxygen Sensor Module DHL FEDEX.
 Key Features
 GPRS based solution, no need of laying communication cables.
 Monitors H2O, CO2, CO, SO2, NO2, NH3.
 Keeps automatic record of above parameters.
 Alarms over email or SMS
 Manage all your plants on a single platform form anywhere.
 API to Upload data to regulatory agencies server.
 Data available for public awareness with comparisons.
 Hardware and Software Designs
 milestone 2: infrastructure
 Quentities that we are going to measure are as follows
 different air and sound levels in environment:
 co percentages
 Hydrocarbonic air percentages
 nox and sox percentages
 sound level percentages
 but there are many more polluted airs in the environment. such as O3,CO2 etc.we are considering the airs which are giving
 higher impact for the climate changes and other victims.
 co percentages measures from MQ7 sensor. Hydrocarbonic air measures from MQ3 sensor.nox and sox measurements will be measuring using suitable sensors.(didn’t buy them yet)
 A GSM module in each node will be sent the date to the server.Data from each node will be sent as JSON obejct and catch them in
 the server.
 In the back end ;
 1).MySQL
 2).Node.js
 Angular.js will use the for the front end.
 sensitive data in this system are as follows;
 1) Data which are coming from the factories should be protected.for example: owner may want to earn profits without considering damage to the environment. this can achieve only by maintaining the desired standards and the levels.therefore ,they might tend to change these value in to standards values in illegal ways.
 2).Information that are stored in the servers.
 Links
 Project Repository
 Project Page
 Department of Computer Engineering
 University of Peradeniya
 Back to top","Monitoring and maintaining air quality and sound quality are most important aspects of city management. A significant proportion of the population lives in cities, where air quality index has exceeded limits for several air pollutants like - particulate matter (PM), ozone, nitrogen dioxide.The Noise pollution can cause hypertension, high stress levels, tinnitus, hearing loss, sleep disturbances, and other harmful effects.These pollution pose serious health and environment risks. Therefore air and sound quality monitoring system is an important component of any smart city.  So in this unified project we develop a system to monitor , forecast and reducing air pollution in a city through actionable data. the data will be received from gas sensors and the sound sensor. we are trying measure the data using this module in urban areas.  All those data taken from those sensors, are sent to a central server and processed there.if the permit levels are exceed Regulatory agencies or pollution control boards will be informed.and also person can see whether the city is suitable for living and what will be the future. they can get the information about Pollution Awareness.",E14,Cyber-Physical Systems Projects,https://projects.ce.pdn.ac.lk/3yp/e14/Air-Quality-Monitoring-system/,https://github.com/cepdnaclk/e14-3yp-Air-Quality-Monitoring-system,https://cepdnaclk.github.io/e14-3yp-Air-Quality-Monitoring-system,https://cepdnaclk.github.io/e14-3yp-Air-Quality-Monitoring-system/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/3yp/E14/Air-Quality-Monitoring-system/
21,Automated Attendance system,"Automated Attendance system
 Projects - Department of Computer Engineering
 University of Peradeniya, Sri Lanka
 Automated Attendance system
 Team
 E/14/018, ANURADHA WIJEWICKRAMA, e14018@ce.pdn.ac.lk
 E/14/068, DE SILVA N.G.M.H.K., e14068@ce.pdn.ac.lk
 E/14/243, SHEHAN DHINUKA, e14243@ce.pdn.ac.lk
 Table of Contents
 Introduction
 Solution Architecture
 Links
 Introduction
 This project aims to automate the attendance procedure of an educational System using biometric technology. This replaces the traditional attendance marking methods to a fool Proof method.This will reduce the time spent in a lecture for attendance marking and increases the accuracy of the process.This will be a hand held device which is passed in the class while the lecture is going on. The students can simply place the finger over the sensor to mark the presence in their class.
 Solution Architecture
 CO321- Embedded Systems
 The fingerprint is recognized using a sensor. A LED display will be used to present the information about the student.
 CO324- Network and Web Applications
 Data (finger print) is transmitted to a centralized server and confirmation of the finger print with the relevant details will be transmitted back in real time. Controlling of the network traffic occurs when there is large number of classes and large number of students will be considered as a design task and it will be designed to reduce the drawbacks.
 CO325- Security
 The data that is transmitted should be encrypted so that no other party can access or modify the data.
 Links
 Project Repository
 Project Page
 Department of Computer Engineering
 University of Peradeniya
 Back to top", This project aims to automate the attendance procedure of an educational System using biometric technology. This replaces the traditional attendance marking methods to a fool Proof method.This will reduce the time spent in a lecture for attendance marking and increases the accuracy of the process.This will be a hand held device which is passed in the class while the lecture is going on. The students can simply place the finger over the sensor to mark the presence in their class.,E14,Cyber-Physical Systems Projects,https://projects.ce.pdn.ac.lk/3yp/e14/Automated-Attendance-system/,https://github.com/cepdnaclk/e14-3yp-Automated-Attendance-system,https://cepdnaclk.github.io/e14-3yp-Automated-Attendance-system,https://cepdnaclk.github.io/e14-3yp-Automated-Attendance-system/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/3yp/E14/Automated-Attendance-system/
22,Automated Bike Sharing System,"Automated Bike Sharing System
 Projects - Department of Computer Engineering
 University of Peradeniya, Sri Lanka
 Automated Bike Sharing System
 Team
 E/14/154, JAYASUNDARA J.M.S.M., e14154@ce.pdn.ac.lk
 E/14/141, IHALAGEDARA I.P.S.B., e14141@ce.pdn.ac.lk
 E/14/194, LOKUGE S.D., e14194@ce.pdn.ac.lk
 Table of Contents
 Introduction
 Solution Architecture
 Hardware & Software Designs
 Links
 Introduction
 As the most of the universities have wide area of land, transportation within the university causes time waste, accidents, congestion because of using the private vehicles, parking problems and the energy consumption related to the mobility of workers and students of the universities. The bicycle sharing programs have received increasing attention in recent years with initiatives to increase bike usage,better meet the demand of a more mobile public and lessen the environmental impacts of our transportation activities. So the project aims to introduce automated bike sharing system to minimize above impacts while evaluating the mobility patterns of academic campuses and assessing the energy consumption and pollutant emissions produced by the universities. This system provides the users to unlock the
 chosen bicycle in the substations via a mobile app and start riding, check the availability of bicycles and authorized people to track the path of rides of all users.
 Due to the time limitation we are focusing only on smart locking system of bicycles and the mobile application.
 As this project is a Unified Project, the three aspects related to each subjects are as follows.
 CO321 : In the substations, the bicycles are locked using a smart lock and the rider have to unlock the chosen bicycle through a mobile app. Here we are using a QR code to open the each lock through the mobile app.
 CO324 : Each docks are considered as a node and they are connected to a another node placed in sub stations.Those nodes in the substation are connected to a centralized node. Each locks in the docks are controlled by the central server. The details of the each users are monitored at the central node. The locations of the bicycles are getting using the mobile app.
 CO325 : The details and data about each rider and the bicycle are sent through the system as encrypted data.
 Intro Video
 Solution Architecture
 Embedded System Designing
 Measuring and Controlling
 RFID reader and tags/stickers - To identify each bicycle is in the exact position and to identify the bicycle when returning to the dock station.
 Electric lock - To lock the bicycle
 Embedded Platform
 Arduino: Arduino
 is an open source computer hardware and software platform which is very easy to use. There are enough libraries and compatible modules which can connect to the arduino board. For serial communication we can have hardware serial ports or software serial ports. To control the locking mechanism there are digital I/O pins and ICSP pins.
 Connecting the system to the network
 Whole locking system will connect to internet using a GSM module
 Users will connect to the system using a Mobile App
 Peripheral devices
 RFID reader - used to read the RFID stickers in the bicycle. These stickers have a unique id which we use as the identification of bicycle. It is a 5V device, so you don’t need a external power source. ICSP pins are going to use for the communication between the reader and the arduino board.
 GSM module - used to connect with central server. TTL pins in GSM module will use to connect the module to Arduino board.
 A linear actuator - used in locking mechanism of the bikes. Digital I/O pins will be used to send control signals to the actuator.
 Limitations of peripheral devices
 There are various security problems with locking mechanism. Additional sensors have to use in order to make more secure.
 Web and Network Application Designing
 Protocols and Middleware
 HTTP - used to maintain the communication between dock stations and central server.
 I2C protocol - used to communicate between locks and the relay node.
 A central server - used to control the locks. A user scan the QR code in the lock and send the information with his login details to the server. Then the server will unlock the relevant lock and start to track the bicycle using the GPS system of the mobile using the given mobile app. It maintains a database of users and bicycles.
 Back End and Front End
 Back end
 Will use Node.js as the server side language
 Mongodb as database management system
 Heroku cloud application platform
 Front end
 Web interface for administrational usage
 HTML, CSS, Javascript
 Mobile Application
 Android studio
 Connecting components through APIs
 REST API
 Use to exchange information among components ( lock and mobile app)
 Google Maps API
 Will use to show to location of the bicycle
 Barcode API
 Will use to parse the QR code with different format
 Network Security
 Sensitive data
 Detail of users are stored in central server. Mobile app is used to login to the system. These login requests need to be secure.
 Passwords of users need to be stored in hash representation.
 Controlling responses from server should be secured
 Security features
 Encrypting the requests and responses.
 Encouraging users to use a strong password.
 Hardware and Software Designs
 Technologies used:
 React
 Redux
 Electron
 Material UI
 Overall system design
 Documents
 Project Report
 User Manual & Technical Note
 Links
 Project Repository
 Project Page
 Department of Computer Engineering
 University of Peradeniya
 Back to top","As the most of the universities have wide area of land, transportation within the university causes time waste, accidents, congestion because of using the private vehicles, parking problems and the energy consumption related to the mobility of workers and students of the universities. The bicycle sharing programs have received increasing attention in recent years with initiatives to increase bike usage,better meet the demand of a more mobile public and lessen the environmental impacts of our transportation activities. So the project aims to introduce automated bike sharing system to minimize above impacts while evaluating the mobility patterns of academic campuses and assessing the energy consumption and pollutant emissions produced by the universities.",E14,Cyber-Physical Systems Projects,https://projects.ce.pdn.ac.lk/3yp/e14/Automated-Bike-Sharing-System/,https://github.com/cepdnaclk/e14-3yp-Automated-Bike-Sharing-System,https://cepdnaclk.github.io/e14-3yp-Automated-Bike-Sharing-System,https://cepdnaclk.github.io/e14-3yp-Automated-Bike-Sharing-System/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/3yp/E14/Automated-Bike-Sharing-System/
23,Automated Electricity Billing System,"Automated Electricity Billing System
 Projects - Department of Computer Engineering
 University of Peradeniya, Sri Lanka
 Automated Electricity Billing System
 Team
 E/14/018, ANURADHA WIJEWICKRAMA, e14018@ce.pdn.ac.lk
 E/14/068, DE SILVA N.G.M.H.K, e14018@ce.pdn.ac.lk
 E/14/243, Name, e14018@ce.pdn.ac.lk
 Table of Contents
 Introduction
 Solution Architecture
 Hardware & Software Designs
 Links
 Introduction
 In present world embedded systems have become essential as every electronic and
 electrical day to day devices use the particular system for it's functionality.
 Here our target is to achieve an automated and centralized electricity billing system with more security and efficiency.
 Functionalities of the new system
 * Accurate meter reading
 * Can get update about the connection frequently
 * Digital display for customers
 * Customer can identify the trend of usage of the electricity
 Solution Architecture
 CO321
 -
 It's planned to implement a new device to measure the current,
 to transmit the data to the regional office and to display the certain facts to the customer using embedded systems.
 CO324
 -
 Data is to be transmitted to the regional office and from regional units then to a centralized server.
 CO325
 -
 Data has to be secured as a third party or the customer may try to alter. Device also has to be secured.
 Hardware and Software Designs
 Implementation of current measuring system
 When setting up the unique id display shows some relevent detalis
 Links
 Project Repository
 Project Page
 Department of Computer Engineering
 University of Peradeniya
 Back to top",    In present world embedded systems have become essential as every electronic and electrical day to day devices use the particular system for it's functionality.     Here our target is to achieve an automated and centralized electricity billing system with more security and efficiency.  Functionalities of the new system          * Accurate meter reading     * Can get update about the connection frequently     * Digital display for customers     * Customer can identify the trend of usage of the electricity,E14,Cyber-Physical Systems Projects,https://projects.ce.pdn.ac.lk/3yp/e14/Automated-Electricity-Billing-System/,https://github.com/cepdnaclk/e14-3yp-Automated-Electricity-Billing-System,https://cepdnaclk.github.io/e14-3yp-Automated-Electricity-Billing-System,https://cepdnaclk.github.io/e14-3yp-Automated-Electricity-Billing-System/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/3yp/E14/Automated-Electricity-Billing-System/
24,Automated Fishing Bot,"Automated Fishing Bot
 Projects - Department of Computer Engineering
 University of Peradeniya, Sri Lanka
 Automated Fishing Bot
 Team
 E/14/222, MEDAWATTE R.C., e14222@ce.pdn.ac.lk
 E/14/317, SENANAYAKE S.M.A.J., e14317@ce.pdn.ac.lk
 E/14/390, WIJEKOON D.W.M.M.P., e14390@ce.pdn.ac.lk
 Table of Contents
 Introduction
 Solution Architecture
 Hardware & Software Designs
 Links
 Introduction
 Fishery Industry has not always been friendly to fishermen. Identifying high density fishing areas, reaching there and spending hours to find and capture fish is just too much for a human being. Therefore, we are going to introduce a fishing bot. This bot will be able to handle above said difficulties and will help fishermen to expand fishing areas and monitor fishing information gathered which will eventually help the industry.
 Solution Architecture
 Hardware and Software Designs
 Hardware : includes Bots, Relay nodes and the Central Server
 Software : includes the website, Database, and a local interface for the relay node
 Links
 Project Repository
 Project Page
 Department of Computer Engineering
 University of Peradeniya
 Back to top"," Fishery Industry has not always been friendly to fishermen. Identifying high density fishing areas, reaching there and spending hours to find and capture fish is just too much for a human being. Therefore, we are going to introduce a fishing bot. This bot will be able to handle above said difficulties and will help fishermen to expand fishing areas and monitor fishing information gathered which will eventually help the industry. ",E14,Cyber-Physical Systems Projects,https://projects.ce.pdn.ac.lk/3yp/e14/Automated-Fishing-Bot/,https://github.com/cepdnaclk/e14-3yp-Automated-Fishing-Bot,https://cepdnaclk.github.io/e14-3yp-Automated-Fishing-Bot,https://cepdnaclk.github.io/e14-3yp-Automated-Fishing-Bot/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/3yp/E14/Automated-Fishing-Bot/
25,Automated Greenhouse Fertilizing System,"Automated Greenhouse Fertilizing System
 Projects - Department of Computer Engineering
 University of Peradeniya, Sri Lanka
 Automated Greenhouse Fertilizing System
 Team
 E/14/287, R.M.K.D Rathnayake, e14287@ce.pdn.ac.lk
 E/14/335, K.R.W.R Subasinghe, e14335@ce.pdn.ac.lk
 E/14/402, G.A.A.M.B Wimalasena, e14402@ce.pdn.ac.lk
 Table of Contents
 Introduction
 Solution Architecture
 Links
 Introduction
 This project aims to calculate the component(N , K , P) levels which are responsible for the growth of the plant in a real time basis and maintain the component level which is required for the relevant plant. Using this system we can increase the lifetime of a plant and can get the maximum harvest out of the plant.
 In the traditional system we won’t get the perfect component levels as we are not checking the current component level. (We are adding the same amount of water or fertilizers to the every plant irrespective of the component level in that plant).
 First using sensors we will get the current component level of each and every plant. Then the data will be transmitted to a centralized server. Then the server will check whether the component level of each plant are up to the required level. For different varities of plants the component levels are also differ. If a component level of a certain plant is not up to the required level the server will send a command
 to the automated pipeline system asking it to send the relevant amount of required components through the pipeline to the relevant plant.
 Solution Architecture
 Project Plan
 Embedded Systems Design
 PH sensors and Humidity sensors are used to measure the PH level and the humidity level of the soil in a particular plant.
 Temperature sensors are used to measure the temperature within the greenhouse.
 Node mcu is used to transfer data from the sensors to the server.
 Arduino uno will be used to control the fertilizer dispersion system.
 Links
 Project Repository
 Project Page
 Department of Computer Engineering
 University of Peradeniya
 Back to top"," This project aims to calculate the component(N , K , P) levels which are responsible for the growth of the plant in a real time basis and maintain the component level which is required for the relevant plant. Using this system we can increase the lifetime of a plant and can get the maximum harvest out of the plant.",E14,Cyber-Physical Systems Projects,https://projects.ce.pdn.ac.lk/3yp/e14/Automated-Greenhouse-Fertilizing-System/,https://github.com/cepdnaclk/e14-3yp-Automated-Greenhouse-Fertilizing-System,https://cepdnaclk.github.io/e14-3yp-Automated-Greenhouse-Fertilizing-System,https://cepdnaclk.github.io/e14-3yp-Automated-Greenhouse-Fertilizing-System/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/3yp/E14/Automated-Greenhouse-Fertilizing-System/
26,Automated Monitoring of Hospital Patients,"Automated Monitoring of Hospital Patients
 Projects - Department of Computer Engineering
 University of Peradeniya, Sri Lanka
 Automated Monitoring of Hospital Patients
 CHATHURANGI E.	E/14/054
 WITHANAGE K.	E/14/413
 DISSANAYAKE D.M.S.N.B	E/14/084
 Team
 E/14/054, CHATHURANGI E., e14054@ce.pdn.ac.lk
 E/14/413, WITHANAGE K.	, e14413@ce.pdn.ac.lk
 E/14/084, DISSANAYAKE D.M.S.N.B, e14084@ce.pdn.ac.lk
 Table of Contents
 Introduction
 Solution Architecture
 Hardware & Software Designs
 Links
 Introduction
 This study develops a remote monitoring diagnostic framework to detect condition of patients in real-time which helps avoiding potential diseases.This system will be basically used in hospitals and medical centers. The proposed system has an embedded micro controller connected to a set of medical sensors (the sensors would differ according to the condition of the patient) and a wireless communication module (Bluetooth) . Each bed in the ward
 is considered as a node in a wireless sensor network and connected to a central node installed at the main monitoring location in the ward through an internet connection. The embedded micro controller checks if the patient health status is going well or not by analyzing the scanned medical signals. If even one of the sensing values goes out of the range, the monitoring system will indicate with an alarm that the particular patient is under a serious condition. If the condition is critical, then the system will automatically alarm the on-call doctor through a message. The medical report of each patient will be recorded separately and the overall details of each ward will be monitored by the hospital board. This system will be also helpful in reducing all the paper work done at hospitals as the records of each patient by the time he gets checked in till time he gets discharged will be recorded. The following aspects will be addressed by the system mainly.
 *Remote monitoring
 *Identifying emergency situations
 *Low cost
 *Data acquisition efficiency
 *Emergency data report
 *Scalability
 Solution Architecture
 Hardware and Software Designs
 Watch
 Links
 Project Repository
 Project Page
 Department of Computer Engineering
 University of Peradeniya
 Back to top",This study develops a remote monitoring diagnostic framework to detect condition of patients in real-time which helps avoiding potential diseases.This system will be basically used in hospitals and medical centers. The proposed system has an embedded micro controller connected to a set of medical sensors (the sensors would differ according to the condition of the patient) and a wireless communication module (Bluetooth) . ,E14,Cyber-Physical Systems Projects,https://projects.ce.pdn.ac.lk/3yp/e14/Automated-Monitoring-of-Hospital-Patients/,https://github.com/cepdnaclk/e14-3yp-Automated-Monitoring-of-Hospital-Patients,https://cepdnaclk.github.io/e14-3yp-Automated-Monitoring-of-Hospital-Patients,https://cepdnaclk.github.io/e14-3yp-Automated-Monitoring-of-Hospital-Patients/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/3yp/E14/Automated-Monitoring-of-Hospital-Patients/
27,Automatic Speed Trap,"Automatic Speed Trap
 Projects - Department of Computer Engineering
 University of Peradeniya, Sri Lanka
 Automatic Speed Trap
 Team
 E/14/349, THILAKARATHNA B.R.M., e14349@ce.pdn.ac.lk
 E/14/238, PATHIRAJA P.M.R.I., e14238@ce.pdn.ac.lk
 E/14/352, THILAKAWANSHA B.M.C.P., e14352@ce.pdn.ac.lk
 Table of Contents
 Introduction
 Solution Architecture
 Hardware & Software Designs
 Links
 Introduction
 Nowadays the transpotation is achieving developments in every alongside. Building highways and developing vehicles with new technologies are two side of developments. As a result of that, the vehicles could be achieved more speed than back days and this highspeed might causes unexpected accidents and might damage human life.
 So in this project we are expected to track the vehicle speed and if there any vehicle that travelling with more than maximum speed allowed, take a clear photo of the specific vehicle and send that to authorities to take further action.
 Solution Architecture
 CO321 - Using the embedded system we are expected to measure current speeed of the vehicles. And according to the pre-specified speed limit, check whether the vehicle travels with high speed and if then, using a specific carmera take a clear picture of the vehicle.
 CO324 - The photo that took on a specific vehicle, transmitted to a centralized server with the vehicle speed when it was captured. In addition to that few more details will be sent, such as date, time etc.
 CO325 - All the transmition are happeninig via encrypted method. Then any other parties cannot access and change the value.
 Hardware and Software Designs
 The overall design can be drawn as below.
 Here we have used an amplification system Doppler Sensor Module. That is because the output of the sensor is not enough to measure. The designed circuit and implementation
 are shown below.
 Documents
 Project Design
 User’s Manual
 Project Report
 Links
 Project Repository
 Project Page
 Department of Computer Engineering
 University of Peradeniya
 Back to top","In this project we are expected to track the vehicle speed and if there any vehicle that travelling with more than maximum speed allowed, take a clear photo of the specific vehicle and send that to authorities to take further action.",E14,Cyber-Physical Systems Projects,https://projects.ce.pdn.ac.lk/3yp/e14/Automatic-Speed-Trap/,https://github.com/cepdnaclk/e14-3yp-Automatic-Speed-Trap,https://cepdnaclk.github.io/e14-3yp-Automatic-Speed-Trap,https://cepdnaclk.github.io/e14-3yp-Automatic-Speed-Trap/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/3yp/E14/Automatic-Speed-Trap/
28,Bus tracking system,"Bus tracking system
 Projects - Department of Computer Engineering
 University of Peradeniya, Sri Lanka
 Bus tracking system
 Team
 E/14/181, KAVINDA T.B.D.H., e14181@ce.pdn.ac.lk
 E/14/178, KARUNASINGHE Y.K., e14178@ce.pdn.ac.lk
 E/14/298, RUWANTHA J.M.D.J., e14298@ce.pdn.ac.lk
 Table of Contents
 Introduction
 Solution Architecture
 Hardware & Software Designs
 Links
 Introduction
 A lot of people do not like to travel in crowded buses. We would love to travel sitting, sleeping. It will be very useful if there is a system to know whether the next bus on our route is full or not, to know the location of that bus, to know whether there are any alternative buses on route and etc.
 So we came up with a solution which is an embedded system and huge network of various devices. Real-time bus data, which comes from the bus tracking system, shows the expected time in minutes until the arrival of the relevant bus. Customers can view this anticipated arrival time at a location via an app, website or, where available on the public information display system at the bus stop.
 The main purpose in this idea is to check whether the buses are heavily loaded and late. So that if a person is willing to use another way of transportation, he can use that considering the information given by the system.
 After all, the collected data will be very valuable also. They can be used to analyse and check which route has heavy load of people, whether the time differences among buses should be adjusted and should there be more buses in a route in a particular time and any more.
 Solution Architecture
 CO321 – The relevant parameters will be measured using sensors such as IR, pressure sensors and GPS trackers and processed raw data using AVR controller.
 CO322 – The data will be transferred to a centralized server via Wi-Fi or using a GSM module. Those processed data can be viewed by an app or through a web interface.
 CO325 – The data will be sent encrypted not to have them seen by a third party before reaching the server.
 Hardware and Software Designs
 Project Proposal
 Project Design
 Links
 Project Repository
 Project Page
 Department of Computer Engineering
 University of Peradeniya
 Back to top","A lot of people do not like to travel in crowded buses. We would love to travel sitting, sleeping. It will be very useful if there is a system to know whether the next bus on our route is full or not, to know the location of that bus, to know whether there are any alternative buses on route and etc. So we came up with a solution which is an embedded system and huge network of various devices.",E14,Cyber-Physical Systems Projects,https://projects.ce.pdn.ac.lk/3yp/e14/Bus-tracking-system/,https://github.com/cepdnaclk/e14-3yp-Bus-tracking-system,https://cepdnaclk.github.io/e14-3yp-Bus-tracking-system,https://cepdnaclk.github.io/e14-3yp-Bus-tracking-system/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/3yp/E14/Bus-tracking-system/
29,Control System for Heliostat Solar Power Plants,"Control System for Heliostat Solar Power Plants
 Projects - Department of Computer Engineering
 University of Peradeniya, Sri Lanka
 Control System for Heliostat Solar Power Plants
 Team
 E/14/233, NIROSHANA T.M.T., e14233@ce.pdn.ac.lk
 E/14/314, SENANAYAKA S.M.M.K.S , e14314@ce.pdn.ac.lk
 E/14/322, SENEVIRATHNE S.D., e14322@ce.pdn.ac.lk
 Table of Contents
 Introduction
 Solution Architecture
 Links
 Introduction
 This is a system which focus sunlight into a one point in a solar tower and heat up the salts in it upto higher temperatures and then use them to store heat energy and produce steam and generate electricity. These systems make it possible to supply power even when the sun is down because of the stored heat energy. In these power plants, array of flat movable mirrors called heliostats are used to focus sunlight into a collector tower to heat salts and generate electricity through steam turbines. This is seen as
 a viable solution for renewable energy.
 Angle of heliostat is very critical in these solar power plants since the temperature of the tower will be significantly rely on the concentration of sunlight focused on it.
 Solution Architecture
 CO321 - The position of the sun is calculated by the local server in the solar tower and will be broadcast to the network via WiFi interface. Then the other nodes (heliostats) receives data and adjust them according to their relative position and give the control signals to the motors to rotate the heliostats. Initial inclination of the heliostats will be sensed through a gyroscope. So after a successful turn a feedback signal will be sent to the local server which could be used to sense malfunctions.
 CO324 - Usually these farms has nearly 2000 heliostats pointing one point. So sending a feedback to local server could induce a network conjunction. Data from the local server will be sent to the centralized server in real time or in short intervals from many farms owned by the same company throughout the country. So the network traffic which could occur in the systems will be considered in the design and it will be designed to minimize the drawbacks and control the overall MO of the system. Also the monitoring system can be used for maintenance scheduling by getting status feedbacks from the equipments, which adds an important feature to the implementation from adding network aspect.
 CO325 - The feedback data sent from local servers of the farms to the centralized server are sent as encrypted data as this implementation mainly focuses on R&D and the collected data is a valuable asset and they have a market value. Therefore, it needs to be encrypted and security becomes a key aspect in the implementation. Also the signals sent to each node is encrypted as outside parties can manipulate nodes to reduce the efficiency of the system by changing direction of the heliostats by giving wrong feedbacks.
 Links
 Project Repository
 Project Page
 Department of Computer Engineering
 University of Peradeniya
 Back to top","This is a system which focus sunlight into a one point in a solar tower and heat up the salts in it upto higher temperatures and then use them to store heat energy and produce steam and generate electricity. These systems make it possible to supply power even when the sun is down because of the stored heat energy. In these power plants, array of flat movable mirrors called heliostats are used to focus sunlight into a collector tower to heat salts and generate electricity through steam turbines. This is seen as  a viable solution for renewable energy.",E14,Cyber-Physical Systems Projects,https://projects.ce.pdn.ac.lk/3yp/e14/Control-System-for-Heliostat-Solar-Power-Plants/,https://github.com/cepdnaclk/e14-3yp-Control-System-for-Heliostat-Solar-Power-Plants,https://cepdnaclk.github.io/e14-3yp-Control-System-for-Heliostat-Solar-Power-Plants,https://cepdnaclk.github.io/e14-3yp-Control-System-for-Heliostat-Solar-Power-Plants/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/3yp/E14/Control-System-for-Heliostat-Solar-Power-Plants/
30,Networked and Automated Weather Monitoring and Alerting System,"Networked and Automated Weather Monitoring and Alerting System
 Projects - Department of Computer Engineering
 University of Peradeniya, Sri Lanka
 Networked and Automated Weather Monitoring and Alerting System
 Team
 E/14/009, ADIKARI A.M.H.I., e14009@ce.pdn.ac.lk
 E/14/404, WITHANA S.S.P., e14404@ce.pdn.ac.lk
 E/14/410, DILSHAN I.D.H.I., e14410@ce.pdn.ac.lk
 Table of Contents
 Introduction
 Solution Architecture
 Hardware & Software Designs
 Links
 Introduction
 Weather conditions and climate changes must be logged and analyzed by relevant authorities like meteorological department each day. They take relevant actions observing those records obtained allover the island. But in the current manual system they collect information only from main cities and
 taking responses manually takes much time and effort.
 So in this unified project we develop a system that can be used to collect information from much many places allover Sri Lanka using an embedded system kept in those locations. The locations may be places in each district close to sea, rivers, reservoirs, tanks and other areas. The data are wind speed, rain status, rainfall, humidity, temperature etc.
 All those data taken from those sensors, are sent to a central server and processed there. If there are extreme weather cases, public must be warned. So the web application gives instant alerts to it’s users including relevant central authorities as well as regional authorities. The process is very quicker than manual method. So the damages due to extreme weather is reduced as well as meteorological department will have newest data from allover the country.
 Introduction Video
 Solution Architecture
 Embedded Systems Design
 What will you be measuring and/or controlling?
 In this unified project we develop a system that can be used to collect information from much many places allover Sri Lanka using an embedded system kept in those locations. The locations may be places in each district close to sea, rivers, reservoirs, tanks and other areas. The data are wind speed, wind direction, rainfall, humidity, temperature.
 Actually we are not going to control any physical quantity since weather cannot be controlled. We are only sensing the weather conditions and reporting.But we have few cases where it is needed to do an physical action based on the sensed data.That is the valve we use to remove water from rainfall-meter must be open only if water is filled and closed when water is removed totally.So the ultra-sonic sensor data is used to control the valve.
 What embedded systems platform(s) will you be using, and why?
 The combined embedded system will have following main sub parts.
 Wind Speed Sensing System - This is a wind Vane that rotates with wind. We are making a mechanism using IR sensor to get the time taken to complete a full cycle by vane. The wind speed is calculated using that data.
 Wind direction measuring System
 Rainfall measuring System - This has a cylindrical vessel with fixed cross section that collects rain water and measure the water level using an ultrasound sensor kept few cm below top of the vessel. Sensor not having at top most is to avoid water drops to go away hitting the sensor.
 Temperature and Humidity measuring System - This is a single sensor that sense temperature and humidity.
 Microcontroller - We use Atmega328p microcontroller for each node connecting all the above mentioned sub parts.
 How does the system connect to the network?
 System connects to network using GSM module. we use SIM808 module for that.
 What peripheral components will you be using, and how do they work? How do they interface
 IR sensor used in wind speed sensing, magnetometer used in wind direction sensing, ultra-sonic sensor used in rainfall sensing and also the temperature/humidity sensor will connect to the Arduino UNO board.
 The automatic valve used to remove water when needed will also connect to the Arduino Uno.
 The SIM808 will connect to serial ports of arduino UNO (using two PMW enabled digital pins and Software Serial library).
 What are the limitations of those components?
 The magnetometer we bought doesn’t give reliable readings as we expected. That is a limitation.
 The ultra-sonic sensor give reliable readings only when the water surface doesn’t shake.
 what workarounds can you come up with in order to deal with those limitations?
 We have to remove magnetometer from sensing. We need to use another technology to gain the wind direction.
 Web and Network Application Design
 What network protocols and middleware will you use, why, and how do they work?
 We use SIM808 GSM module as middleware and use HTTP protocol to send data.
 TCP/IP is used to get the connection.
 What back-end and front-end technologies will you use, and how do they work?
 Back-end technology is PHP and Front-end technology is HTML5. PHP used to get URL encoded data and to send data to database sing MYSQL queries.
 What APIs will you use when connecting different parts of the system?
 GoogleMap APIs are to be used in getting map services to the user interface.
 Computer and Network Security
 Are there any sensitive data in your system that need to be secured?
 All the data are weather related data. They must be protected from unauthorized changes.
 How might an unauthorized third party obtain data from your system?
 Unauthorized third party could access the data and has no problem. As the data are visible to anyone easily on the front end, no one will need to access data
 unethically.
 How might an unauthorized third party manipulate your system?
 Unauthorized third party should not be allowed to manipulate the system. Reliability and accuracy of data must be maintained.
 What security features are you able to implement, and how?
 As the very important data are sent to the central server from systems in rural places, the security of data is a fact to consider much. As stated by the examiners of Project Milestone 1 in comments, data encryption is not required in this case because we
 not need to hide data from anyone. Only thing we need is to protect data from being changed externally. So we are going to use MAC -Message authentication code for the requirement.
 Hardware and Software Designs
 Embedded system
 The combined embedded system will have 6 main sub parts.
 Wind Speed Sensing System
 This is a wind Vane that rotates with wind. We are making a mechanism using IR sensor to get the time taken to complete a full cycle by vane. The wind speed is calculated using that data.
 The model wind vane concept is given below.
 Infrared barrier module is the IR sensor we are planning to use. It emits infra red waves and monitor the reflected rays. It can identify black and white screens.
 Wind direction measuring System
 Wind direction is measured using a Vane that has a Triple-axis Magnetometer (Compass) Board - HMC5883L0
 Below is the model.
 Triple-axis Magnetometer (Compass) Board - HMC5883L is the sensor used to get the angle of rotation.
 Rainfall measuring System
 This has a cylindrical vessel with fixed cross section that collects rain water and measure the water level using an ultrasound sensor kept few cm below top of the vessel. Sensor not having at top most is to avoid water drops to go away hitting the sensor.
 Also this has a automatic valve to remove water daily after reporting to server, based on the sonar sensor reading.
 Also this has a rain sensor to identify whether it is raining or not.
 This is the ultrasonic sensor we are planning to use (HC-SR04)
 This is the automatic valve we are using. It is a plastic solenoid valve
 Temperature and Humidity measuring System
 This is a single sensor that sense temperature and humidity.
 For that we use Humidity and Temperature DHT11 Module
 Microcontroller
 We use Atmega328p microcontroller for each node connecting all the above mentioned sub parts.
 We will use ARDUINO UNO board which has Atmega328p microcontroller.
 Network Module
 As we need the embedded unit at places normally we can’t expect WIFI access, we decided to use a GSM module to send data collected to the central server.
 So the GSM module we are using is
 sim808 module.
 WEB APPLICATION
 We are sending the data collected in each time to the central database. we have implemented the database in 000webhost.com. So we are implementing a web application for users to view the data log using a user interface.
 We are sending data using protocol HTTP. HTTP GET request is used to send data. The microcontroler will send requests to the SIM808 as AT commands using serial port. Software Serial is used for that as digital pins are used for serial data sending.
 Front end will run HTML5 and back end will run PHP maintaining the connection with the database. MYSQL is used for database managing.
 User interface will allow users to view parameters rainfall, wind direction, wind speed, temperature, humidity based on map(current situation/latest) also based on time(history).
 This photo is an example image.
 The design of the Back End programme of the web application is given below.
 NETWORK SECURITY
 As the very important data are sent to the central server from systems in rural places, the security of data is a fact to consider much. As stated by the examiners of Project Milestone 1 in comments, data encryption is not required in this case because we
 not need to hide data from anyone. Only thing we need is to protect data from being changed externally. So we are going to use MAC-Message authentication code for the requirement.
 Rainfall measuring System
 Wind Speed Sensing System
 Demonstrations
 This is the video depicting the operation of GSM/GPRS module to send weather data to the server at 000webhost.com and how basic front end display it.
 This is a video demonstration of the functioning of the Embedded system including the sensors, GSM module and web application. All the sensors are working fine now. Only few are demonstrated in the video.
 Documents
 Project Report
 Design Document
 Links
 Project Repository
 Project Page
 Department of Computer Engineering
 University of Peradeniya
 Back to top",In this unified project we develop a system that can be used to collect information from much many places allover Sri Lanka using an embedded system kept in those locations.,E14,Cyber-Physical Systems Projects,https://projects.ce.pdn.ac.lk/3yp/e14/Networked-and-Automated-Weather-Monitoring-and-Alerting-System/,https://github.com/cepdnaclk/e14-3yp-Networked-and-Automated-Weather-Monitoring-and-Alerting-System,https://cepdnaclk.github.io/e14-3yp-Networked-and-Automated-Weather-Monitoring-and-Alerting-System,https://cepdnaclk.github.io/e14-3yp-Networked-and-Automated-Weather-Monitoring-and-Alerting-System/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/3yp/E14/Networked-and-Automated-Weather-Monitoring-and-Alerting-System/
31,Real Time Water Qualtiy Measurement System,"Real Time Water Qualtiy Measurement System
 Projects - Department of Computer Engineering
 University of Peradeniya, Sri Lanka
 Real Time Water Qualtiy Measurement System
 Team
 E/14/142, INDIKA J.A.A., e14142@ce.pdn.ac.lk
 E/14/364, WARUSAMANA D.N., e14364@ce.pdn.ac.lk
 E/14/380, WELIKALA E.Y., e14380@ce.pdn.ac.lk
 Table of Contents
 Introduction
 Solution Architecture
 Links
 Introduction
 The traditional method adopted by environmental authorities in measuring the quality of water is to collect water samples from various locations and send them to the laboratories for for analysis, but this is inefficient and ineffective as it is time consuming, wasting human labour, and not economical. To get a analysis by observing previous records will also be difficult in this technique. If real time monitoring is required, this method is unable to provide expected outcomes as it may not be possible to take samples and send them immediately to the laboratories for analysis.
 The system consists of several sensors located in remote locations to measure the water quality real time (most probably near water resources around industrial environments). The information received by sensors are sent to a centralized system which is monitoring the water quality measurement through GSM modules. These information can be sent at a predefined interval of time and they can be analyzed from the central monitoring system. Moreover, this system is only accessed by the authorized personnel only who works at environmental authorities. Hence, the system is secured with access restrictions.
 Main objectives are as follows
 Cost effective
 Time saving
 Remote monitoring and centralized controlling
 Higher scalability
 Higher security
 Easy Report generation for analysis
 Power efficient/ Power saving
 Overview
 These are the main subject areas to be covered in each course as this is a joint project.
 CO321 - Power efficiency is planned to control mainly through the embedded system. Since, this system is operating real time power consumption has to be controlled efficiently. Micro-controller based system is utilized for this purpose. Design has to be focused on power efficiency.
 CO324 - Information sensed through sensors are sent to the centralized system containing a central server which handles all the data received from various sensors located at different locations. Since information are sent through various nodes simultaneously, there will be a heavy network traffic and it has to be handled efficiently by techniques to handle them.
 CO325 - The system is only accessible by the officers working in the environmental authorities and data transactions are secured so that they cannot be retrieved or intercepted by third parties. Information authentication and verification, secure data transactions are the main issues regarding security.
 Solution Architecture
 Overview of Infrastructure:
 The qualities that can be measured vs qualities we wish to measure are as follows:
 According to the Central Environmental Authority (CEA) of Sri Lanka and Environmental Protection Agency(EPA) of United States of America these are the typical water quality parameters measured.
 pH
 Conductivity
 Temperature
 Turbidity
 Dissolved Oxygen (DO)
 BOD - Biological Oxygen Demand
 COD - Chemical Oxygen Demand
 Dissolved Heavy Metals and their concentrations
 Fully or partially dissolved organic and inorganic compounds and their concentrations
 From these only qualities from 1) to 4) can be measured in this project since rest of the qualities involve deep chemical analysis of samples.
 Embedded Systems Design
 pH value of water is measured using a pH probe and it is connected to an end node connected to the system.
 Temperature is measured using an LM35 temperature sensor
 Turbidity sensor has to be made manually. For that the amount of light traveled through the water is measured. Either an LED or a laser is used as a light source and it is measured
 using a photo diode/LDR and the levels are calibrated accordingly into levels.
 Conductivity is measured using two electrodes connected at an end node of the system.
 Atmega MCU is used at each node and it is used to manipulate the power consumption by each sensor. Further, it used to control all the peripherals connected to the end node.
 All the above components are connected to make a single end node of the system.
 Web and Network Application Design
 A GSM module connected at each end is used to communicate between the node and the central server.
 GPRS is used to transfer data packets from the end node to the central server via the Internet.
 TCP/UDP are used as Transport Layer protocols and IP is used as Network Layer Protocol.
 Back end
 The following technologies are used to develop the back end web application
 MySQL
 PHP or Node.js
 Front end
 HTML
 CSS
 JavaScript
 Bootstrap
 Computer and Network Security
 Sensitive data of this system include the following.
 1) Monitored data from each node - They have to be protected.For instance, an industry owner which may be releasing harmful substance to water resources may intercept the data transfer from a node situated in a water source near the industry and send fake data to cover up their actions
 2) User information stored in the central server
 3) Login requests to the central server
 Most of the above sensitive data can be encrypted to ensure security.
 Project overview video
 Documents
 A detailed description of infra structure
 User Manual
 Project Design
 Links
 Project Repository
 Project Page
 Department of Computer Engineering
 University of Peradeniya
 Back to top",The system consists of several sensors located in remote locations to measure the water quality real time.,E14,Cyber-Physical Systems Projects,https://projects.ce.pdn.ac.lk/3yp/e14/Real-Time-Water-Qualtiy-Measurement-System/,https://github.com/cepdnaclk/e14-3yp-Real-Time-Water-Qualtiy-Measurement-System,https://cepdnaclk.github.io/e14-3yp-Real-Time-Water-Qualtiy-Measurement-System,https://cepdnaclk.github.io/e14-3yp-Real-Time-Water-Qualtiy-Measurement-System/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/3yp/E14/Real-Time-Water-Qualtiy-Measurement-System/
32,River water level and speed monitoring and alert system,"River water level and speed monitoring and alert system
 Projects - Department of Computer Engineering
 University of Peradeniya, Sri Lanka
 River water level and speed monitoring and alert system
 Team
 E/14/080, DILSHANI T.H.K., e14080@ce.pdn.ac.lk
 E/14/228, MUNASINGHE S.L., e14228@ce.pdn.ac.lk
 E/14/240, PAVITHYA M.B.D., e14240@ce.pdn.ac.lk
 Table of Contents
 Introduction
 Solution Architecture
 Hardware & Software Designs
 Links
 Introduction
 This is a system that will monitor the water levels of a river/lake/stream and predicts the occurrence of a flood and generate an alert accordingly. This system would not only measure the water level but also the water velocity in order to give a better prediction of the risky situations.
 Background work
 Currently, this site gives an update of the river status of Sri Lanka. However this system is based on manually collected data and is updated only once per day. By constantly keeping track of this site we found that there are certain times that the system doesn’t output any data at all. Due to these reasons we proposed a sensor based flood alert system through water level and velocity monitoring.
 This project will be covering aspects from embedded systems (CO321), Network and web application design (CO324), Computer and network security (CO325)
 Intro Video
 Solution Architecture
 As this is a unified project, this project is consisted of aspects of embedded systems, networking and security.
 CO 321 - Sensors are used to measure the water level and to detect the speed of water flow. An equipment with the above mentioned capabilities would be built to analyse those parameters real time and to capture the data.
 CO 324 - The captured data would be sent to a centralized server in which a warning would be generated if a risk of a flood situation is seen.
 CO 325 - The network system should be encrypted and the alert system should be accessed only by the authorized personnel.
 Hardware and Software Designs
 Embedded Systems Design
 We would be measuring the water level of streams/rivers/lakes and the velocity of the water flow of such bodies. To measure the water level we are using ultra sonic sensors which should be implemented above the water body and the sensor would output the distance from that given body to the surface of the water. Through this we could check whether there’s a significant change in the water level by comparing this data with the pre-collected data. There were other sensing methods to get the water level but we chose ultra-sonic sensors as the best due to its accuracy when compared with other methods and its easy-maintainability since the components do not get in touch with the water (no rusting, less depreciation)
 The flow rate sensor has to be emerged somewhere below the surface (according to the principles of hydro physics as at this point a more accurate measurement regarding flow rate of a river could be obtained) The flow meter works on the principle of the Hall effect. According to the Hall effect, a voltage difference is induced in a conductor transverse to the electric current and the magnetic field perpendicular to it. Here, the Hall effect is utilized in the flow meter using a small fan/propeller-shaped rotor, which is placed in the path of the liquid flowing. The liquid pushes against the fins of the rotor, causing it to rotate. The shaft of the rotor is connected to a Hall effect sensor. It is an arrangement of a current flowing coil and a magnet connected to the shaft of the rotor, thus a voltage/pulse is induced as this rotor rotates. In this flow meter, for every liter of liquid passing through it per minute, it outputs about 4.5 pulses. This is due to the changing magnetic field caused by the magnet attached to the rotor shaft as seen in the picture below. We measure the number of pulses using a micro-controller.
 We chose Arduino UNO as the micro-controller because of its moderate price over other micro-controllers and because it serves our purpose. We just need to get the output signals of our sensors and some simple computations, so arduino UNO would suffice.
 We’d be also using an A7 GSM module to capture the signals from the micro-controller and to transfer these signals to the centralized server.
 Interface of the system
 Limitations
 Ultra-sonic sensor : The height from the surface to the fixed point can be obtained only with an accuracy of +-1cm. But since we do not need the minute changes of the water level and measure only the drastic changes, this error could be tolerated. With time, there might be a possibility to have a moisture layer on the face of the sensors and that’d change the density between the surface and might lead to erroneous results.
 Flow-rate sensor : Since the flow rate is embedded within a tube, the friction and the cohesive forces exerted by the walls of the tube would hinder obtaining the exact flow rate at the point. But since the velocity of a flowing water body is not uniform, there’s no point in trying to get the exact velocity. We can position the flow rate sensor in a place the maximum velocity could be measured (somewhere just beneath the surface) and that would be effective than getting the exact value since this measurement is going to be an average estimation of the velocity.
 Web and Network Application Design
 Protocols and middle ware
 Http/Https : To access the web application
 TCP/IP
 UDP : for DNS
 GPRS protocol
 Centralized web server: This centralized web server will maintain the main database and will be connecting to the web interface and the mobile application. The server should also manage the accounts of the administration.
 Back end technologies
 000Webhost(might switch to a cloud platform like heroku)
 Laravel
 Front end technologies
 Web Interface:
 HTML
 CSS
 Bootstrap
 JavaScript
 Mobile Application:
 Android Studio
 We’d be using Google maps API in the web interface in order to show the location based river status of the country.
 Computer and Network Security
 There are no sensitive data on our system,as anyone outside viewing the data won’t do any harm (since it is anyway accessible by the public through the web interface and the data contains only the measurement of water level and velocity) . But there should not be a
 possibility that any unauthorized 3rd party manipulating the data. The server protection should be taken into serious consideration and
 therefore authentication of the server could be done.
 Power Supply
 Threshold values
 The finalized threshold values used in the safety prediction application :
 In order to generate the danger alerts with respect to a river body, knowing the velocity alone won’t be sufficient. In order to give the maximum accuracy we are improving the system by taking many other factors into consideration. The safety of a river body would vary with aspects like depth and the level of height that is submerged by a man.
 In waist deep water it takes roughly 2-2.5 feet per second to push a man over a water body and in chest deep water it’s roughly about 1-1.5 feet per second.
 The threshold values may vary due to the depth of the water body also.
 Documents
 See for the attached pdf for the final Plan and design blue prints. This includes all the fine details regarding hardware design,network design,database design,the technologies used and the security aspects. This also covers the budget and the timeline plan.
 Project Plan
 Links
 Project Repository
 Project Page
 Department of Computer Engineering
 University of Peradeniya
 Back to top",This is a system that will monitor the water levels of a river/lake/stream and predicts the occurrence of a flood and generate an alert accordingly. This system would not only measure the water level but also the water velocity in order to give a better prediction of the risky situations.,E14,Cyber-Physical Systems Projects,https://projects.ce.pdn.ac.lk/3yp/e14/River-water-level-and-speed-monitoring-and-alert-system/,https://github.com/cepdnaclk/e14-3yp-River-water-level-and-speed-monitoring-and-alert-system,https://cepdnaclk.github.io/e14-3yp-River-water-level-and-speed-monitoring-and-alert-system,https://cepdnaclk.github.io/e14-3yp-River-water-level-and-speed-monitoring-and-alert-system/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/3yp/E14/River-water-level-and-speed-monitoring-and-alert-system/
33,Smart Breathalyzer Test,"Smart Breathalyzer Test
 Projects - Department of Computer Engineering
 University of Peradeniya, Sri Lanka
 Smart Breathalyzer Test
 Team
 E/14/010, ADIKARI A.M.N.P., e14010@ce.pdn.ac.lk
 E/14/028, BANDARA D.M.N.T., e14028@ce.pdn.ac.lk
 E/14/065, DASSANAYAKA D.S.M.M.B., e14065@ce.pdn.ac.lk
 Table of Contents
 Introduction
 Solution Architecture
 Hardware & Software Designs
 Links
 Introduction
 This project is mainly targeted for the Police Department in Sri Lanka. Until now they use conventional balloon test to identify drunk drivers. Providing an easy , efficient
 and more secure iot based solutions replacing old conventional method our aim is to reduce the chance of escaping such situations by bribing.
 Our project is to innovate a digital alcohol meter equipment communicating with a secured database-system with a user friendly GUI. We are aiming to produce two of such devices. These devices interact with the remote central server along with other local servers which will be located at regional police departments.These devices has their own ids. This method is fully automated such that the police officer has no any authority over the device. Once the police officer checks with the equipment the device automatically does everything to file a legal action against the drunk driver. And also we are developing our database such that only the higher level authority can interact with the informations. Once the data is sent to the remote server it sends a reply to the device requesting driver and vehicle informations which can be entered using the keypad included into the device itself. It is the only thing the police officer has to do.
 For this project basically we intend to use
 alcohol sensor for each device
 microcontroller (arduino)
 a keypad
 A GSM module
 Solution Architecture
 Embedded Systems Design
 Alcohol percentage in breath is measured using the MQ3 gas sensor which is calibrated to response to alcohol gas.
 Temperature and humidity sensors are used in the aim of getting accurate results in any changing environment.
 Design of the Smart breathalyzer device is completely innovated by us.
 Device is embedded with an Arduino uno, MQ3 sensor along with temperature and humidity sensors and a GSM/GPRS module to communicate with the server.
 A keypad, lcd display is attached to the module itself.
 Design templates and final outcome of the Embedded systems will be followed by this post.
 Web and Network Application Design
 A GSM module connected at each end is used to communicate between the device and the central server.
 GPRS is used to transfer data packets from the end node to the central server via the Internet.
 TCP is used as Transport Layer protocols and IP is used as Network Layer Protocol.
 Back end
 The following technologies are used to develop the back end web application
 Node.js with Express
 Free web hosting service like AWS
 Front end
 Javascript supported Frontend Framework such as Vue.js, AngularJS or REACT
 Computer and Network Security
 Driver License Number and Vehicle Number are the data that has to be sent to the central server from the device. Therefore there is not much security aspects involved in the device aspect.
 But when considering server
 No one should be able to edit, delete, or change data
 Accessing website should be secured (Only authorized persons should enter)
 Should establish a secure connection
 between device and server such that no any external device can enter data to server.
 Hardware and Software Designs
 Smart Breathalyzer Design
 3D Print Output
 Final Product Demo Video
 Documents
 Project Document
 Design Document
 Links
 Project Repository
 Project Page
 Department of Computer Engineering
 University of Peradeniya
 Back to top",This project is mainly targeted for the Police Department in Sri Lanka. Our project is to innovate a digital alcohol meter equipment communicating with a secured database-system with a user friendly GUI.,E14,Cyber-Physical Systems Projects,https://projects.ce.pdn.ac.lk/3yp/e14/Smart-Breathalyzer-Test/,https://github.com/cepdnaclk/e14-3yp-Smart-Breathalyzer-Test,https://cepdnaclk.github.io/e14-3yp-Smart-Breathalyzer-Test,https://cepdnaclk.github.io/e14-3yp-Smart-Breathalyzer-Test/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/3yp/E14/Smart-Breathalyzer-Test/
34,Smart Shopping Cart,"Smart Shopping Cart
 Smart Shopping Cart
 Home
 About
 Solution Architecture
 Implementation
 Team
 Supervisors
 More..
 budget
 Timeline
 Mobile App
 Desktop App
 Testing
 Get Started
 Better Experiance with Smart Shopping Cart
 Make your shopping more easy
 Get Started
 Watch Video
 Introduction
 Smart Shopping Cart is an innovative consumer purcjasing product that is designed to help shoppers' fast-track their shopping experince. The concept of this smart cart will revolutionize the purchasing experience
 of every buyer.
 Problem
 Long waiting queues
 Waste time on searching products
 Forget to buy products
 Safety and health during a pandemic
 Difficult to display offers
 Lot of labour for scanning and billing process
 Our Solution
 Customer can purchase the product by just scanning the barcodes printed on the products before adding it to cart
 Mobile app keeps adding the items in list and the total amount is updated accordingly
 Customers can scan and remove any item
 LCD will show the improved bill at each instance
 Billing will be done automatically
 Customers can do the bill payment through their preferred payment method
 Customers will be able to view their digital receipts via app
 Solution Architecture
 Solution based on our project idea
 Data Path
 In our system have many data transfer process this is the data transfer flow in software, hardware and server
 Overview
 These are our implementations for our project
 Mobile App
 Mobile Application developed for the Customer who uses our smart shopping cart
 Web App
 Web Application developed for the Admin and staffs they can manage the activity and transactions.
 Hardware
 Our Hardware system will be placed on the shopping cart
 Testing
 Software and Hardware tesing
 BUDGET
 Timeline
 Our project plan timeline
 Timeline
 We planed to improve the project in this time flow
 Team
 We are the members who develop this projects
 Rilwan M.M.M
 E/17/292
 Studing at university of peradeniya
 Kavinaya Y
 E/17/159
 Studing at university of peradeniya
 Piriyaraj s
 E/17/256
 Studing at university of peradeniya
 SUPERVISORS
 The supervisors who give guidance for the project
 Dr. Isuru Nawinne
 Senior Lecturer
 Dr. Mahanama Wickramasinghe
 Senior Lecturer
 © Copyright SMART SHOPPING CART. All Rights Reserved
 Designed by
 KRP ROCKERS",Trolleys are used in supermarket by customers in the modern world. but they remain in the same state without many improvements. Our target is to develop a smart trolley which satisfies all needful of customers.  ,E14,Cyber-Physical Systems Projects,https://projects.ce.pdn.ac.lk/3yp/e14/Smart-Shopping-Cart/,https://github.com/cepdnaclk/e14-3yp-Smart-Shopping-Cart,https://cepdnaclk.github.io/e14-3yp-Smart-Shopping-Cart,https://cepdnaclk.github.io/e14-3yp-Smart-Shopping-Cart/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/3yp/E14/Smart-Shopping-Cart/
35,Smart Warehouse Monitoring for Paddy Storage,"Smart Warehouse Monitoring for Paddy Storage
 Projects - Department of Computer Engineering
 University of Peradeniya, Sri Lanka
 Smart Warehouse Monitoring for Paddy Storage
 Team
 E/14/336, SUCHINTHANA A.P.N., e14336@ce.pdn.ac.lk
 E/14/011, AGALAKUMBURA S.T., e14011@ce.pdn.ac.lk
 Table of Contents
 Introduction
 Solution Architecture
 Links
 Introduction
 Networked embedded systems have become
 quite important nowadays, especially for monitoring and control of distance and dislocated objects. This concept is applied in many fields in today as it increases the accuracy rather than controlling manually.
 Therefore we thought to implement a embedded system to monitor and control physical quantities of a rice paddy storage. Controlling such conditions precisely will ensure the best quality. This will helpful to the businessmen who having number of such stores in many locations as it is hard to control them manually. So by using this system they can operate it easily and being in any location.
 This system can be used in storage by changing the desired physical quantities.
 Features
 Monitoring and controlling physical quantities
 Automatically controlling physical quantities up to given preset value
 Maintaining a inventory system
 Project Complexity
 v
 Embedded system to
 Ø
 Sense the physical quantities
 Ø
 Control the physical quantities
 Ø
 Transfer data
 v
 Web interface to monitor and control manually.
 Each store have their own node and they are remotely operated.
 Solution Architecture
 ·
 ♦ CO321 – The desired quantities are measured using sensors and the values are transmitted through a micro-controller to the server and the monitoring system(on pre-defined time interval/ user request). Those conditions are automatically controlled up to preset value. Other than that quantities can be controlled manually.
 ·
 ♦ CO324 – Data from sensors and controllers are sent to a centralized server using GSM module as a SMS. Since there are many nodes, network traffic get high. It is concerned during design process to avoid drawbacks.
 ·
 ♦CO325 – The feedback data and control data is sent as encrypted to ensure the security. Otherwise any unauthorized parties can change the message(data) unethically. This can occur a huge loss. Therefore ensuring the security is must to have a good market value
 to our system.
 Hardware and Software Designs
 Detailed designs with many sub-sections
 Links
 Project Repository
 Project Page
 Department of Computer Engineering
 University of Peradeniya
 Back to top","Networked embedded systems have become  quite important nowadays, especially for monitoring and control of distance and dislocated objects. This concept is applied in many fields in today as it increases the accuracy rather than controlling manually.  Therefore we thought to implement a embedded system to monitor and control physical quantities of a rice paddy storage. Controlling such conditions precisely will ensure the best quality. This will helpful to the businessmen who having number of such stores in many locations as it is hard to control them manually. So by using this system they can operate it easily and being in any location.  This system can be used in storage by changing the desired physical quantities.",E14,Cyber-Physical Systems Projects,https://projects.ce.pdn.ac.lk/3yp/e14/Smart-Warehouse-Monitoring-for-Paddy-Storage/,https://github.com/cepdnaclk/e14-3yp-Smart-Warehouse-Monitoring-for-Paddy-Storage,https://cepdnaclk.github.io/e14-3yp-Smart-Warehouse-Monitoring-for-Paddy-Storage,https://cepdnaclk.github.io/e14-3yp-Smart-Warehouse-Monitoring-for-Paddy-Storage/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/3yp/E14/Smart-Warehouse-Monitoring-for-Paddy-Storage/
36,Smart Waste Disposal Monitoring System,"Smart Waste-Disposal Monitoring System
 Projects - Department of Computer Engineering
 University of Peradeniya, Sri Lanka
 Smart Waste-Disposal Monitoring System
 Team
 E/14/122, Ahmedh J.R., e14122@ce.pdn.ac.lk
 E/14/213, Ariyarathna K.G.D. N., e14213@ce.pdn.ac.lk
 E/14/144, Jaseel M.I. A., e14144@ce.pdn.ac.lk
 Table of Contents
 Introduction
 Solution Architecture
 Hardware & Software Designs
 Links
 Introduction
 Waste management is one of the primary problem that the world faces irrespective of the case of developed or developing country. The key issue in the waste management is that the garbage bin at public places gets overflowed well in advance before the commencement of the next cleaning process. Sometime the garbage collector truck not sufficient for collecting all the garbage. These in turn leads to various hazards such as bad odor & ugliness to that place which may be the root cause for spread of various diseases. Another issue is in the waste management is having more trucks and human resources than needed. To avoid all such hazardous scenario and maintain public cleanliness and health this work is mounted on a smart garbage system.
 We are living in an age where tasks and systems are fusing together with the power of IOT to have a more efficient system of working and to execute jobs quickly! With all the power at our finger tips this is what we have come up with.
 The traditional way of manually monitoring the wastes in waste bins is a cumbersome process and utilizes more human effort, time and cost which can easily be avoided with our present technologies. The main theme of the work is to develop a smart intelligent garbage alert system for a proper garbage management. What our system does is it gives a real time indicator of the garbage level in a trashcan at any given time using weight and volume (solar) sensors. Using that data and as the bins are containing GSM module we can give an alert signal to the municipal web server then optimize waste collection efficient routes, sufficient number of trucks and labors. It allows trash collectors to plan their daily/weekly pick up schedule.
 Solution Architecture
 CO321 Embedded Systems:
 Garbage monitoring system detect the level of waste in the garbage bin and identify whether garbage bin is full or not by sensors. Location of the garbage bin will identify using GPS. After municipal send the truck to collect wastes from particular bin, a light indicator in bins switch on to inform the people that garbage collector trucks are on their way. Another light indicator in bins is used to inform the worker that the status of garbage bins.
 CO324 Network and Web Application Design:
 Send the data/message from garbage bins to municipal council to take relevant action. A summary report will send from municipal council to centralized server in main department within a period (per week/ per month).
 CO325 Computer and Network Security
 The data are sent by the controller in the garbage bin to the server. Database and network system is protected as it sent as encrypted data. Therefore third party couldn’t access and change the data.
 Hardware and Software Designs
 The basic Model works like so…
 We will first have to enter the height of the dustbin. This will help us to generate the percentage of trash in the trashcan. We then have two criterias which needs to be satisfied to show that the particular bin needs to be emptied :
 1.The amount of trash - let’s say if a bin is half full we don’t really need to empty it. Our thresh, or maximum amount that we permit of trash, is 75% of the bin.
 2.If supposing a particular trashcan fills up 20% and then for a week doesn’t change, it comes into our second criteria, time. With time even the little amount will start rotting leading to a smelly surrounding. To avoid that our tolerance level is 3 days, so if a trashcan is less than 75% but it is two days old it then will also need to be emptied.
 COMPONENTS IN OUR SYSTEM
 An ultrasonic sensor will be placed on the interior side of the lid, the one facing the solid waste. As trash increases, the distance between the ultrasonic and the trash decreases.
 An weight sensor will be placed on the bottom of the bin. Weight sensor detect the weight of the trash.
 This live data will be sent (distance & weight) to our microcontroller.
 Our micro- controller then processes the data and through the help of GSM/GPRS module sends it to an app (to MC).
 What the app does it visually represents the amount of trash in the bin with a small animation.
 This process will indicate all the bins which require attention, leading the user to take the most effective route.
 Links
 Project Repository
 Project Page
 Department of Computer Engineering
 University of Peradeniya
 Back to top","Waste management is one of the primary problem that the world faces irrespective of the case of developed or developing country. The key issue in the waste management is that the garbage bin at public places gets overflowed well in advance before the commencement of the next cleaning process. Sometime the garbage collector truck not sufficient for collecting all the garbage. These in turn leads to various hazards such as bad odor & ugliness to that place which may be the root cause for spread of various diseases. Another issue is in the waste management is having more trucks and human resources than needed. To avoid all such hazardous scenario and maintain public cleanliness and health this work is mounted on a smart garbage system.  We are living in an age where tasks and systems are fusing together with the power of IOT to have a more efficient system of working and to execute jobs quickly! With all the power at our finger tips this is what we have come up with.       The traditional way of manually monitoring the wastes in waste bins is a cumbersome process and utilizes more human effort, time and cost which can easily be avoided with our present technologies. The main theme of the work is to develop a smart intelligent garbage alert system for a proper garbage management. What our system does is it gives a real time indicator of the garbage level in a trashcan at any given time using weight and volume (solar) sensors. Using that data and as the bins are containing GSM module we can give an alert signal to the municipal web server then optimize waste collection efficient routes, sufficient number of trucks and labors. It allows trash collectors to plan their daily/weekly pick up schedule.",E14,Cyber-Physical Systems Projects,https://projects.ce.pdn.ac.lk/3yp/e14/Smart-Waste-Disposal-Monitoring-System/,https://github.com/cepdnaclk/e14-3yp-Smart-Waste-Disposal-Monitoring-System,https://cepdnaclk.github.io/e14-3yp-Smart-Waste-Disposal-Monitoring-System,https://cepdnaclk.github.io/e14-3yp-Smart-Waste-Disposal-Monitoring-System/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/3yp/E14/Smart-Waste-Disposal-Monitoring-System/
37,Telepresence Robot,"Telepresence Robot
 Projects - Department of Computer Engineering
 University of Peradeniya, Sri Lanka
 Telepresence Robot
 Team
 E/14/262, Sanoj Punchihewa, e14262@ce.pdn.ac.lk
 E/14/305, Chandima B Samarasinghe, e14305@ce.pdn.ac.lk
 E/14/337, Yuvini Sumanasekera, e14337@ce.pdn.ac.lk
 Table of Contents
 Introduction
 Solution Architecture
 Hardware & Software Designs
 Conclusion
 Links
 Introduction
 A telepresence robot is a mobile, remote-controlled device that enables a person to be virtually present and to interact in a remote place. This project will be based upon developing such a robot for the department. With the use of interactive elements such as high definition audio and video, the robot will allow users (students/staff) to collaborate in person and remotely.
 Solution Architecture
 EMBEDDED SYSTEMS DESIGN
 What will you be measuring and/or controlling?
 Measuring/Inputs
 Ultrasonic Sensor: to measure the distance to the nearest obstacle in front of the robot.
 GPS Sensor: to track the robots position (assuming general purpose usage of the robot other than using inside the department)
 Microphone: to input voice signals.
 Web camera: to input video signals.
 Controlling
 Motor Controllers: to control the robots movement
 Servo Motor: to change the angle of the (camera+LCD screen)
 What embedded systems platform(s) will you be using, and why?
 Raspberry Pi 3 Model B
 To implement the controlling logic and all the softwares we are going to use high level programming languages such as Java and Python. It is required to choose a microcontroller capable of installing a OS like Linux.
 Has an inbuilt Wifi module.
 To control motor controllers, servos, ultrasonic sensors and other peripherals the raspberry pi has about 40 GPIO pins.
 To do real time full-duplex video+audio streaming, microcontroller with a high performance CPU is required.
 The robot includes a LCD screen. The cheap microcontrollers such as Arduino are not capable enough to produce required minimum frame rate, while the raspberry pi has dedicated GPU. Having a raspberry pi cut off the extra GPIO pin requirements of connecting the LCD as well.
 How does the system connect to the network?
 Primary:
 Using the inbuilt Wifi module, the robot will connect to the available department’s Wifi router to get access to the internet.
 Secondary:
 If for some reason Wifi is not available, the robot will connected to the internet using the GSM (module).
 Users:
 Users connect to the network using their END devices.
 What peripheral components will you be using, and how do they work?
 LCD Screen
 LCD screen will use to display the video of the user and to display the relevant information. This display will connect through the HDMI port of the raspberry pi. Power will be given through a suitable voltage regulator.
 Motor Controller (BTS7960 43A, Type: IBT_2) x 2
 Motors will draw huge current when in operation. In order to avoid drawing that current from the microcontroller pins these motor controllers are used. Motor controllers get power from the external power source and drive the motors according to the given signals by the GPIO pins connected to the microcontroller.
 Servo
 A servo is connected to the head section of the robot to change the angle of the head section (LCD+Camera+Mic…). Control signals to this servo is given through the GPIO pins and the power source is connected through a 6V voltage regulator.
 Ultrasonic Sensor (HC-SR04)
 Used to get the distance measurement to the nearest obstacle in front of the robot. Connected via the GPIO pins. Normally working current of these module is 15mA. Since the maximum current of a GPIO pin signal is 16mA (recommend 8mA), to be safe this module will also powered by the external power source through a 5V voltage regulator and only the control signals will give through GPIO pins.
 GSM module
 Will be using as the secondary method of connecting to the network. Control signals will give through GPIO pins. Power will give through a voltage regulator.
 GPS module
 Used to get the location data. Will connect through the GPIO pins. Power will be given through a suitable voltage regulator.
 Inbuilt WiFi module
 Primary method of connecting to the network.
 Web Camera
 Used to get the video input. Connected through the USB port of the raspberry pi.
 Speaker
 Will connect through the audio connection of the raspberry pi
 Mic
 Since there is no separate connector in raspberry pi to connect a mic, USB extension module will be used to connect a microphone to the raspberry pi.
 Hardware and Software Designs
 Conclusion
 #Issue
 Eventhough we successfully tested the peer-to-peer video streaming using default browser applications when it comes to integrating that into our java program we got to know that all the regular web components that we can use with java are not given permission to access resources like camera by the base implementation. Therefore, we have checked for the alternative web components and got to know that the only opensource web controller we can use with java is the CEF browser (chromium, JCEF) component. But, we had to build that from the source files before integrating that in our program. Eventhough, we were able to build that and run on amd64 architecture , we haven’t found clear explanations or successfull builds (or build methods) for arm architecture. Therefore, it seems to be integrating a browser component which can access to cameras in the java application itself seems to be rather difficult (or impossible) task.
 [we have used raspbian os in raspberry pi 3 b]
 #Possible Solutions
 01) Opening the video streaming mode using a regular browser (only for the video streaming mode)
 02) Using a different platform : (Android Things)
 The browser component we normally used in android application development can access to the resources like cameras. Therefore, we though of using android things os as our base os in our raspberry pi. Since, we are already familier with android application development we can develop the an andorid application for the bot.
 #The Solution we are currently working on : 2nd solution.
 Links
 Project Repository
 Project Page
 Department of Computer Engineering
 University of Peradeniya
 Back to top","A telepresence robot is a mobile, remote-controlled device that enables a person to be virtually present and to interact in a remote place. This project will be based upon developing such a robot for the department. With the use of interactive elements such as high definition audio and video, the robot will allow users (students/staff) to collaborate in person and remotely.",E14,Cyber-Physical Systems Projects,https://projects.ce.pdn.ac.lk/3yp/e14/Telepresence-Robot/,https://github.com/cepdnaclk/e14-3yp-Telepresence-Robot,https://cepdnaclk.github.io/e14-3yp-Telepresence-Robot,https://cepdnaclk.github.io/e14-3yp-Telepresence-Robot/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/3yp/E14/Telepresence-Robot/
38,Train Movement Tracking and Level Crossing Safety Control,"Train Movement Tracking and Level-Crossing Safety Control
 Projects - Department of Computer Engineering
 University of Peradeniya, Sri Lanka
 Train Movement Tracking and Level-Crossing Safety Control
 Team
 E/14/213, MAHALIYANA R.K., e14213@ce.pdn.ac.lk
 E/14/136, HETTIARACHCHI H.A.D.C., e14136@ce.pdn.ac.lk
 E/14/125, HASSANA A.L.F., e14125@ce.pdn.ac.lk
 Table of Contents
 Introduction
 Solution Architecture
 Hardware & Software Designs
 Links
 Introduction
 Our project is to built a monitoring system for railways. In SriLanka, there is a traditional system to monitor trains in most of the stations. Usually from one station, they issue a device to the train and once that train reached to another station, train driver has to give that old device and get a new device from the newly reached station. Because of that, that train can be only monitored by the stations which are located along its path. So we intend to develop a system which can monitor all the trains from any where. As a feature of our system, we’ll add a railway gates controlling part.
 Solution Architecture
 Our system has a centralized sever. It monitors the trains and controls the gates. There are separate devices for those tasks. Basically we use GPS to get the locations of trains and continuous internet connection to feed the data to the server. If something happens to the GPS signals, there is a failsafe sensor for that. That sensor will trigger the gate controlling device when the train is in right position. That is how our system works.
 Hardware and Software Designs
 The GPS module that we are using for the project is Adafruit Wearable Ultimate GPS Module. The images will show you how its configured and the output.
 We have created a website for upload data and monitor the train. We hosted that website in 000webhost.com .
 This is the link for that site.
 Video - How our site works
 Video - Process
 Project Report
 Links
 Project Repository
 Project Page
 Department of Computer Engineering
 University of Peradeniya
 Back to top",Our project is to built a monitoring system for railways. We intend to develop a system which can monitor all the trains from any where.,E14,Cyber-Physical Systems Projects,https://projects.ce.pdn.ac.lk/3yp/e14/Train-Movement-Tracking-and-Level-Crossing-Safety-Control/,https://github.com/cepdnaclk/e14-3yp-Train-Movement-Tracking-and-Level-Crossing-Safety-Control,https://cepdnaclk.github.io/e14-3yp-Train-Movement-Tracking-and-Level-Crossing-Safety-Control,https://cepdnaclk.github.io/e14-3yp-Train-Movement-Tracking-and-Level-Crossing-Safety-Control/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/3yp/E14/Train-Movement-Tracking-and-Level-Crossing-Safety-Control/
39,sleep apnea detection,"Sleep Apnea Detection | e14-3yp-sleep-apnea-detection
 e14-3yp-sleep-apnea-detection
 Sleep Apnea Detection
 Objective
 Developing a 100% non intrusive method for detecting sleep apnea in infants.
 Approach
 A video feed is processed to detect the breathing pattern and then it is assessed for anomalies.
 Algorithms
 Canny edge detection, Center of graviry tracking, subspace filtering, adaptive filtering, maxima detection.
 Hardware
 Raspberry pi model 3 b
 Raspberry pi camera
 Software
 Python3, OpenCV
 People
 Gihan Jayatilaka , Harshana Weligampola , Suren Sritharan and Pankayaraj Pathmanathan developed this system as a course project for CO321 CO323 CO325.
 The project was supervised by Dr. Roshan Ragel and Dr.Isuru Nawinne .
 The embedded system was developed by Nuwan Jaliyagoda and Anupamali Willamuna.
 Acknowledgements
 Sanjaya Herath provided the hardware components. Dinidu Bhathiya provided a dataset.
 Future work (ideas)
 Identifying deafness in infants through behavioural analytics
 Identify risky behaviour of the baby (trying to climb out of the cot)
 Publications
 One algorithm developed in this project was published as G. Jayatilaka, H. Weligampola, S. Sritharan, P. Pathmanathan, R. Ragel and I. Nawinne, “Non-contact Infant Sleep Apnea Detection,” 2019 14th Conference on Industrial and Information Systems (ICIIS), Kandy, Sri Lanka, 2019, pp. 260-265, doi: 10.1109/ICIIS47346.2019.9063269.
 arXiv preprint arXiv:1910.04725.
 You may cite this work as,
 <pre>
 @INPROCEEDINGS{non-intrusive-sleep-apnea-detection,
 author={G. {Jayatilaka} and H. {Weligampola} and S. {Sritharan} and P. {Pathmanathan} and R. {Ragel} and I. {Nawinne}},
 booktitle={2019 14th Conference on Industrial and Information Systems (ICIIS)},
 title={Non-contact Infant Sleep Apnea Detection},
 year={2019},
 volume={},
 number={},
 pages={260-265},}
 </pre>",Non intrusive method for detecting sleep apnea in infants.,E14,Cyber-Physical Systems Projects,https://projects.ce.pdn.ac.lk/3yp/e14/sleep-apnea-detection/,https://github.com/cepdnaclk/e14-3yp-sleep-apnea-detection,https://cepdnaclk.github.io/e14-3yp-sleep-apnea-detection,https://cepdnaclk.github.io/e14-3yp-sleep-apnea-detection/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/3yp/E14/sleep-apnea-detection/
40,Covid Tracer,"Covid Tracer
 Covid Tracer
 Home
 About
 Features
 Solution Architecture
 High Level
 Hardware
 Security
 Budget
 Timeline
 Development
 Software
 Hardware
 Testing
 Team
 Autonomous Covid-19 Tracking System
 For a Well Aware and Safe Sri Lanka
 Project Repo
 Covid-19
 New Normal Situation due to the Pandemic
 Get Started
 Introduction
 Overview
 The Covid-19 Pandemic is the defining Global Health Crisis of the contemporary society. Since its emergence in late
 2019, the virus has spread across the entire world and led to a dramatic loss of human life. The economic and social
 disruption caused by the pandemic is devastating. Almost every sector is affected by this pandemic. In addition,
 this prevailing situation has raised unprecedented challenges to the daily routines of the people. While taking
 immense measures on infect diagnosis, Corona Virus treatments and controlling the spread of the disease a reasonable
 focus should be given to how we adjust to this new normal situation.
 When will the world be free of Covid? is an unanswered question. Our focus is to make the new normal situation due
 to Covid easily adjustable for the people while making their lives safe.
 Real World Problem
 Due to the prevailing pandemic situation people have a responsibility of providing accurate details of themselves at
 entrances of various commercial places like shopping malls, banks etc. and get their temperatures checked and accepted
 before entering. This situation is currently handled manually by people writing details on a book at entrances of various places.
 Some major issues with this system includes:
 Inefficient thus time consuming
 Risk of getting infected by everyone touching same stationary is high
 Details get blot by touching with wet hands after washing or sanitizing
 Personal details of people are publicly made available
 High probability for the guard at the entrance checking temperatures to get in contact with infects
 People providing inaccurate data, neglecting providing details and getting the temperature checked
 Inability to detect people under quarantine
 Inability to keep track of infected percentages of the locations visited
 Adhering to workplace safety and health practices and ensuring access to decent work and the protection of labour rights
 in all industries will be crucial in addressing the human dimension of the crisis.
 Features of our system
 Computerized system
 Covid-19 management software with computerized database system for the Government. This is also mainly used for demonstrating how over system gets latest
 covid information from the Government Authorities through a REST API
 Autonomous detail entering and temperature checking
 Details about the person will be entered to the system by scanning the barcode of NIC/ QR scanning, system will detect if the
 person is under quarantine. The temperature will be measured using sensors. These temperature measurements along
 with the visited location will be sent to the server and stored for a 2 weeks period. Your privacy will be highly protected
 Automated door opening
 This comes as an optional feature of our system. If certain satisfactory conditions are met like quarantine status, max
 temperature level then the doors will be opened automatically
 Covid tracking
 Check about reported infects of a particular location as a percentage before visiting.
 Our system will also keep records about the visited locations of a person for a period of 2 weeks. Infect percentages
 of these locations reported later will be tracked
 Website
 Local community can visit our website to view covid related information like daily/cumulative cases and deaths, infect percentages
 of areas. A seperate user login will be provided to view personal details like temperate fluctuations using data obtained
 at entrances, infect percentages of the locations visited for a period of 2 weeks
 Mobile app
 Similar interface as the website. This is designed for the easy use at the comfort of a mobile phone.
 High Level System
 Description
 Our system can be described in four segments.
 A software for data entering, data representing along with a computerized database system will be provided to the
 Government Authorities. They maintain detailed Covid related information like infects, pcr test reports,people under quarantine etc. in their
 server system. Our database system will be updated on certain intervals with the Government Server through a REST API.
 Our server system will be deployed in the cloud. We will keep information about people's NIC numbers,
 addresses, contact numbers, visited locations, temperature readings obtained at various entrances etc. Various calculations on data
 for example infect percentages will be done on a cloud virtual environment
 Commercial Merchants including shopping malls, banks etc. will be provided with an embedded system
 device based on a micro-controller. People can scan their IDs before entering and the system will check with the servers if the
 person's under quarantine. The temperature will be measured by contact-less sensors and checked if satisfactory. Whether the person
 is allowed to enter or not will be notified using LED displays and buzzers. ID number along with the temperature and location will be stored
 in our servers for a 2 week period. Therefore merchants don't have the burden of handling customer details. At the same time
 people don't have to provide their details publicly. Commercial merchants have the choice of implementing our door system which will
 be opened automatically based on the customer's conditions
 The local community will be provided with a website as well as a mobile app. They can visit our website/app to
 view general Covid related information in the country eg: daily/cumulative cases and deaths, infect persentages of different areas. A seperate
 user login is provided to view personal data eg: infect percentage reported later of visited locations, temperature fluctuations,
 general health alerts released by Government eg: vaccinating dates in different areas as well as user specific health alerts
 Embedded Hardware Device
 Microcontroller
 Microcontroller is the base of our device and it should accomplish several task at the same time.
 Hence, having a dual core processor in the microcontroller will be efficient to handle those tasks
 seperately. Low power consuption is a major concern as it is active 24x7 mostly.
 Modules
 GSM module
 GSM module(SIM 800L) supports a sim card to connect with server using 2G mobile data as a backup option when
 wifi is not available.
 External Flash Memory
 W25Q32(4MB) model is used as the external flash memory to increase the scalability of the device. As the system has to check with
 the cloud server whether the person is quarantined or not for each person, and since the server gets such requests from
 all over the country, the efficiency and scalability will be less. Therefore a local caching system will keep a list of quarantined people in there are,
 Thus only the information that is not available to the device can be requested from the server.
 Battery
 The device contains an internal rechargeable battery to keep the device active in case of
 power failures.
 Sensors
 IR Sensor
 Contactless IR sensor is used to detect the body temperature.
 Ultrasonic Sensor
 This sensor is used to detect the distance from IR sensor to hand and to the execution of sanitizing unit.
 Barcode Scanner
 Gm66 model 2D barcode reader is used to read the barcode in the ID card.
 Actuators
 Piezo Buzzer
 A piezo buzzer is used to indicate that certain process is successfully done or not.
 LCD Display
 The result of each process is dispalyed on the LCD.
 Security Aspects
 Why is it important?
 For any system it's crucial to pay attention to the security aspects. No system can be automatically immune.
 If we don’t consider this seriously, the impact and recoveries can be very expensive. Looking from the perspective of the CIA triad,
 Availability is necessary because the system mostly works 24/7 and the whole entrace system of the country depends on it.
 If the system is not available the effects can be very significant.
 Confidentiality also plays a major role as we handle sensitive data. Only the authorized people should be able to see the data.
 Integrity is necessary because if the data is modified without the knowledge of authorized people, it can break trust of people
 towards the system. Imagine providing inaccurate health details to people!
 what are the sensitive data that needs to be secured?
 Any system that handles PII(Personally Identifiable Information) or SPII(Sensitive Personally Identifiable Information) or any Health Care data, are legally obligated to protect these data.
 If these information are compromised it can lead to devastating consequences like identity theft incidents, high risk of damage to a individual, loss of
 trust on the system and many more.
 There are many international guidlines such a system needs to adhere to. For example GDPR(General Data Protection Regulation), HIPAA(Health Insurance Portability and Accountability Act)
 Our system keeps personal details about people like full name, NIC, address, contact details, locations they have visited, health details like temperatures and oxygen levels. These data have high criticality. When such system is deployed,
 many attackers try to play with the data since the data can be of interest to many. For example if a person can access to someone else's mobile app, he can checkin to places pretending to be someone else.
 If our database is compromised it can lead to a data breach which will expose all the PII details of the community.
 This can even result in Ransomware attacks.
 Budget
 Item
 ESP32 30-pin DOIT Board
 External Flash(W25Q32)
 Ultrasonic sensor(HC-SR04)
 Barcode reader(GM66)
 IR temperature sensor(MLX90614)
 Piezo buzzer
 16x2 LCD
 Power Supply Module
 9V AC/DC Adapter
 Transistor
 Water Pump
 Nozzle
 ESP32 Antena
 Rechargable battery
 ***
 Quantity
 1
 1
 2
 1
 1
 1
 1
 1
 1
 1
 1
 2
 1
 1
 ***
 Unit Cost
 2400.00
 300.00
 200.00
 5000.00
 3000.00
 30.00
 320.00
 1100.00
 750.00
 130.00
 300.00
 180.00
 220.00
 1200.00
 Total
 Total
 2400.00
 300.00
 400.00
 5000.00
 3000.00
 30.00
 320.00
 1100.00
 750.00
 130.00
 300.00
 360.00
 220.00
 1200.00
 15510
 Timeline
 Our Team
 Kenath Perera
 E/17/252
 Deanna Coralage
 E/17/044
 Ruchika Perera
 E/17/242
 Our Supervisors
 Dr. Isuru Nawinne
 Senior Lecturer
 Dr. Mahanama Wickramasinghe
 Senior Lecturer
 Covid Tracer
 We are a group of undergraduates from Department of Computer Engineering, University of Peradeniya.
 This system was implemented as our third year project
 ----------------------------------- Links
 Project Repo
 Department Projects
 Department of Computer Engineering
 -----------------------------------
 Faculty of Engineering
 University of Peradeniya
 © Copyright eBusiness. All Rights Reserved
 Designed by BootstrapMade",Fully Autonomous Covid19 Tracking System https://cepdnaclk.github.io/e17-3yp-Covid-Tracer,E17,Cyber-Physical Systems Projects,https://projects.ce.pdn.ac.lk/3yp/e17/Covid-Tracer/,https://github.com/cepdnaclk/e17-3yp-Covid-Tracer,https://cepdnaclk.github.io/e17-3yp-Covid-Tracer,https://cepdnaclk.github.io/e17-3yp-Covid-Tracer/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/3yp/E17/Covid-Tracer/
41,E Parking System,"E-parking system
 QuickPark
 Menu
 Home
 About
 Design
 Progress
 Development
 3D Models
 The Fastest & Easiest Way To Manage Your Parking Lot
 Assign parking spots and manage payments with ease
 Project Repository
 About us
 The motivation behind our product
 Urbanization has led to quick and efficient parking being vital.
 Human-operated parking lots have a lot of inherent flaws.
 An autonomous parking system could address these issues effectively.
 Why choose our product
 Manual systems are highly inefficient and leads to long queues and waiting times.
 It is difficult to manage the allocation of parking spaces.
 Owners cannot easily get an idea of the overall status.
 Our objective
 We aim at eliminating the overheads and inefficiencies associated with manual parking systems to provide a comprehensive solution that addresses the concerns of both the consumers as well as the owners of the parking lot.
 Features
 Key Features Of Our Product
 Reservation of parking spots
 Make reservations in advance and save time
 Providing directions to assigned spots
 Get directions with our mobile app
 Payments made easy
 Register through our mobile app for payments
 Bird’s-eye view of parking lot
 Parking lot owners can manage with ease and collect usage statistics
 Progress
 Product Timeline
 Week 7-8:Backend Design
 Week 9-10:Hardware design
 Week 11:UI and algorithm designs
 Week 12-14:App development
 Week 15:Cloud deployment
 Week 16:Integration
 Week 17-18:Testing
 Introduction
 Product Demo
 QuickPark
 An autonomous system for assigning and managing parking spots and processing payments in a car park.
 Team Members
 E/17/296 Ravisha Rupasinghe
 E/17/251 Sandun Sanjaya Perera
 E/17/018 Imesh Balasuriya
 Supervisors
 Dr. Isuru Nawinne
 Dr. Mahanama Wickramasinghe
 Related Links
 Project Page
 Department Website
 Faculty Website
 Copyright ©2022 All rights reserved.",An autonomous system for assigning and managing parking spots and payments in a car park. https://cepdnaclk.github.io/e17-3yp-E-Parking-System/,E17,Cyber-Physical Systems Projects,https://projects.ce.pdn.ac.lk/3yp/e17/E-Parking-System/,https://github.com/cepdnaclk/e17-3yp-E-Parking-System,https://cepdnaclk.github.io/e17-3yp-E-Parking-System,https://cepdnaclk.github.io/e17-3yp-E-Parking-System/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/3yp/E17/E-Parking-System/
42,Landmine Detector,"React AppYou need to enable JavaScript to run this app.IntroductionSolution ArchitectureHardware DesignSoftware DesignTestingBudgetTimelineTeamLandmine Detection RobotLandmine DetectorIntroductionAboutLandmines are prominent weapons designed to destroy or disable enemy targets which detonate by the movement of its target, pressure, sound, magnetism and vibration. More than a hundred million land mines are scattered around the world which remain hazardous for years even after conflict termination posing a significant threat to civilians and the economy. More Effective and sophisticated tools to detect, locate and deactivate landmines are urgently needed to make lands safer for humans to use.ProblemCurrently available demining methods which include humanitarian and mechanical demining utilize manual prodders or metal detectors operated by humans and mine clearing machines respectively to detect the location of mines. This conventional methods are labor intensive, expensive, time consuming and possess high risk for the humans and machines involved in the process.SolutionWe introduce 'The Landmine Detection Robot' that accurately detects mines through sensors and send the GPS coordinates of the specific locations where mines are located to the server where a map of the mine field is updated. The deployment of multiple Mine Detection Robots to the mine field facilitates finding a safe pathway through the mine fields safely and efficiently without any human involvement making the challenging process of demining easier than ever.Solution ArchitectureWeb ApplicationThe Web Applciation is hosted using Amplify Serverless methods. Acts as the access point for all the users in the User Groups. After logging in users can access either modular front page for controling a spcific robot or User Admin Page related to a group who has all data access.Considering control page for a specific robot a detail entry point and control options will be available along with graphical representation of search area maps.Automated RobotIs the main hardware component of the project. Includes a core functionality of landmine detection, autonoumus navigation and data exchange. Accoring to tha main data flow the key aspects of the robot will be, accepting cordinates from server, storing a data structures to maintain search area details, passive landmine detection while navigatting through the path while updating the data and sending back data to the servers.Web ServerThe backend servers acts as an immediate between the hardware component and user all while doing essential tasks of data storing , calculating parameters and orgainzing inputs and outputs. When the inital inputs received from user server cloud functions would calculate intial boundaries for search area and relevent parameters for setting up the search.Once the search starts the robot updates data stored here to give a real time like data accessability for the users.DATA FLOWData Flow Generation at the user interfaceInserts GPS cordinates
 along with a search area. The data will be sent to the servers as raw inputWhen recieved cloud functions would run within the backend to calculate relevent boundaries of given search area in the real environmentThese values will be stored at the servers and at the same time send calculated values to the robot through the established networkWhen received a data structure would be created within the Robot to keep track of the path it travelled, Landmine locations, boundaries.While travelling the details of the search would first be stored at the local data structure. Then with timed HTTP requests would send the updates of the data to the backendThe received values would get stored at the servers, and also providing read access to the webserver for the updated dataWhen data received a virtual map drawn to represent the location data gets drwan at the user interface. Providing visual
 and informative data to the userSTATE DIAGRAMER DIAGRAMRELATIONAL SCHEMAHardware DesignMetal DetectorOur design of the metal detector will be built using the collpits oscillator circuit.The circuit provides a constant oscillation by a feedback loop and is in the range of 100kHz.The oscillation will be fed into an atmel (atmega328p) chip.When detected would case an interrupt.Calibrating the strength of the metal detector can be done by changing the current through the inductior (achieved by changing the supply voltage) or changing the number of wraps aroud the coil.Obstacle DetectionObstacles detected using Ultra-Sonic sensorsDirectly using the sensors gives somewhat accurate results but can deviate from actual value due to temperature and humidity.The landmine detector is an outdoor robot which means the values can change.To resolve this issue we add temperature and humidity compensation to distance calculated.DHT11 humisity and temperature sensor is used to get required values.Navigation and LocationingMainly done by using the Neo8M GPS module and the GY80 IMU.Through the sensors we get the position, direction, and orientation of the robot.In the event of obstacle or landmine detected locations of these would be stored and sent back to the server.Calculation of an alternative path will happen avoiding obstructions. Motor DriverThe motors will be driven by L298N motor driver.GY80 's sensors and encoders would provide requierd feedback for motor controlling.Selecting motors where done for our requirements of,Weight
 : 5kgNo of Motors
 : 4Radius
 : 0.045m(4.5cm)Velocity
 : 0.5m/sMax incline
 : 30[deg]Supply Voltage : 12VDesired Acc
 : 0.1m/s2Weight : 5kgOperating Time : 1.5hrsTheroetical values of a motor providing for above requirements,RPM
 : 150RPMTorque
 : 0.4NmTotal Power
 : 4.8WMax Current
 : 0.4ABattery Req
 : 1.6AhMetal DetectorOur design of the metal detector will be built using the collpits oscillator circuit.The circuit provides a constant oscillation by a feedback loop and is in the range of 100kHz.The oscillation will be fed into an atmel (atmega328p) chip.When detected would case an interrupt.Calibrating the strength of the metal detector can be done by changing the current through the inductior (achieved by changing the supply voltage) or changing the number of wraps aroud the coil.Obstacle DetectionObstacles detected using Ultra-Sonic sensorsDirectly using the sensors gives somewhat accurate results but can deviate from actual value due to temperature and humidity.The landmine detector is an outdoor robot which means the values can change.To resolve this issue we add temperature and humidity compensation to distance calculated.DHT11 humisity and temperature sensor is used to get required values.ConnectionsThe robot has a main I2C bus connecting two masters and sensors as slaves. Also an atmel chip is used as a slave for controlling motors and interrupt generator for the metal detector. One aster being the ESP32 handles the calculations while the second master the atmel chip is responsible for communiation and data buffering. The data would be stored in an SD card using the SPI conncetion protocol. Communication happens through the SIM900A GSM module using the UART protocol.PowerThere are three main power lines within the robot. A 5V power line for some sensors that require it and a 3.3V power line for specific components which needs it for optimum performance. Such as the ESP32 , GY80 and the NEO8M. The dedicated 12V would be for the motors and the metal detector. The power for the whole system will be provided using a LiPo 12V 10000mAh battery and voltage regulators will be used to get required voltages.Software Designcloud architectureUser is interacted with our front end which is hosted in AWS amplify. Authentication and authorization happen using cognito pools and identity groups. In the event of creating a user a lambda function is called to sign up a user to a relevant group. With authorized credentials the user calls the API to initiate a new search. The input values are stored in the database and at the same time lambda function is called to publish the data to a topic in the IoT core. Then the robots which are subscribed to the same topic receive data via the MQTT protocol. Returned data from the robot will be published using MQTT and using IoT rule by which the data can be processed. Once the data is received, an aws lambda function will trigger and check for identity and access of the robot and call the API to store the data into the database. And at the same time data is sent to the user.UI designsLogin UItwo-step verificationUser InterfaceHistoryProfileTestingAPI Postman Used to test GraphQL API calls to check for authorization, data manupilation, data type integration. Requests are sent using API keys to validate the requests. Used to tesst Appsync Graph QL requests. Helpful for accessing data elements with varying authorization levels.Lambda function Important when using a serverless architecture. Used to execute functions of servers. Currenctly all defined lambda functions in the system are running on node js. Tests are done to verify authorization roles and accessebility of data on trigger ro function call.Hardware Integration Mqtt Explorer used here to emulate node hardware behaviour. Check for viablity on mqtt messages passed to and from node hardware. Also to check for neccessary trigger aexecution. BudgetComponent Unit Price(Rs.)QuantityPriceESP32150011500Atmega328p75021500NEO8M-GPS219012190GSM Module195011950GY80 - IMU280012800Motors 150RPM150046000Tires Chassis and other robot components500015000DT1110001 1000Ultrasonic sensors(HCSR04)2503750Copper wire2501 (10m)250Resistors capacitors and other components500150012V battery LiPo500015000Total28440Timeline22 July 2021Milestone 1Project Proposal03 September 2021Milestone 2Designs of Hardware and SoftwareFull data and control flowGetting familiar with technologies.Milestone 3Implementation of Backend.Implementation of Frontend.Software Testing.Milestone 4PCB designs of circuits.Prototype & Testing3D Models for Robot.Milestone 5Complete Working SystemTeamAkila Karunanayake  Thisara Manohara  Vishva Navanjana  SupervisorsDr. Isuru Nawinne    Dr. Mahanama Wickramasinghe    Contact UsAkila KarunanayakePhone:
 +94 81 239 33 00 Thisara ManoharaPhone:
 +94 81 239 33 00 Vishva NavanjanaPhone:
 +94 81 239 33 00 University of PeradeniyaPhone:
 +94 81 239 33 00 Web-site:http://www.pdn.ac.lk/Faculty of EngineeringPhone:
 +94 81 239 33 02 Web-site:http://eng.pdn.ac.lk/Computer Engineering DepartmentPhone:
 +94 81 239 39 14 Web-site:http://www.ce.pdn.ac.lk/© 2022 LANDMINE DETECTOR | All right reserved | Terms of Service | Privacy",3rd Year Cyber-Physical Systems Project on detecting land mines.,E17,Cyber-Physical Systems Projects,https://projects.ce.pdn.ac.lk/3yp/e17/Landmine-Detector/,https://github.com/cepdnaclk/e17-3yp-Landmine-Detector,https://cepdnaclk.github.io/e17-3yp-Landmine-Detector,https://cepdnaclk.github.io/e17-3yp-Landmine-Detector/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/3yp/E17/Landmine-Detector/
43,Milk Testing and Collecting System,"Milk-Testing-&-Collecting-System
 MilkTab
 Introduction
 Solution Architecture
 Designs
 Designs
 Mobile User Interfaces
 Admin Panel Interfaces
 Hardware Designs
 Security
 Others
 Others
 Testing
 Timeline
 Budget
 Team
 Git Repository
 Milk-Testing & Collecting-System
 Introducing technology oriented milk collecting platform with a better way of testing milk.
 Easy usage
 Portabale
 Low Power consumption
 Detects milk adulterants
 Get Started
 Introduction
 View Introduction video
 The Problem
 In dairy industry, dairy collectors measure only few parameters when buying milk from dairy farmers. But, they usually don't measure quality parameters like fat content, acidity etc of milk at that spot. Also the financial deals are happening
 in a traditional way. The quality is very important in dairy related products. Currently small scale manufacturers do not use technology based methods to measure the quality of milk. They just use organoleptic tests such as smell and
 visual observation for that. The quality of raw milk is the primary factor determining the quality of milk products. Good-quality milk products can be produced only from good-quality raw milk. Milk testing and quality control should
 be carried out at all stages of the dairy chain
 Milk Adulterants
 There are many milk adulterants that are added to raw milk by supplier in order to get some financial benefits.
 Some of the major milk adulterants which cause serious adverse health effects are urea, formalin, detergents, ammonium sulphate, boric acid, caustic soda, benzoic acid, salicylic acid, hydrogen peroxide, sugars and melamine.
 Preservatives are special adulterants which increase the shelf life of milk.
 Eg: Formalin, Salicylic acid, Boric acid, Hydrogen Peroxide
 Water:
 Water is added to raw milk to increase milk volume. This is a common adulterant which is not harmful but decreases the milk quality.
 Urea:
 Urea, being a natural constituent of milk, constitutes the major portion of non-protein Nitrogen in milk. Maximum allowable limit for urea in milk is 70 mg/100 mL.
 Milk Adulterants
 Check here for more details
 Food Industry
 Quality is an important thing in food industry. Due to bio-chemical activities, there are changes in food time to time. Therefore, preservation is so impotant in food industry. Further, wastage of food happens due to expiration and many other reasons.
 In the case of diary industry, above problem becomes considerable because the wastage can be increased because of adding milk adultarents. It can be seen that almost all the dairy products we can see in the market are
 produced under the brand names of large scale companies. In Sri Lanka, it can observed that there are only few dairy products which are produced as rural domestic products like buffalo curd. Also a higher proportion
 of the dairy requirements are imported from other countries such as Australia and New Zealand because the domestic milk production is not sufficient. This is a good opportunity for small scale manufacturers to supply
 dairy products to catch up with the demand.
 Primary supply chain in Dairy industry
 Some restrictions for starting dairy related production;
 Not having an easy method to test the quality
 Not having knowledge of quality parameters and testing
 inability to buy laboratory equipments for testing.
 Find more details on 'Opportunities for dairy sector growth in Sri Lanka' here.
 Our Solution
 Our system can be used to measure and record important quality parameters of milk when the deal happens between the dairy collector and farmer to ensure milk is healthy and also it can be used to calculate financial value of milk according
 to those parameters and manage the financial records in a cloud based system.
 Whom do we target?
 MILK COLLECTORS
 LIQUID MILK SALES OUTLETS
 HOTELS & RESTAURANTS
 SMALL SCALE PROCESSORS
 Detection of Milk Adulterants
 Required Quality
 pH : 6.5 - 6.7
 Density : 1.026-1.032 g/ml
 Fat Content :
 Cow's milk (3.5-4.5)%
 Buffalo's milk (6-7)%
 Water
 Milk collectors use lactometer to measure the density of raw milk as a traditional method to detect if additional water is added to the raw milk. But, this method does not give an accurate result if the temperature
 is not considered, since the density varies according to the temperature.
 Water+Ammonium Sulphate
 Ammonium sulphate is added to increase the lactometer reading by maintaining the density of diluted milk.
 Urea
 Adding urea causes the increase of fat content. Since urea is basic, the PH value of raw milk can be used to detect this adulterant. Because urea is harmful, milk is unhealthy to consume when urea is added to
 it.
 Expected Outcomes
 Collectors
 Reducing wastage
 Get rid of traditional methods
 Reducing the need of high level laboratory testing
 Easy to expand production level
 Farmers
 Ability to get higher value & demand for good milk
 A smart way to access financial records
 Solution Architecture
 Measurements
 Using our hardware unit, following measurements of milk can be taken.
 1.PH value
 2.Fat rate
 3.Volume
 4.Weight
 5.Temperature
 Using these measurements as primary measurements, quality parameters which are focused on software sector are calculated.
 1.PH value
 2.Fat rate
 3.Corrected density
 Density can be calculated using weight and volume at the measured temperature. Then, that value is converted to the parameter 'corrected density' considering the temperature deviation from room temperature. Simply, the density is mapped for room temperature.
 PH value
 pH value is a important parameter in our system, which gives a better understanding about milk adulterants. The output voltage (analog) of the sensor is proportional to the pH value of the milk. The sensor is first calibrated using
 a standard sample (distilled water).
 Fat rate
 A high intensity LED is used as a light source and a LDR is placed to detect the amount of light passing through the milk. The more the fat content in the milk, more amount of light will be scattered by that milk.
 Corrected density
 Data communication
 Bluetooth is a popular communication method which is half duplex.
 We choosed bluetooth for communication because of the portable behaviour of the system.
 Milk Grading System
 What we do;
 Before the launching, we test fresh milk samples without any adultarents and get necessary data to observe the required range for each parameter. Then parameter values are tested after adding water content and graph it with
 additional water percentage as the independent variable and parameter values as dependent variables.
 How it works;
 According to the parameter values of a sample, a graphical visualization of each parameters can be obtained to get a better understanding about tested milk. Further, a grade will be displayed according to the quality as A/B/C/D.
 Main objective
 The grading system gives not only a grade for milk but also it is very useful to manage milk storage. It is recommended to store only milk with same grade in the same container, in order to reduce the possible wastages.
 Possible improvements
 This system can be improved so as to get an approximate value for fat percentage of milk which is called SNF value. Since the buyers are not interested in that type of parameters, the proposed grading system is sufficient for
 the targeted problem.
 Background Mechanism of Grading
 SNF - Solid Non Fat , CLR - Corrected Lactometer Reading
 The Solids-Not-Fat (SNF) means a collection of proteins, lactose, minerals, acids, enzymes and vitamins contents of the milk. It is the total solid content minus the fat content. The total milk solids are the sum of Fat and
 SNF. SNF can be calculated using following formula:
 SNF = (CLR/4) + (Fat x 0.21) + 0.36
 Our system is not highly focusing on SNF percentage or fat percentage. But what this formula shows is Fat and SNF percentages are factors which affects the milk quality. Hence, deviations from fresh milk is considered in our
 system to generate the Grade.
 Future Improvements with Machine Learning
 After the basic circuit is built up, a fresh milk sample is tested for all parameters with respect to additional adulterant percentages. This data set is used as the training data set to build a data model. Then, this model
 can be used to predict approximate adulterant percentage and notify to the user using a mobile notification. Since AWS supports jupyter notebook, after integrating this feature, user can get a better understanding about
 milk that he is going to purchase.
 Billing System
 According to the grade of tested milk, here is a mechanism to change the price for unit volume (1 litre) by the buyer(collector). For each grade he can assign a value in descending order as grade varies from A to D.
 History record of how the values varied is also displayed.
 Technologies
 Flutter
 Flutter is an open-source framework developed by Google and it is one of the most popular mobile development frameworks used by developers worldwide. It comprises all the essential elements, including cross-platform
 and native development models required to build high-performing and feature-rich applications in minimal time.
 Laravel
 Laravel is the best PHP framework to use and build efficient applications in any scale. Laravel application maintenance process is easier and also it provides high-security features such as token authentication.
 Laravel speeds the application development process by utilizing databases efficiently
 AWS
 AWS is designed to allow application providers, ISVs, and vendors to quickly and securely host the applications – whether an existing application or a new SaaS-based application. AWS is made up of so many different
 cloud computing products and services. AWS utilizes an end-to-end approach to secure and harden our infrastructure, including physical, operational, and software measures.
 Mobile Application User interfaces
 Previous
 Next
 Admin Panel User interfaces
 Admin dashboard home page
 Admin dashboard devices page
 Admin dashboard collectors page
 Price graph
 Volume graph
 Login page
 Admin change password page
 Admin change password page
 Admin change password page
 Previous
 Next
 3D Designs of model
 Milk container
 Main unit
 Sensor holder
 Container with main unit
 Register
 Login
 Configure
 Connect
 Analyze
 Buy
 Select as collector
 Enter a valid Email address
 Enter a strong password and re-type
 Click next to proceed
 Enter a valid mobile number
 Type your address
 Select your role
 Milk collector
 Small scale proceesor
 liquid outlet
 Hotel/Restaurent
 Sumbit your details
 Enter your email & password
 If you don't remember password, there is an option to reset it.
 If you still do not have an account, you can use guest mode
 Sign in to get the experience
 You can use this page to manage your profile
 Your basic details are displayed here
 Use edit options to change them if you want
 Use change price rate option to handle your deals
 In the home screen connect button will turn on bluetooth connection
 You can see the names of all connections
 Use lower text input to provide the name of the connection from the device
 Then each time, the conection will be automatically set.
 This page can be used to update your price values
 Only restriction is to enter values in descending order
 This values are public for other users
 This screen shows requests from farmers to buy milk from them
 After accepting both parties can contact each other.
 You can see contact details and locations of them
 Everytime before testing, you can select your seller
 A list of people you already connected, will be displayed
 If he/she does not use the app enter his/her nickname and select milk type you buy from him/her
 Sumbit your details
 After receiving data from the device parameter values will be displayed
 Upper box displays the grade and curret price value for a unit volume you have set.
 Then you have to enter the total volume of the milk, your sample belongs to
 After analyzing the report you can add or reject.(It is not recommanded to buy milk of Grade D)
 Anytime past details can be seen in a timeline
 There is the record for a specific farmer for a date
 You can see all the details aout the deal
 Features
 Low power consumption
 In this project, since the hardware unit is consumed less, the power usage is really effective. (Hardware use rechargeable batteries to get power)
 Portabale milk tester
 Our system is designed with the ability of using it as a portable device. In a milk collecting center also, it is applicable.
 Easy connection
 System can be used easily with bluetooth connection using your mobile phone as a smart way to test milk.
 Cloud based
 Users can experience an effective cloud based business environment and get rid of traditional testing and biling systems
 Calender based schedule
 All the records can be viewed in a timeline with an easy environment to see past records
 Customize your options
 Changing user details, price rate values can be done whenever you want.
 Synchronizable
 With the price changes in industry, it is easy to update the price rates.
 Testing
 Software Testing Tools
 Postman
 Postman is a scalable API testing tool which is a popular API workflow in testing and development. It has nice features like ability of creation of tests, ability of automated testing & etc.
 Testing unit
 What is tested
 Results
 Conclusion
 sign-up
 If any mandotary field is null
 Errors without allowing to submit
 email verification
 Successfull email notifications
 password verification
 check strength before submitting
 contact number verification
 sign-in
 If email is wrong
 login failed!
 If password is wrong
 login failed!
 If any field is null
 Errors without allowing to submit
 Appium
 Appium is an open source automation tool for running scripts and testing native applications, mobile-web applications and hybrid applications on Android or iOS using a webdriver.
 JMeter
 JMeter is a test tool from Apache used to analyze and measure the performance of applications, different software services and products.
 Security
 Admin panel security
 Registration as an admin is allowed only for specified people. This is done by inviting him/her by super admin with a time limit. Creating the super admin is done by seeding required credentials to the database when deploying.
 Details about invitations can be seen and it is possible to remove any email address that is already invited if admin wanted to invite him/her again.
 A session will expire after a specific time although an admin did not logged out. When registering and changing the current password, a strong password has to be entered and when changing the current password, admin has to verify his/her identity by giving
 the valid email and current password.
 Mobile Application Security
 JWT is the method of authentication for the users of the mobile application. A JSON web token (JWT) is an open standard (RFC 7519) and it is a compact and self-contained way for securely transmitting information between systems as a JSON object. This
 information can be verified and trusted because, it is digitally signed.
 Once the user is logged in and authenticated by the server, then the JWT token is generated and passed in response, and in each subsequent request, the token is passed to the server. This JWT token contains the information for
 the user's access and permission, which is part of the authorization.
 After registering and after using 'forgot password' option user has to verify himself by entering the OTP he received into his email. This is one security action in our mobile application.
 Timeline
 Project Planning
 Done
 6 weeks
 In first 6 weeks, our team discussed about several project ideas. Among them, we decided to build a milk analysor which can be used by dairy collectors to enhance the deal with dairy farmers. After deciding Hardware components, technologies and timeline,
 project proposal was presented in this phase.
 Backend Design
 4 weeks
 In this phase, basic backend functionalities are implemented in a Laravel project. API designing is the main task in this 4 weeks after creating database models.
 Frontend Design
 4 weeks
 Developing mobile application as a flutter project takes place in this 4 weeks. User interfaces are created in a way that the users of the application can easily manage their activities.
 Hardware Design
 In progress
 4 weeks
 Hardware solution is designed in this phase. All sensors are connected to the main unit and tested by getting measurements.
 Cloud Deployment
 1 week
 This week is for Deployment tasks. Some testing activities also take place on hosting. All issues in previous phases are also resolved.
 Integration
 1 weeks
 This is the last phase of our project where we test all the systems after integrating them together. After this phase, a final model is presented.
 Estimated budget
 #
 item
 quantity
 price(LKR)
 1
 ATmega328p
 1
 750
 2
 Bluetooth module HC-05
 1
 950
 3
 PH sensor
 1
 3800
 4
 HC-SR04 Ultrasonic sensor
 1
 200
 5
 20x4 LCD display
 1
 950
 6
 3.7 V power supply
 2
 900
 7
 Temperature sensor
 1
 250
 8
 Resistors
 10
 100
 9
 Load cell 5 kg
 1
 500
 10
 LDR
 1
 55
 11
 LED
 5
 100
 12
 Piezo Buzzer
 1
 25
 13
 HX711 converter
 1
 200
 14
 Battery charger
 1
 450
 15
 Cost for model
 1
 2000
 16
 Wires and others
 300
 Total
 11530
 Conclusion
 Team
 E/17/012, Aminda Amarasinghe, e17012@eng.pdn.ac.lk
 E/17/038, Anuruddha Chandrasekara, e17038@eng.pdn.ac.lk
 E/17/101, Anjalee Gunathilaka, e17101@eng.pdn.ac.lk
 Project Supervisors
 Dr. Isuru Nawinne
 Dr. Mahanama Wickramasinghe
 Aminda Amarasinghe
 Anuruddha Chandrasekara
 Anjalee Gunathilaka
 Department of Computer Engineering
 University of Peradeniya
 Department Projects
 Copyright © 2021 All rights reserved | MilkTab",cyber physical project to develop milk collecting process.,E17,Cyber-Physical Systems Projects,https://projects.ce.pdn.ac.lk/3yp/e17/Milk-Testing-and-Collecting-System/,https://github.com/cepdnaclk/e17-3yp-Milk-Testing-and-Collecting-System,https://cepdnaclk.github.io/e17-3yp-Milk-Testing-and-Collecting-System,https://cepdnaclk.github.io/e17-3yp-Milk-Testing-and-Collecting-System/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/3yp/E17/Milk-Testing-and-Collecting-System/
44,Remote Gatekeeping System,"Remote Gatekeeping System
 3YP(Group-19)
 Home
 Intro
 Components
 Timeline
 BOM
 Testing
 Scalability
 Security
 Team
 3rd Year ProjectDepartment of CE - UoP
 Remote Gatekeeping System
 Control unit wit|
 Control unit with intercom, Smart Mailbox, Remote gate
 unlocking system, Mobile controlling interface, Administrative
 web services
 Project Repo
 Web Application
 The motivation to our solution
 REAL WORLD PROBLEM
 In the busy and complicated lifestyle today, keeping
 interactions with outsiders is, inefficient, impractical and
 vulnerable. The ongoing pandemic has worsen the situation
 becase people are afraid to having physical intractions with
 outsiders.
 We have understood that people are having troubles with taking
 online deliveries to their door steps securely. Since people
 in Sri Lanka are so busy today, there is a good chance of not
 being in the house when an outsider or delivery person comes
 to the door.If the homeowners are not at home , delivery
 people are adapt to keep the delivery outside the
 house.Therefore delivery package is subjected to get stolen or
 get damaged.
 Otherthan that , when an outsider comes to the door, if the
 parents are not home, it is not a good idea to expose the
 children and elder people to that outsider. It would be better
 if there is a way to communicate with the outsider when she/he
 is at the gate.(before entering to the house premise)
 We thought we can come up with a system of interconnected
 devices (inspired from IoT) to provide an efficient solution
 for those problems.
 Next level delivery receiving and gatekeeping
 Our Solution
 Our solution is to introduce a system, to communicate remotely with
 an outsider who is at our gate or to undertaking deliveries, without
 making physical interactions.
 Watch the video
 How it works
 Components of the Sytem
 Control Unit
 This is the main component of the system and all the
 cotrolling related to the system is done using this
 unit.Outsider interacts with this component.
 Smart Mailbox
 This is used to receive the delivery packages safely and this
 can be remotely controlled by the homeowner from anywhere in
 the world.
 Smart Gate Locking
 Remotely controlled by the homeowner to serve the request of
 the outsider and locking and unlocking of the gate is done
 using this.
 Mobile Application
 Used by the homeowner and various features like capturing the
 photo of the outsider, intercom with the outsider and
 mailbox/gate controlling is handled using this.
 Web Interface
 Used to sign up, establish the connection between the system
 with the user account , download the mobile application and
 various other purposes.
 What happens under the hood
 Solution Architecture
 This is a high level representation of our system and it shows
 how our system works.
 All of the modules of our system is connected to the
 microprocessor Raspberry Pi 3 and it will be connected to the
 internet using built-in wifi module through the home wifi
 access point.
 Then it would be connected to the backend of our system which
 is firebase.Backend transfer data to the mobile application
 and the web interface. Connecting and data sharing between the
 hardware front-end and two software front-end is the main goal
 of the backend.
 Data/Control signal distribution
 Data and Control Flow
 This is a high level representation of our system and it shows
 how our the controlling and data transmission happens in the
 system.
 Purple Lines : Control Signals
 Black Lines :
 Data Transmission
 Control signals are supplied to all the modules that are
 connected to the Raspberry Pi3. Data is input to the Raspberry
 Pi by camera module, push button, usb microphone. The SD card
 is used to load the raspbian OS to the system and if there are
 any data to be stored, they are stored using SD card.
 IoT device is connected to the backend of our system therefore
 obvously data sharing happens there. Using the backend , the
 data transmission between mobile application, web site and the
 IoT device is achieved.
 Blocks and Components
 Block Diagram
 This figure shows how various components of our system
 interconnected.
 All the controling mechanisms are handled by the control unit
 of our system. When the homeowner wants to open up the smart
 mailbox , a control signal will be send to the control unit
 and control unit will send a control signal to the smart
 mailbox and it will be opened. This exact behaviour is applied
 for the smart gatelock as well.
 The control unit is going to connected to the power supply
 adapter and will get power from the wall plug. Then the power
 required for smart mailbox and smart gatelock will be provided
 by the control unit itself.
 Then it would be connected to the backend of our system which
 is firebase.Backend transfer data to the mobile application
 and the web interface. Connecting and data sharing between the
 hardware front-end and two software front-end is the main goal
 of the backend.
 UX-UI Design
 Mobile Application
 Login Screen
 Welcome Screen
 Active Event
 Close Active Event
 Previous Events
 Previous Event Info
 Download Mobile Application
 UX-UI Design
 Web Application
 Home Page
 Login From
 Signup From
 Dashboard
 UX-UI Design
 Hardware Interface
 Initialize system
 Choosing option
 Realtime Updates
 Realtime Updates
 Realtime Updates
 Failure Handling
 Algorithms of the System
 Algorithm runs on the hardware module
 Algorithm runs on the Mobile Application
 how we store the data
 Database Schemata
 We use Firebase Realtime Database as database of our system
 which is a cloud-hosted database. Data is stored as JSON and
 synchronized in realtime to every connected client. That means
 all the clients connected to our system share one Realtime
 Database instance and automatically receive updates with the
 newest data.
 Firebase Realtime Database is a non sql, dynamic and non
 relational database.
 Physical designs of our system
 3D Models
 Control Unit - Front End
 Control Unit - Isometric View
 -->
 How the modules are connected
 Circuit Diagram
 What we have implemented so far...
 Progress
 Initializtion Node
 Failure Handling
 Web App Demostration
 Interaction with the outsider Demostration
 -->
 hardware modules
 Sensors and Actuators
 Raspberry Pi Camera Module v2 (Sony IMX219)
 8 Megapixels
 Several image formats (JPEG, BMP, PNG) - smaller file size for
 faster and effective transmission
 300mA current, 1.2V
 RPi library: PiCamera
 16 x 2 LCD (CM 162-4)
 Affordable
 Suits our requirements
 Consumes less power compared to other options (3.3 ~ 5V)
 RPi library: Adafruit_CharLCD
 Servo Motor (SG90)
 Mechanical locks of the Mailbox and the Front gate
 Has enough torque to do small operations (2.5kgcm)
 Consumes less energy (5V) compared to other motors
 Mini USB Microphone (Adafruit 3367)
 Very reliable for voice recording, which allows optimum
 communication in Intercom
 Works great with a Raspberry Pi computer
 5V of electricity with a maximum current of 0.5A
 Factory Calibrated
 From Week 1 to Week 15
 Project Timeline
 Week 1 - Week 4
 Project Planing
 - Coming up with an idea
 - Getting familier with the tecknologies
 Week 5
 Project Proposal
 - Presentation explaining the Product and the Tecknologies
 Week 6 - Week 8
 Preperation and Learning Phase
 - Getting along with the tecknologies
 - Building the Algorithms
 Week 6 - Week 10
 Web App and Server Backend Dev
 - Development of the User registration system with ReactJS
 - Configuration of Firebase server backend and the database
 Week 7 - Week 12
 Mobile App Dev
 - Design of the application structure and laout
 - Implementation of the application
 Week 9 - Week 13
 Microcontroller Environment Dev
 - Implementation of Intercom capabilites
 - Setting up sensors and actuators
 Week 10 - Week 18
 Error Handling and Final Test
 - Handling the errors arise while combining the technologies
 - Testing of the final product
 - Fine tuning the system
 Proposed on Week 5
 Bill of Material
 Description
 Price - LKR
 Rasberry Pi 3 (Model B)
 8000
 Raspberry Pi Camera Module
 3800
 Raspberry Pi LCD
 2500
 Speaker Module
 2000
 USB Microphone Module
 1000
 Servo Motor × 2
 1000
 Micro SD card
 850
 Power Adapter
 800
 Other Expenses (Push Buttons, Cables, etc.)
 1000
 Total
 20950
 Testing
 Full test results can be found in the
 GitHub Repository
 Functionalaties ofthe Administrative Web Service
 Type : Automated
 Framework : Selenium
 Porgramming Languge : Python
 Test
 Description
 Purpose
 Expected Results
 Result
 User Login (i)
 Use logoing with correct email and password
 Check the user loging with with correct credentials
 Successful login to the system, Initial message of the History
 Table , Correct User name and the email appear on the Dashboard
 Pass
 User Login (ii)
 User loging with wrong email or password
 Try to use loging with wrong credential
 Blocking the invalid users
 Pass
 Device Initializtion (i)
 Device initialize with correct serial number & Valid use
 information
 Check whether the initialzaton work correct or not
 Successful signup to valid-user
 Pass
 Device Initializtion (ii)
 Device initialize with incorrect serial number
 Check whether the initialztion work with incorrect serial or not
 Unable to sign up an invalid user
 Pass
 Secutiry Fetures ofBackend & Frontend
 Type : Manual and
 Automated         Framework,
 Simulators & Libraris : Rules Playground - Firebase
 Test
 Description
 Purpose
 Expected Results
 Result
 Malicious user login
 Brute Forcing passwords
 Checking the security againsts the malicious access
 Security against the malicious access Blocking the selected
 email after a few unsuccessful attempts for a time period
 Pass
 Datebase Access (i)
 CRUD operations on the Users , Messages , InitNodes , Evernts ,
 Nodes collections
 To fine-grain tuning the outside access to the database
 Users - read and write access, authenticated users only for
 their collection.
 Nodes - Read access to any outside users, write access only to
 authenticate users for their collection
 Pass
 bakcend services
 Scalability
 For now, we are using the free tier of the Firestore. Cloud
 Firestore offers free quota that allows you to get started at
 no cost. The free quota amounts are listed here.
 Firebase offers upto 50,000 document writes per day. We have
 observed about 25 database reads will take for a single event.
 If we assume a normal user may encounter 10 events per day, we
 can handle about 200 nodes per day in the free tier.
 Firebase Realtime Database provides 100 simultaneous
 connections for the free tier. Since we are not expecting that
 much of events at the same time, it won't be a bottleneck. If
 it was, we can easily boost it to 200k by upgrading the system
 to paid tier.
 Firebase Realtime Database offers upto 5GB of data storage per
 day for the free tier. We have analysed for several test cases
 and found out average event may contains files upto 3MB. Since
 we are hoping to provide 1 month of backup data, it will be
 enough for 5 nodes. We can reduce the backup size to
 accommodate more users, or we can switch to paid tier.
 Additional 40GBs costs $1 and for that we can provide backup
 for 40 users.
 As a future Implementation, we are going to scale the system
 to have the ability of connecting several users for a single
 hardware node.
 simultaneousconnections
 upto 100
 initializeddevices
 upto 200+
 additionalbackup storage
 $1/ 40 users
 Security Aspects
 authentication
 To authenticate users with their email addresses and passwords, we
 are using Firebase Auth, which provides methods to
 create and manage users.
 At an event of Brute-force Attacks, we are temporarilty disabling
 the account
 which is being compromized to protect the system from the
 attackers. Firebase's
 Managed email-password authentication service
 tightens the default quota of the endpoints to prevent these
 attacks.
 To connect the backend to the hardware node and to communicate with,
 we are using a token-based authentication system, which uses
 JSON Web Tokens - JWT.
 authorization
 Firebase Security Rules secures the database and the storage,
 by fine-graining the user access. We have implemented our own rules
 which restricts the users from accessing unauthorized collections of
 the database.
 hosting
 We have deployed our web application using the Firebase service,
 Firebase Hosting. Since
 Zero-configuration SSL is built into Firebase Hosting, the
 content is always delivered securely, so we don't have to take
 care of it.
 web & mobile app
 We have implemented user input validation for both interfaces, at
 the client-end as well as at the server-end.
 Firebase Security Rules provide the validation at the
 server-end where as at the user interfaces we have implemented
 validation manully.
 Group 19
 Team Members
 Achintha Harshamal
 Computer Engineering UG
 Ishara Nawarathna
 Computer Engineering UG
 Pubudu Bandara
 Computer Engineering UG
 Advisors
 Dr. Isuru Nawinna
 Senior Lecturer
 Department of CE
 Dr. Mahanama Wickramasinge
 Senior Lecturer Department of CE
 Contact Info
 gatekeeper.remote@gmail.com
 Department of Computer Engineering
 Faculty of Engineering
 University of Peradeniya",A system which manages human interactions securely with outsiders.,E17,Cyber-Physical Systems Projects,https://projects.ce.pdn.ac.lk/3yp/e17/Remote-Gatekeeping-System/,https://github.com/cepdnaclk/e17-3yp-Remote-Gatekeeping-System,https://cepdnaclk.github.io/e17-3yp-Remote-Gatekeeping-System,https://cepdnaclk.github.io/e17-3yp-Remote-Gatekeeping-System/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/3yp/E17/Remote-Gatekeeping-System/
45,Secure Food Delivery,"Secure Food Delivery
 Home
 About
 Architecture
 Design
 Security
 More
 Testing
 Budget
 Timeline
 Conclusion
 Team
 Want to deliver your order to the doorstepmore secure manner?
 Secure Food Delivery
 Let's explore the new way to get your order !
 Project Repository
 Current Scenario
 As we are really busy with our day-to-day life, almost everyone is like to have their meal wherever and whenever they want.
 So, online food delivery services are really famous and very popular among people. There are so many food delivery services in Sri Lanka and there are huge beneficiaries also.
 Motivation
 There are some services, that are providing online food delivery services already. But, there are some serious issues regarding those services. The main thing is about security (Reliability/Trust).
 In this particular area, those questions 'How far do we can trust those services? as a customer and 'How far do we can trust those services? as a restaurant owner can occur.
 There are two major roles here. The first one is the restaurant owners who provide food through delivery services and the second one is the customer who orders food through delivery service (Website or Mobile App).
 We saw some complaints and some news about stating 'not delivering the ordered food accordingly' from the customers. Also, we could see those restaurant owners could not do anything about it since the delivery service provides the service of delivery. But they have an issue with their customer's trust. This becomes a problem in good service and we want to give a solution that would help to develop the online food delivery services more reliable.
 Our Solution
 As it becomes a real-world problem, we thought about having a solution for that.
 We provide a smart locking system for the food delivery carrier, which can only be accessible from two ends. The person from the restaurant side providing food can unlock the container using an RFID card and put the order into it. After locking the container, it can only be unlocked by the customer using an OTP or an RFID card, or both.
 That must satisfy customers as it can't be opened within the delivery. So, they can trust whatever they buy. And also both client and customer have proof about the delivery.
 Design Architecture
 Solution Architecture
 The solution architecture consists of both software and hardware node. In the software node, there is an API that is responsible for handle data. Also, there is a mobile APP that will use to access the lock, and the client system is the one that assigns a particular order to the related delivery carrier.
 In the hardware node, using a microcontroller, the carrier will be locked with a digital lock. To unlock the delivery carrier, an RFID tag or OTP will use through the mobile app. If there is an unauthorized opening then it will be detected. By using an LCD display, the necessary information such as OrderID that is in the carrier at that moment; will be displayed.
 Control and Data flow
 In the control and data flow, there are three significant roles in the system. They are restaurant admin, regular customers, and delivery riders.
 Once a regular customer ordered foods through the existing online food delivery service system, he will get an Order-ID. Then on the restaurant side, the restaurant admin will confirm the order and put the order in one of the available motorbike carriers by using provided RFID tag for that restaurant. In this process, the restaurant admin will confirm the order with that particular Carrier-ID. Then, the mobile number of that regular customer who ordered the food; will be sent to the SFD API with the Order-ID and Carrier-ID.
 In the SFD API, the OTP (One-Time-Password) will send to the customer. Also, API will search for the mobile number in the database to check whether that customer has registered for the RFID tag, then if yes, the relevant RFID code will send to the microcontroller.
 Once the regular customer logs into SFD's app, and if he entered the correct OTP then an unlocking signal will be sent to the microcontroller through the API. Here is the delivery rider's job once he confirms that particular order has arrived at the customer's place, then only the locker is available for the customer to unlock. Until then, the customer is not allowed to open the delivery carrier and that will display to the customer through the mobile app.
 Here is the delivery rider's job once he confirms that particular order has arrived at the customer's place, then only the locker is available for the customer to unlock. Until then, the customer is not allowed to open the delivery carrier and that will display to the customer through the mobile app.
 In the locking system, there is an unauthorized opening detection unit for the delivery carrier. Whenever the delivery carrier opened, it will generate an interrupt signal, if the unlocking signal from the API is not present at the microcontroller, it will alert the API as an unauthorized opening. Otherwise, the interrupt will ignore.
 The current status of the delivery carrier will display through the LED indicators, and the current order's Order-ID will display on the LCD screen.
 Design of the System
 Database ER Diagram
 Hardware - Power, Data, and Control Flow
 Circuit Design
 Delivery Box 3D Model Design
 UI Design
 Welcome Page
 This is the landing page of both customer and rider
 .
 Register Page for Customer
 If any customer wants to register in SFD service
 they can register by providing above details
 .
 Login Page For Customer
 To unlock the device, customer has to login in to the SFD app
 by providing contact details and the order id
 .
 Unlock Page using OTP
 After successfully logged in
 customers can unlock the device using OTP
 .
 Register Page for Rider
 If a rider wants to register in SFD service
 they have to provide above details with pre registered Device ID
 .
 Login Page for Rider
 Riders can log in to the app using contact details and the password
 .
 Order List Page for Rider
 After rider login, they can see their assigned orders
 also they can confirm orders by clicking the arrow button
 .
 Previous
 Next
 Hardware Components
 Microcontroller
 Communication Device
 Locking Device
 LCD Display
 RFID Sensor
 Unauthorized Access Detector
 BMS
 ESP32 Devkit V1
 Microcontroller unit of the system
 240 GHz Dual Core Processor
 4MB flash memory
 32-bit architecture
 512 KB RAM
 GPIO pins
 Peripherals (ADC/DAC/SPI/UART/I2C/PWM)
 Total of 30 Pins
 SIM800L GSM-GPRS EVB V2
 Communicational device of the system
 Communicate via GPRS
 Quad-band 850/900/1800/1900MHz
 AT Commands interface with “Auto Baud” detection
 Connect via UART protocol
 Clock Rate : 256kHz
 Data rate : 1.2 kbps - 115 kbps
 Consumes 2A in active mode, 0.7A in sleep mode
 12V Solenoid Lock
 Draws 650mA at 12V
 1-10 seconds long activation
 20x4 LCD Display
 20 Characters x 4 lines
 Connect via I2C interface
 LCD consumes 2mA
 Backlight consumes 40mA
 MFRC522 RFID Module
 Data Rate : 420 kbps
 Clock speed : 13.56 MHz
 Communication with Microcontroller over 4 pin SPI
 13 - 26 mA current consumption
 A3144E Hall Effect Sensor
 Operating Voltages : 4.5-24 V
 Maximum current : 25mA
 4S Battery Management System
 4 cells in series
 Maximum rated charge/discharge current : 30A
 Charging voltage : 16.8 V
 Balance current : 60mA
 Software Technologies
 Flutter
 For the implementation of the mobile aplication, Flutter has chosen as it supports both android and IOS applications with single code base.
 NodeJs
 To built the API, Node.js javascript runtime enviroment is used.Single treaded non-blocking operations, make the speed of the application much higher.
 MySQL
 As the database, the API uses a relational database.MySQL is used to build the relational database for the API.
 AWS Services
 AWS EC2 Service is used to host the API.For the MQTT connections, the AWS IoT Core is being plan to use.
 Security
 Security is the most important thing in the IT world right now.
 In our solution, we have deeply considered the security point of view of our system.
 We have developed our system with the help of the following techniques to enhance the application security in our solution.
 JSON Web Tokens (JWT)
 For authentication purposes, our API uses the JSON Web Token to ensure integrity.
 In the successful event of login, the user must receive a JWT, which is signed by the server.
 Whenever the client wants to communicate with API (after the login stage), it has to pass that returned token with the
 request.
 If somehow the token is modified, API would deny access to the data.
 Input Validations
 User Inputs are the most common way to commit a malicious attack.
 To prevent such scenarios, we validate the user input from both the front-end application and the back-end API.
 If inputs are not in the correct order, in the front-end, it denies the inputs. And from the back-end, it denies the request.
 Encrypt Sensitive Data
 Although we are using a relational database to work with data, we do not directly store sensitive data on the database.
 For that, we encrypt the sensitive data on the API with the help of well-known powerful libraries and then store those in the database.
 SSL/TLS Communication
 To ensure secure communication between the mobile application, and the API, we used the HTTPS protocol to securely transmit the data.
 With that, we can guarantee the message's integrity as well as the confidentiality of the data.
 In our API itself is secured with AWS EC2 security features. Hence the security of the system is more in an advanced manner.
 MQTT Broker Authentication
 Hardware to hardware communication (API and the Microcontroller), we have used the MQTT as the messaging protocol.
 For that, we have currently used AWS IoT Core as an MQTT message broker, which is a secured private message broker that requires certificate-based authentication for the MQTT clients.
 Testing Plan
 Budget
 Project Timeline
 Our Team Members
 Mahela Ekanayake
 E/17/083
 Lahiru Pathum
 E/17/405
 Nadeesha Diwakara
 E/17/240
 Our Advisors
 Dr. Isuru Nawinne
 Dr. Mahanama Wickramasinghe
 About Us
 We are a group of undergraduates from Department of Computer Engineering, University of Peradeniya. This system was implemented as our third year project.
 Mail to us
 Quick Links
 Project Repository
 Department of Computer Engineering
 Faculty of Engineering
 University of Peradeniya","Secure Food Delivery is a system that will help users to get delivered their ordered foods securely. Also, it will privilege for sellers to gain their success by growing the trust in users' minds.",E17,Cyber-Physical Systems Projects,https://projects.ce.pdn.ac.lk/3yp/e17/Secure-Food-Delivery/,https://github.com/cepdnaclk/e17-3yp-Secure-Food-Delivery,https://cepdnaclk.github.io/e17-3yp-Secure-Food-Delivery,https://cepdnaclk.github.io/e17-3yp-Secure-Food-Delivery/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/3yp/E17/Secure-Food-Delivery/
46,Smart Cradle,"Smart Cradle
 Home
 Introduction
 Solution Architecture
 Software Design
 Hardware Design
 Security AND SAFETY
 Testing
 Budget
 TimeLine
 Team
 Contact Us
 Smart Cradle
 ""Because you and baby deserve care!""
 PROBLEM
 As we are very well familiar with the hurdles faced by Parents to nurture their infant and especially in case if both the Parents are working.
 To give 24 hours of time in such cases is next to impossible.
 However they still need to look after their babies,thereby increasing workload and stress.
 They either send their babies to their parents or hire a baby caregiver while they are working.
 Some parents worry about the safety of their babies in the care of others.Thus they go home to check on their babies during their free time,
 such as lunch or tea break.
 A baby Monitoring System that can monitor the babies' condition real time is proposed to solve these problems.
 A baby monitoring System consisting of a vedio camera and microphone without limitations of coverage.It can send data and immediately notify the
 parents about urgent situations,thereby shortening the time needed to handle such scenarios.
 OUR SOLUTION
 The proposed solution involves live monitoring of the child through a mobile application remotely.Noice Sensor for the detection of the child's
 crying activity.When detect crying, cradle send a message to parent,plays a song.The Thermal sensor notifies the parent about the envioremental
 temperature near the baby and switch on the fan automatically with temperature.
 The proposed system uses the cloud service for remotely monitoring the child.
 Learn more
 SOLUTION ARCHITECTURE
 There is a cry detection mechanism which detects cry of the baby and send instant mobile app notifications to the user, at the same time some music will be played to soothe the baby.User can use play music option using their mobile application according to their wish at any occation even if a cry is not detected in order to soothe the baby.
 Using the temperature sensors, room temperature is delected and if it exceed 30 celcius ,mini fan will be turned on automatically.Turn on and off functions of the mini fan can be fully controled using the mobile application also according to the need of the user.Swing of the cradle is fully controlled by the Mobile application.
 Central server(AWS ) is used to maintain a database to keep a track of registrations and logins.Whenever the user log in to the system, mobile application will conect to the server and verify authentications.
 Hardware Control Flow
 Software Control Flow
 ER Diagram
 EER Diagram
 SOFTWARE DESIGN
 Mobile application is designed in a more user friendly manner.Graphics and picture are used to repesent the functionalites/options of the smart cradle.This will clearly deliver the content to the user and they can easily navigate to their requirements without much frustration.
 Basic functionaites of the mobile application:
 Monitor the baby
 Swing the Cradle
 Check the room temperature
 Play Music
 Switch on the Fan
 Get Notifications
 User have to first register giving their basic information,thereafter whenever they use the application,
 they can login giving their user name and the passward.This will assure our users that their information is protected and secure.
 Since real time visual monitoring of the baby option is available in the mobile application, assuring the privacy of the users as well as thier babies is more important.
 Because of high protection over the all the user information and the passwords,users can be make user that no unauthorized person can see their baby and no third party people can access the control of the smart cradle
 UI Design
 Front End Validation
 All the user inputs are checked before sending them to the server
 SignUp and Login
 When adding, removing and selecting a device
 Backend Validation
 Alert Boxes are displayed according to the resposnses recieved from the server
 SignUp
 Login
 Select a device
 Music
 Swing
 Settings
 Add a Device
 Remove Device
 Mobile App Demostration
 Technologies used
 HARDWARE DESIGN
 Hardware Components
 NodeMCU
 GPIO pins + WIFI module inbuilt
 Operating Voltage: 3.3V
 Input Voltage: 7-12V
 Flash Memory: 4 MB
 SRAM: 64 KB
 Clock Speed: 80 MHz
 Sound sensor
 Operating Voltage: 3.3V to 5V DC
 LM393 comparator with threshold prest
 Induction distance: 0.5 Meter
 Operating current:
 4~5 mA
 Microphone Sensitivity (1kHz): 52 to 48 dB
 Small, cheap and easily available
 Motor Driver
 Power Supply: DC 5 V - 35 V
 Operating current range: 0 ~ 36mA
 On-board +5V regulated Output supply
 Thermal sensor
 Working voltage: DC 3.3-5V
 can measure : -55°C to 150°C
 ±0.5°C
 Accuracy
 Drain current is less than 60uA
 Low cost temperature sensor
 Small and hence suitable for remote applications
 Servo Motor
 Torque: 5.5kg/cm (at 4.8V)
 Working Voltage: 4.8V-6V
 Current Usage: <1000mA
 Camera Shield
 Voltage: 2.5V to 3.0V
 Lens Size: 1/6"" , Vision Angle: 25 degree
 High sensitivity for low-light operation
 Color saturation level auto adjust
 Mini fan
 DC Mode: 450 ~ 2150 RPM
 Operating Voltage: DC 5V～13.2V
 speaker
 Schematic Circuit Diagram
 PCB Design
 3D Model
 SECURITY AND SAFTY
 When we consider about a product like cradle ,security and safety is the most important thing.Security features of this product
 are achieved by using the above technologies.
 AWS :
 This is the cloud based serever that we use.We have choosen it ,because it provides secure services.
 It uses certificates to authenticate machine to machine communication and provides policies to control the
 actions of the devices.
 Amazon RDS :
 We create our own database server in cloud, and amzon RDS is the storage service that we use.We have choosen it because
 it supports MySQL and it is secure.It uses data encryption to secure data.The data stored in the disk is encrypted.
 The data which is transmited via the network is also encrypted.
 JSON Web Token (JWT) : This is used for the authetication.Once the user login to the system the server sends this token to the user.
 Passward Hashing : The orginal password is not stored in the database.Hashing performs a one-way
 transformation on a passward, turning the passward into another string.
 Sending JSON Web Token at the login.
 Store hashed passwords in the MySQL database.
 TESTING
 POSTMAN is a scalable API testing tool.
 HTTP requests that are sent to the server in each api call, are tested using this tools.
 Mobile application testing is done using APPIUM
 This enables testers to write test scripts against multiple platforms such as iOS and Android using the same API.
 Flutter test package is used for unit testing
 Front end validation is done using this
 API testing with POSTMAN
 Frontend And Backend Testing
 Testing Summary
 WHY we tested,
 signup and login : It is important to make sure that the system is only accessible by authorized users with accurate user details.
 add device and remove device : It is important to make sure that the ‘device’ and the ‘ownership’ tables in the database are correctly updated.
 select device : It is important to make sure that the devices are
 only accessible by their
 owners.
 swing , paly music ,fan , temperature monitor and settings : It is important to make sure that the basic functionalities of the mobile application are working properly
 backend : It is important to make sure that the all API endpoints are working correctly.
 BUDGET
 TIMELINE
 TEAM
 Our Team Members
 Madush Dilshan
 E/17/040
 Shashini Upekha
 E/17/356
 Hasara Wijesooriya
 E/17/407
 Our Advisors
 Dr.Isuru Nawinne
 Dr.Mahanama
 Contact Us
 University of Peradeniya.
 Phone: +94 81 239 33 00
 Email: vc@pdn.ac.lk
 Web-site: http://www.pdn.ac.lk/
 Faculty of Engineering.
 Phone: +94 81 239 33 02
 Web-site: http://eng.pdn.ac.lk/
 Computer Engineering Department.
 Phone: +94 81 239 39 14
 Web-site: http://www.ce.pdn.ac.lk/
 © Untitled. All rights reserved.Design: e17-3yp-Smart-Cradle_UOP",An automated cradle which is controlled by a mobile application.,E17,Cyber-Physical Systems Projects,https://projects.ce.pdn.ac.lk/3yp/e17/Smart-Cradle/,https://github.com/cepdnaclk/e17-3yp-Smart-Cradle,https://cepdnaclk.github.io/e17-3yp-Smart-Cradle,https://cepdnaclk.github.io/e17-3yp-Smart-Cradle/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/3yp/E17/Smart-Cradle/
47,Smart Locker,"Smart Locker
 Overview
 Solution
 Mobile App
 Architecture
 Technologies
 Security & Failure Handling
 Testing
 Budget
 Demonstration plan
 Timeline
 Supervisers & Mentors
 Overview
 The Smart Locker is a improvement for the traditional locker where we are trying to provide the lockers for public usage.
 Normal traditional lockers were used privately but as a team we think if lockers can be provided in a public way where citizens will
 book lockers for their access in public places it would be a market opportunity.
 We are as a team will develop a locker with digital improvements where users can unlock the locker by purchasing it online for a certain time period.
 The Lock itself will be developed by a arduino based electronic system where set of lockers will be monitered by a centeralized system.
 Motivation
 People from remote areas travel a long distance to the town in their day-to-day life. Employees come for their jobs.
 Students come for educational purposes such as schools and tuition classes. All these times they carry a lot of things here and there
 because they have to carry everything that they need in one go. So as a team we understand it is very difficult for those people.
 Then we came up with the idea of public lockers for making their day-to-day life easy.
 As a result we also understand that there are many people who can benefit from our solution rather than people who live in remote areas. This led to finalizing our idea as well.
 Our Solution
 ⚫ Operable by a phone’s mobile application
 ⚫ With map assistant to find where the lockers are
 ⚫ Users can book lockers using application for certain duration of time.
 ⚫ When booking users will receive a token.
 ⚫ User can either use their phone or enter the token number to unlock the locker
 ⚫ Token can share with another trusted person for accessing the locker by himself.
 Mobile Application
 Infrastructure and Architecture
 Components Used Inside the Locker
 This system consists of three main segments. The first one is the Locker. Others are
 backend and frontend which is also known as the mobile application. Smart locker includes four main parts.
 There are two input devices and they are Keypad and Ultrasonic sensor. There are two output devices and
 they are LCD display and solenoid lock. The keypad is used to enter tokens and other data. The
 Ultrasonic Sensor, helps the user to detect locker is empty or not. Through LCD display user can observe
 messages from the locker and the token which is entred.
 Initially, general information of lockers is stored in the cloud server database and it provides data to
 the mobile application and lockers. Also, it collects user data through the mobile application and the Locker. Users
 can ask about lockers and it gives details such as availability etc. If the user books a locker, then the
 backend generates a new token and sends it to the locker and user. The main functionalities of the locker
 are handled by the microcontroller and it makes a connection to the server through a Wi-Fi network. In
 advance, as a security feature buzzer is used. Finally, to enhance user experiance two LEDs are used for
 correctly identify whether the lock is in locked mode or not.
 Circuit Diagram
 Database Schema
 Technologies Used
 Security and Privacy
 In Software
 ⚫ Prevent unauthorized access from the mobile application
 ⚫ Block user IPs using a rate limiter
 ⚫ Validate user inputs
 ⚫ JSON Web Tokens for authorization
 ⚫ JWT signature validation
 ⚫ Encrypt the user password in the database
 ⚫ Update locker password after every single usage
 ⚫ Record login details
 ⚫ Block unwanted traffic using built-in AWS firewalls
 In Hardware
 ⚫ Durable Solenoid locks and lockers
 ⚫ Sound buzzer if something wrong went wrong or malicious thing happen
 ⚫ Private wi-fi connection for each cluster of lockers
 Failure Handling
 ⚫ Solenoid lock is in Locked state at a power failure
 ⚫ Solenoid lock is in Locked state at a server failure
 ⚫ Show warnings to users
 ⚫ Data fetched from sensors are stored within the device for a certaion period
 ⚫ Send that data again to the server to mitigate if lost updates happen
 Testing Plan
 Budget
 Demonstration plan
 We are implementing two lockers as a bunch with the functionalities described above.
 Then an item is put in one locker. So it is unavailable. One of our team members books the
 available locker using the mobile application with map assistance. Then he opens it through the mobile
 app and puts something in it. Afterwards, he shares the token that he received with another team member. This other
 guy opens the same locker using the mobile application as well as the keypad by entering the token. Moreover, after
 expiring the time duration, the owner cannot open the locker furthermore.
 TimeLine
 Supervisers
 Dr. Isuru Nawinne
 Dr. Mahanama Wickramasinghe
 Mentors
 Mr. Dinidu Thilakarathna
 Group Members
 PGAP Gallage - E/17/091
 KPCDB Jayaweera - E/17/144
 KHSP Kodagoda - E/17/168
 Project Page
 Github Project Repo",,E17,Cyber-Physical Systems Projects,https://projects.ce.pdn.ac.lk/3yp/e17/Smart-Locker/,https://github.com/cepdnaclk/e17-3yp-Smart-Locker,https://cepdnaclk.github.io/e17-3yp-Smart-Locker,https://cepdnaclk.github.io/e17-3yp-Smart-Locker/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/3yp/E17/Smart-Locker/
48,Smart Pet Feeder,"Smart Pet FeederYou need to enable JavaScript to run this app.↑HomeAboutArchitecturesDesignsHardwareSecurityProgressTestingTeamWELCOME TOSmart Pet Feedersaving one pet won't change the world, but for that one pet the world will change foreverMobile AppPlay VideoPlayCurrent Time 0:00/Duration Time -:-Progress: NaN%Non-FullscreenThe ProblemSmart Pet Feeder is a product that helps you to take care of your pets. It will help you to build the relationship with your pet better and better even you are not in the home. Have you ever been worried about your pet's meals when you are away from your pet? We provide the platform to come up with this problemThe SolutionWhen people are getting busy, they forget to take care of their pets even though they love their pets. Taking care of a pet's diet can be hard if they want to take good care of their pet's health. A smart pet feeder is one of the best solutions for that. It is capable of feeding a pet, in absence of its master. So, though the master is not at home, his pet will not miss his food. Smart pet feeders can be controlled by using a mobile app or a website. A small camera, which is mounted on the pet feeder, allows the master to see the machine's surroundings and observe the pet's behavior. Master can move the machine remotely while watching through the camera, to find the pet and deliver his food on time. If the master is too busy, even to operate it remotely through the mobile app or the website, he can switch on the automatic mode and schedule when to give food. A container that is placed on the machine can be used to store the foods and when the pet is being fed, the right amount can be passed to the plateReal-time visualizationGet the real-time activities of your pet, through the camera .SchedulingSchedule a feeding plan when you do not have time to control it manuallyHD live streamingRGet high quality video frames and using the UI to see real-time activities around.UPTO 4 TIMESFeed your pet 4 times per a dayEASY INSTALLATIONSetup the pet feeder and connect to the UI easilySTATUS VISUALIZATIONSee the remaining rounds, battery percentage and scheduling plan through the UI or the in-build displaySolution ArchitectureThe main device of our system is the pet feeder. It is connected to the Home Wifi and home Wifi is connected to the AWS server through the internet. In order to communicate with the AWS server there'll be a mobile application as well as a web application. Initially the user needs to log into the mobile application or web application by entering their email and the password. After logging into the system, they can control the pet feeder in order to feed their pets. Users can get real time visualization of their pets through the camera which is mounted on the pet feeder. To get a clear view of the pets, users can rotate the camera through the UI. If the users are unable to manually feed the pets, they can use the scheduling option. So they can create a scheduling plan in order to feed their pets at a given time. Users can see the status(Remaining feeding times, Battery capacity) of the pet feeder through the UI or inbuilt display.Data FlowThe users of the pet feeder can schedule a plan or control it manually using the website or a mobile app. Then From the UI, data will get into the Web server and the microprocessor in the Raspberry pi 3 will receive the data from the server. And also the users of the pet feeder can see their pet using the camera, which is in the feeding machine. That camera can be rotated remotely, and the live stream data will be sent to the UI through the AWS cloud. Mainly there are two different control units in the feeding machine.Food serving unitVisualizing unitFood serving unit is responsible for food serving. This unit contains a stepper motor and a motor controller. In the machine there is a food container which has a cylindrical shape and it has divided in to four partitions. To serve the foods, the food container in the pet feeder should be rotated to a certain angle. That is done by using the stepper motor. The raspberry pi will send the relevant control signals to the motor controller and the motor controller will control the rotation of the motor according to that signals.Visualizing unit is responsible for live streaming. There is a 5MP Omnivision 5647 Camera Module in this unit. When user wants to get a real time visualization of his/her pet, he/she will be able to get the live stream data to the UI through this camera. Also a 0.91 Inch LCD Display has included to this unit, and it will be used to display the data such as battery level or feeding times etc.AWS ArchitectureInitially the pet feeder will be configured as a thing in the AWS IoT. Pet feeder and AWS IoT will communicate using the MQTT protocol. When IoT receives a message from pet feeder, AWS IoT will execute a lambda function according to the rules that are defined. AWS IAM is used for Authentication purposes. AWS DynamoDB is used as the storage. It interacts with AWS lambda functions. AWS lambda The CRUD operations of AWS DynamoDB is performed by AWS Lambda functions.Live streaming data from the pet feeder is directed to the AWS Kinesis. That data will be processed using video processing application and converts to formats like MPEG4. Converted Streams will be sent to the UI for streaming.The UI can communicate with the UI via the AWS API gateway. When request is received to the gateway, API gateway will execute relevant lambda functions. Before executing relevant lambda functions, the request will be validated using another AWS lambda function. For that token based authentication is used.3D Design Of The Pet FeederThe Pet FeederThis is the 3D Design of the pet feeder. Mainly it includes a food container and a camera.The Food Container.This is a cylindrical shaped food container, which has diveded in to four partitions and it is rotatable through its axis. Every partition has a opening at the bottom, and there is a path to the food plate from the bottom of the cylinder. To serve foods the relevent partition should be coincided its opening with the path.UI DesignUsers can log into the system using the mobile application or web application by entering their email and password. After logging into the system, they can see the current status of the pet feeder. Status information includes remaining feeding times, scheduling plan and the battery capacity. They will be able to feed their pets by selecting the feeding option in the UI. And also they can get a real time visualization of their pets through the UI. To get a clear view, the UI provides another feature to rotate the inbuilt camera. There is a special feature called scheduling which allows users to schedule a feeding plan through the UI to feed their pets at a given time.allhomeuseradminHomeSign UpLoginUser StatusUser HistoryUser FeedbackUser NotificationsAdmin UsersAdmin FeedbackHardware componentsController PlatformCPU: 4 x ARM Cortex-A53 , 1.2GHzOS comes pre-loaded with python programming languageEnd nodes are connected to hardware interfaces.4GB SD card is used as memory5V main power supplyAs the main Controller Platform Raspberry Pi 3 Model B is used. It comes with pre loaded python programming language. It has 4 x ARM Cortex-A53 CPU which have 1.2GHz processing speed. Has a seperate Camera Serial Interface. Also 40 GPIO pins. Raspberry Pi 3 Model B comes with onboard Wi-Fi network interface which has about 38Mbps bandwith.As the other hardware components, Is has included a Camera Module v1.3 (MD0263), 0.91 Inch LCD Display, L298N Dual Bridge DC Motor Controllers and 12v Stepper motor. There are some main reasons to use the MD0263 camera module such as its high resolution, frame rate and ability to connect directly to microprocessor through camera serial interface. As the other hardware components,It has included an 5MP Omnivision Camera Module, 0.91 Inch LCD Display, L298N Dual Bridge DC Motor Controllers, 12v Stepper motor, Infrared IR Sensor and 5V Realy Module.SENSORSThe IR Sensor will be used to set the food container to its initial position and that Sensor shoul be given a input voltage between 3-5V.ACTUATORSWhen considering the actuators the 12v bipolar junction stepper motor has 200 step per revelution and it is capable of giving a high-torque up to 40 N.cm. Then the raspberry pi camera mocdule will give a Full HD video quality of 1080p with 30fps and if it reduced the quality to 720p the frame rate can be increased to 60fps. And the LCD display will be used for display the data such as signal strength, battery level and next feeding time.Also a motor controller has used to control the speed, direction and rotating angle of the stepper motor. A 5V relay module has used to supply the 5V input to the Raspberry PI from the 12V battery.As the power supply component, It has used a 12V Lithium battery of 3000mAh. Its Good capacity, lightweight, and rechargeability are very helpful to reduce the total weight and keep the machine active for a long time using battery current.Circuit DiagramSecurity Aspects2 factor authentication for login is used as a security mechanism. When user trying to login to the system he will receive OTP to his mobile phone. So If an attacker steals the email and password of a user he cannot login to the system unless he has owner’s mobile phone.Another security mechanism is AWS Web application firewalls. The firewall helps to protect our API from common web attacks and bots.Json web token are used to communicate between API and the UI. After user login to the system API will given a token to the frontend. So UI send request to the API along with the token. So the attackers cannot access our API without the token.BudgetProgressSIGNUP AND LOGINPlay VideoPlayCurrent Time 0:00/Duration Time -:-Progress: NaN%Non-FullscreenUSER FUNCTIONALITIESPlay VideoPlayCurrent Time 0:00/Duration Time -:-Progress: NaN%Non-FullscreenADMIN FUNCTIONALITIESPlay VideoPlayCurrent Time 0:00/Duration Time -:-Progress: NaN%Non-FullscreenMOBILE APPLICATIONPlay VideoPlayCurrent Time 0:00/Duration Time -:-Progress: NaN%Non-FullscreenIOT CONNECTIVITYPlay VideoPlayCurrent Time 0:00/Duration Time -:-Progress: NaN%Non-FullscreenTestingSOFTWARE TESTINGPlay VideoPlayCurrent Time 0:00/Duration Time -:-Progress: NaN%Non-FullscreenSECURITY TESTINGAPI TESTINGPlay VideoPlayCurrent Time 0:00/Duration Time -:-Progress: NaN%Non-FullscreenE2E TESTINGPlay VideoPlayCurrent Time 0:00/Duration Time -:-Progress: NaN%Non-FullscreenTesting ResultsPlay VideoPlayCurrent Time 0:00/Duration Time -:-Progress: NaN%Non-FullscreenTimelineProject Proposal(Milestone 1)Presenting our project proposal was the first milestone.19th July, 2021Design 3D ModelsDesign 3D models of the pet feeder.25th July, 2021Circuit & Software DesignDesign block diagrams, circuit diagrams, database schema, UI. Draw flow charts and design the algorithms.10th August, 2021Progress Review (Milestone 2)Tentative evaluation criteriaBlock diagramsCircuit diagramsDatabase schemataAlgorithms / Flow ChartsUI DesignsPerformance, Power, Security requirements.Failiure handlingSensors and actuatos.Controller platforms (programming, memory, available interfaces, connectivity, speeds, data-rates, built-in units, power, security, cost, etc.).Network technologies and protocols (interfacing, medium, bandwidth, security, availability, reliability).Back-end technologies (programming, storage, accessing, backups, security, cost, 3rd party services)Front-end technologies (programming, data visualization, security).30th August, 2021Front-End Of The Web ApplicationDesign and implement front end of the web application using React JS15th September, 2021Front-End Of The Mobile ApplicationDesign and implement front end of the mobile application using React Native25th September, 2021Develop Back-EndImplement the database and the back-end of the smart pet feeder using Node.js And MongoDB05th October, 2021Deployment And Software TestingDeploy the application in AWS servers and test the software20th October, 2021Progress Review(Milestone 3)Tentative evaluation criteriaCompleteness of back-end softwareCompleteness of front-end softwareCloud deploymentClear overview of the systemEnhance the user experience of software/hardware components and of the overall productClearly explain features and functionalities (including reliability, scalability and security aspects)Clearly explain implementation details27th October, 2021Implement Hardware PartDesign and implement the pet feeder unitNot Started YetConnect Software and HardwareEstablished the connection between pet feeder, cloud and the UINot Started YetProgress Review(Milestone 4)Tentative evaluation criteriaWorking PrototypeProgress video clipProgress presentationViva voceNot Started YetTestingTest the smart pet feeder and do relevant updates Not Started YetComplete product(Milestone 5)Tentative evaluation criteriaPresentationDemonstration of working productDesign Manual, User Manual, GitHub Repository, GitHub PageNot Started YetOur TeamR M S M GunathilakaE/17/100K S D PereraE/17/246R L D A S RathnayakeE/17/284Our AdvisorsDr. Isuru NawinneAdvisorDr. Mahanama WickramasingheAdvisorSmart Pet Feeder is a product that helps you to take care of your pets. It will help you to build the relationship with your pet better and better even you are not in the home. Have you ever been worried about your pet's meals when you are away from your pet? Smart pet feeder provide the platform to come up with this problemUseful LinksAbout UsContact UsContact UsSmart-pet-feeder, UOP, Kandy+94 76 869 9448+94 76 682 1877smartpetfeederuop@gmail.comwww.smartpetfeederuop.com© 2021 Smart Pet Feeder. All rights reserved",Smart Pet Feeder is a product that helps you to take care of your pets. It will help you to build the relationship with your pet better and better even you are not in the home. Have you ever been worried about your pet's meals when you are away from your pet? Smart pet feeder provide the platform to come up with this problem,E17,Cyber-Physical Systems Projects,https://projects.ce.pdn.ac.lk/3yp/e17/Smart-Pet-Feeder/,https://github.com/cepdnaclk/e17-3yp-Smart-Pet-Feeder,https://cepdnaclk.github.io/e17-3yp-Smart-Pet-Feeder,https://cepdnaclk.github.io/e17-3yp-Smart-Pet-Feeder/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/3yp/E17/Smart-Pet-Feeder/
49,Smart Pour,"Smart pour
 Home
 About
 Features
 Architecture
 Design
 Processing Unit
 Circuit Diagram
 E-R Diagram
 UI/UX
 Demos
 Mobile App Demo
 Backend Demo
 3D Model
 Data Flow
 Testing and Security
 Others
 Budget
 Timeline
 Team
 Now you can feel the Energy
 Coffee Making
 at your fingertips
 Project
 Repository
 Smart Pour
 An automated coffee machine that is controlled by an app
 The Smart Pour is an improvement for traditional coffee makers.
 Traditional coffee making is inconvenient with today's hectic
 lifestyle. We, as a team, came up with a more convenient coffee
 making system that can be controlled by an app.
 Features
 Make your life easier with Smart Pour
 Scheduling
 You can schedule your coffee at anytime and enjoy a hot coffee
 whenever you want
 Ingredient tracking
 You can know the ingredient amounts from the mobile application
 and shortages by notifications.
 Customizing user’s preference
 Smart Pour can store the recipes of the users.So you can
 experience your favourite coffee without any effort
 User friendly interface
 Smart Pour mobile application is very easy to handle and it's
 with simple structure which anybody can understand
 High Security
 Smart Pour is high security product only users with password can
 operate through the mobile application
 Durability
 Smart Pour device can be used for a long time and it will
 provide you a satisfied service
 Solution Architecture
 Mobile Application
 An Android mobile application is available for SmartPour coffee
 machine to allow users to remotely make coffee in a scheduled
 manner as well. The mobile application sends a notification when
 the coffee is made and reminders for scheduled coffee.Users can
 log in to the mobile app anytime to check the availability of
 the ingredients to make coff.The app is designed using Flutter.
 Web Server
 AWS server is used as the web server for the SmartPour System.
 Signals from sensors are passed to the mobile application as
 well as control information from mobile applications are passed
 to the machine and response mechanisms through this server.
 Database which is used to store favourite recipes,
 availabilityof the ingredients and logs of the system is hosted
 at the server.
 SmartPour Machine
 The coffe making is done in the machine. The pouring of coffee,
 ingredient seperation, ingredient tracking is identified through
 sensors. It will consist of wireless sensors suchas ultra-sonic
 sensors, reflective optical sensors, valves and servo motors.
 NodeMCU is used as the microcontroller of the SmartPour and
 ESP8266 WiFi modules are used for wireless sensors.
 Processing Unit
 The Processing Unit handles all the data processing and remote
 communications. The storage containers have the ultra-sonic
 sensor to track the ingredients availability. The valve is used
 to seperate liquid and servo motors are used to seperate dry
 ingredients from the storage containers. A reflective optical
 Sensor is there to detect the availability of a cup before
 pouring the coffee. The machine consists of storage containers
 to store ingredients. A heater and motor are used to boil water
 and stir the ingredients respectively. The power supply is given
 via current electricity.
 The processing unit also consists of inbuilt Wi-Fi via an
 ESP8266 Development board. Through this the consumer can
 directly connect our device to the internet through their home
 router and send data to our server
 Circuit Diagram
 E-R Diagram
 UI / UX
 Previous
 Next
 App Demo
 The mobile aplication is implemented using Flutter technology and Dart language.
 All the features and functionalities have been completed. Some unit testings and validation
 tests have been done for the application using Flutter test cases. To enhance the
 user experience, a simple design with a user friendly interface has been used.
 Back End
 For the backend, springboot and mysql technologies are used.
 The whole backend is deployed on Amazon Web Services(AWS) cloud as EC2 instances.
 Backup services, S3 services and security services given by AWS are
 used for this process. This demo shows the relational database implemented for
 the project accessed through EC2.
 3D Model
 This is the expected prototype view. Dimensions can be slightly varied
 due to the components of the real hardware.
 Dataflow
 Previous
 Next
 Testing and Security
 Previous
 Next
 Budget
 Item Name
 Quality
 Unit Price(LKR)
 Total Cost(LKR)
 Nodemcu ESP8266 12E
 1
 985
 985
 Heater
 1
 400
 400
 DC Motor
 1
 95
 95
 Relay
 3
 60
 180
 Valves
 2
 690
 1380
 Servo Motor
 2
 350
 700
 TCRT5000L Reflective Optical Sensor
 1
 175
 175
 Ultra-sonic Sensor
 3
 165
 495
 Containers
 4
 250
 1000
 12V 1A Full Wave Transformer
 1
 550
 550
 Others
 1000
 Total
 6960
 Timeline
 Week 1
 Idea Selection
 Week 3
 Software and Component Selection
 Week 4
 UI & Circuit Design
 Week 5
 Front End Development
 Week 7
 Back End Development
 Week 8
 Cloud Development
 Week 9
 Hardware Development
 Week 10
 Integrate Software and Hardware​
 Week 11
 System Testing & Debugging​
 Week 12
 Performance Evaluation
 Team
 Shazna Isthikar
 E/17/122
 Odasara Karunachandra
 E/17/153
 Mishel Rossmaree
 E/17/294
 About Us
 An automated coffee machine that is controlled by an application
 Copyright ©
 2022
 All rights reserved | Smart Pour
 Newsletter
 Stay update with our latest
 Follow Us
 Let us be social
 Home
 About
 Features
 Architecture
 Design
 Processing Unit
 Circuit Diagram
 E-R Diagram
 UI/UX
 Demos
 Mobile App Demo
 Backend Demo
 3D Model
 Data Flow
 Testing and Security
 Others
 Budget
 Timeline
 Team","'Smart Pour' is an automated coffee machine that can be controlled through a mobile application. This will contain features like coffee scheduling, ingredient tracking and coffee customizing etc.",E17,Cyber-Physical Systems Projects,https://projects.ce.pdn.ac.lk/3yp/e17/Smart-Pour/,https://github.com/cepdnaclk/e17-3yp-Smart-Pour,https://cepdnaclk.github.io/e17-3yp-Smart-Pour,https://cepdnaclk.github.io/e17-3yp-Smart-Pour/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/3yp/E17/Smart-Pour/
50,Wild Life Tracker,"Wild Life Tracker
 Wild Life Tracker
 Home
 About
 Design
 Budget
 Timeline
 Contact
 Menu
 Video by NickyPe from Pixabay
 3rd year project
 Wild Life Tracker
 Project Repositry
 Introduction
 Problem Overview
 The traditional method of wildlife researching in Sri Lanka, and many other developing as well as developed countries in the world is to stay in wild areas for a long time, observe animals randomly, and identify areas that animals are most active. This method is ineffective because some animals hide when they sense humans. They don't behave naturally around humans. Thus, it consumes a lot of time to observe their natural behavior. Researchers have to stay in the wild throughout this long period. So, this is a formidable process. Sometimes their life is in danger because there is a possibility to face an attack by wild animals while they are in the jungle. An immense amount of time, money, and human resources are spent in vain for traditional tracking of wild animals for researchers.
 Solution
 Our solution to this problem is a,
 Remotely controlable
 Real time
 Efficent and effective
 tracking system.
 This system consists of three major components, A hardware unit, a cloud server, and a web App. The hardware unit consists of camera traps, PIR sensors, and a location tracker. One unit of these three components makes a station. Researchers need to set up this station in the researching area. One researcher can have more than one station established in different places according to their choice. The sensors in a station can detect animals when they are in the sensing range and it will trigger the camera. A real-time photo is captured at that moment and they are stored in the station itself and also sent to the cloud server. Then the researcher can analyze these observed data through the web app.
 Remotely controlable
 Researcher can monitor the station through the web app after it is successfully established in researching area.
 High Performance Camera Module
 A powerful camera unit to capture perfect photos in any lighting condition.
 Also provides video recording facilities, recorded videos are stored in the own memory of station.
 Tracking System
 Pinpoint the exact location of the station.
 The most active station can be observed by the Web App.
 Solar Power
 The stations are powerd by battaries.
 A solar cell system is used to recharge them.
 Durable
 Can be set up in any environment. Made with highly durable and waterproof materials so that animals
 and unforgiving weather can't damage it.
 Self Storage In Stations
 Data obtained by the system is stored in itself when there is no connection with the database.
 NEXT GENERATION OF WILDLIFE RESEARCHING!
 **********************
 Project Design
 Web Application
 Organization of the Web App
 Home
 Registration
 Login
 Password recovery
 User Dashboard
 Admin Dashboard
 Previous
 Next
 Here are the two registration pages in our web application.
 01. The user registration Page
 02. The admin registration Page
 Form validations and feedbacks are used to give a better user experience.
 Clients have to fill all the fields correctly in the forms to enable the submit button.
 The button provided in left top corner directs user to the home page.
 Previous
 Next
 Here are the two login pages in our web application.
 01. The user login Page
 02. The admin admin Page
 02. The error message if the credentials are not valid.
 Form validations and feedbacks are used to give a better user experience.
 The user have to provide the correct credential to login to the system.
 If the credentials are not matching it will display the error message. With error message, links back to the login page or
 password recovery are provided.
 Also in the login pages we have provided a link to password recovery. Users can reset their password using this link.
 Previous
 Next
 These are the pages in user dashboard.
 There is a home tab, photos tab, location tab and a contact tab.
 In photos tab user can see the photos taken by devices connected to his profile. Also there is a option to connect new devices
 with the profile.
 The map provided in location tab pinpoints all the location of devices connected to users profile.
 Previous
 Next
 Here are the pages in admins dashboard. The user tab will show all the user requests to the system.
 Admin can see details and the letter uploaded by each user by clicking on a request in the list.
 Then they can submit or reject the request. If they are rejecting the request, they have to provide the reason for the rejection
 in given text box.
 This is the home page of our web application. Clients can login to there accounts by click on the log in button.
 The get started button directs clients to user registration form.
 The button provided in the right bottom corner provides a link to contact us via emails.
 Previous
 Next
 There are two pages for password recovery process. The first one is for reqest a password recovery.
 Here, user have to enter the email of his account and submit.
 If the entered email is valid, it will show up the success message. Else, it will showup the error message.
 Database Schema
 We use MongoDB as our database managment system. Because of its scalability,
 its powerful querying capabilities and the fexibility it provides for modele and
 manipulate data structures.
 Cloud Deployment
 We have used AWS EC2 instance to deploy our web application. NGINX is used as the load balancer And
 revers proxy of our frontend because of its capability of handling high volume of connections. This helps our
 web application to handle number of clients at once. Since we have used NodeJS to implement the backend of our
 web aplication we have used PM2 process manager to keep our nodeJS application alive at all the times.
 Because of the limitations of AWS Educate accounts we were not able to use R3 services to get a domain name.
 Therefore we had to use seld signed certificate to serve HTTPS.
 We have used MongoDb Atlas as our database managment system. Therefore we have connected the database to server
 running in EC2 instance.
 Devices are sending data to AWS IoT core using MQTT. These data are stored in a S# bucket. Then the server running in
 EC2 instance is accessing these data. We were not able to implement this part yet because the AWS educate account dont have permissions
 to use IAM services. Therefore, at this point we have created the file structure in the server itself.
 Hardware
 Organization of the Hardware System
 Algorithm that runs in the camera unit
 Hardware Implementation
 Camera And Sensor unit
 Connection And GPS unit
 Power unit
 Data processing unit
 This unit handles the sensing and taking snapshots of wild animals
 accordingly.
 It is consist of ESP32 Cam Board, PIR motion sensor(HC-SR501), Flash
 LED and a LDR.
 PIR sesor is capable of sensing motions within 7 meters and 120° range.
 This sensor will trigger the camera
 to take photo and then a 30 second video is captured.
 ESP32 cam board is consist of a powerfull image processing
 unit that allows to take very clear photos
 and a SD card slot. The PIR sensor is directly connected to the ESP32
 cam board. The captured photos are
 stored in the SD card before send to the cloud database. Videos are not
 sent to cloud database they will be stored
 in the SD card.
 The LDR is used to monitor light conditions and according to
 those readings the LED flash will automatically turn on.
 This allow us to capture clear photos in bad light conditions.
 The connection and GPS unit is consist of SIM800L module and
 GPS module.
 The GPS module is used to get the best possible position information.
 User can check the exact
 position of the device through the web app. The data observed by this
 GPS module is used to
 pinpoint the location.
 SIM800L module is used to communicate with the AWS server.
 This module allows for GPRS
 transmissions without Wi-Fi. This module is very suitable for long range
 connections.
 The main components in this unit are 3.7V 1800mAh batteris,
 Solar panel, voltage regulator and battery charger.
 Solar cell recharges the batteries when there is optimal sunlight.
 This generated power can be used to power the system very efficiently in
 harsh environments.
 ESP32-SIM800L module is the main component in this unit. It
 interconnects all the units
 and gathering data from other units to send to the AWS IoT core. This
 module consist of inbuilt
 SIM800L module which capable of sending and receiving GPRS data without
 WiFi.
 All MQTT certificate and GPS data are stored in inbuilt 4MB
 flash of ESP32-SIM800L module.
 Stored GPS data are used to check whether there is a defferance between
 new readings.
 If there is a difference GPS data is sent to the cloud.
 Security Features
 Our system handles really sensitive information about wildlife. Therefore, the CIA triad (Confidentiality, Availability, Integrity) of the application is maintained in the best way possible.
 JsonWebTokens (JWT) are used for the authorization of users. These tokens are stateless, portable, high performing and decentralized which improves the security and scalability of the web application as well.
 HTTPS protocol is used for secure communication.
 A special system of user registration is introduced to ensure the users of our web app are researchers.
 Testing Plan
 Unit Testing
 Test functionality of each page of the web app
 Test login, registration and form validations
 Test sensors and hardware components
 Integration Testing
 Test front end and back end intergration.
 Test database and back end intergration.
 Test web app and hardware device intergration.
 End to End Testing
 Test overall functionality of the web app and hardware device.
 Check user registraton, registration aproval.
 Check data flow from devices to user profiles.
 Check data flow from devices to cloud.
 Testing Tools
 Front End Testing
 We use selenium automated testing framework to test the front end of our web app.
 Main reasons to pick selenium is it supports different browsers and platforms. Also,
 it supports testing scripts written in multiple languages.
 We are planing to use selenium to test, form submissions, form validations
 ,response handling functionalities in our web app.
 Back End Testing
 We use insomnia API testing tool to test the back end of the web application.
 insomnia allows us to send HTTP requests using a graphical user interface.
 insomnia acts as the client and it is possible to analyze server responses
 to HTTP requests.
 We are using insomnia to test, routings implemented in backend, authentication and authorization,
 error handlings and sending responses in the API.
 Testing Procedure and Results
 Unit Testing
 Front end testing
 As an unit test we tested the functionality of the front end of our web application.
 This is very important because the front end is used by our clients. So, it must provide all the expected outcomes correctly to give a
 better user experiance. Therefore, we needed to make sure that the front end of our web application works fine.
 We have used the automate testing tool selenium to test front end. The first test we have done is the navigation test. We wrote a
 script to click the all possible links inour web application and the we have checked whether it navigate to expected page. Form validations are
 another important aspect in UI. So, we wanted to test whether our app provides correcet feed backs when the client fill a form. For this also we
 wrote a selenium script. In this script, we tested all of our forms with invalid data to make sure they are giving correct feed back. Also, we tested
 the froms with valid data to make sure that the forms are not giving any invalid feedback wheth client inputs valid data.
 The scripts are attached here.
 The testing results are listed below.
 Back end testing
 Prior to the intergration of frontend and backend, we wanted to make sure that our rest API returns correct responses to
 clients requestes. We use 'Insomnia' to test the API. We have implemented lot of routes in our API to do various tasks and to handle various
 requests from the frontend of our web application. It is very important to make sure that the API is providing correct responses. Because the
 security of the web application is depends on these responses. Using insomnia we have tested each route in our API for valid and invalid data.
 We have tested whether the API responds with correct status codes, data and error messages for different data sets. Also as an security test,
 we have passed wrong credentials to the API using insomnia and checked whether it rejects those requests. We have used JWT tokens to authorize clients.
 It is very important to make sure tha API is rejecting invalid tokens. Therefore we have tested whether the API is rejecting invalid tokens by sending
 invalid token using insomnia. All of these tests are conducted manually. The below figure contains all the cases we have tested using insomnia.
 Integration Testing
 To test the integration of the backend, frontend and the database, we used selenium. We have tested the response from the server to
 logins, user registrations, password recoveries and invalid inputs to make sure that the data flow from front end to back end and from back end to
 data base is fine. We have wrote scripts to enter valid and invalid credentials in login pages and check the response of the server.
 If these compnent intergrations are fine, the logins for valid credentials must be successfull. We have tested whether the
 request with correct credentials directs the client to the dashboards in the scripts. We had to check the database manually to make sure all the
 data entered by the script is recorded in database correctly.
 Also, we have tested the same script for three popular browsers. Google Chrome, Microsoft edge and Operamini to make sure our web
 application is ccompatible with different platfoorms.
 The testing scripts are attached here.
 Testing Results
 Results of tests by Selenium
 Test run No 1: Chrome Browser
 Passed: 6
 Faild: 1
 Test run No 2: Chrome Browser
 Passed: 7
 Faild: 0
 Test run No 1: Microsoft Edge Browser
 Passed: 7
 Faild: 0
 Test run No 2: Microsoft Edge Browser
 Passed: 6
 Faild: 1
 Test run No 1: Operamini Browser
 Passed: 6
 Faild: 1
 Test run No 2: Operamini Browser
 Passed: 7
 Faild: 0
 Test Case
 Chrome
 Edge
 Opera
 Navigations to all the pages
 passed
 passed
 passed:1 Faild:1
 Form validations for incorrect inputs
 passed:1 Faild:1
 passed:1 Faild:1
 passed
 Form validations for correct inputs
 passed
 passed
 passed
 User login and dashboard function
 passed
 passed
 passed
 User registration test for valid and invalid data
 passed
 passed
 passed
 Admin login and dashboard function test
 passed
 passed
 passed
 Password recovery test for valid and invalid emails.
 passed
 passed
 passed
 Results of tests by Insomnia
 Test Case
 Returned Status Code
 Expected Status Code
 Observation
 Faild / Passed
 User and Admin login for correct credentials
 200
 200
 Backend responds with status code 200 and send a token to client
 passed
 User and Admin login for incorrect credentials
 401
 401
 Responds with status code 401 and rejects the request. Just recived credentials invalid message.
 passed
 User and Admin Registration for emails that are already registered in system
 409
 409
 Rejects registration for same emails.
 passed
 Password recovery for emails that are not registered in system
 401
 401
 Rejects password recovery for invalid emails.
 passed
 Password recovery for emails that are registered in system
 201
 201
 Recived message indicating that the password recovery email is sent to entered email.
 passed
 Try to reset password for second time using a link used before.
 401
 401
 Recived message indicating that the password recovery link is invalid or expiered.
 passed
 Try to reset password of a accont using a token sent to another email
 401
 401
 Recived message indicating that the password recovery link is invalid or expiered.
 passed
 Try to register as an admin using a link that already used to register.
 401
 401
 Recived message indicating that the link is invalid or expiered.
 passed
 Try to log in to the admin dashboard using a token recived for user login
 401
 401
 Recived message indicating that the client is not an admin. Redirection rejected.
 passed
 Confirm or Reject request with admin token.
 200
 200
 With admin token. Possible to confirm or reject client request.
 passed
 Confirm or Reject client request with user token.
 401
 401
 With user tokens. Cannot confirm or reject requests. Recieved message unauthorized.
 passed
 Project Budget
 (Budget is made for 1 Hardware unit only)
 Hardware Component
 Amount
 Price(Rs)
 Place
 Infrared PIR motion Sensor
 1
 235.00
 tronics.lk
 ESP32 Cam Board
 1
 1890.00
 tronics.lk
 FTDI connector
 1
 470.00
 tronics.lk
 ESP32-SIM800L module
 1
 3250.00
 tronics.lk
 Ublox NEO_6M GPS module
 1
 985.00
 tronics.lk
 1W LED
 1
 25.00
 tronics.lk
 LDR
 1
 10.00
 tronics.lk
 Solar Panel (>6V) 165mA
 1
 560.00
 duino.lk
 Battery Charger
 1
 90.00
 tronics.lk
 3.7V 4800mAh batteris
 2
 700.00
 nilambaraelectronics.com
 Other Expenditures
 -
 1500.00
 TOTAL
 9715.00
 Project Timeline
 Supervisors
 Dr. Isuru Nawinne
 Dr. Mahanama Wikramasinghe
 Group Members
 Group 9
 E/17/176
 Kumara W.M.E.S.K
 E/17/006
 Alahakoon A.M.H.H
 E/17/338
 SRIMAL R.M.L.C
 Project Repositry
 Project Page
 Department Of Computer Engineering
 University of Peradeniya
 A Project by, group of 3rd year computer engineering undergraduates, University of Peradeniya, Sri Lanka",Wild life tracking system specially designed for researchers to study behaviors of wild animals remotely. ,E17,Cyber-Physical Systems Projects,https://projects.ce.pdn.ac.lk/3yp/e17/Wild-Life-Tracker/,https://github.com/cepdnaclk/e17-3yp-Wild-Life-Tracker,https://cepdnaclk.github.io/e17-3yp-Wild-Life-Tracker,https://cepdnaclk.github.io/e17-3yp-Wild-Life-Tracker/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/3yp/E17/Wild-Life-Tracker/
51,maker mate,"Maker Mate
 MakerMate
 Home
 About
 Services
 Team
 Makes your makersfeel comfortable
 See how it works
 Project Repository
 What
 Maker Mate brings to you?
 Maker Mate enables the lab users to apply for the project/ experiment requirements online and collect tools and equipment accordingly from a pickup locker while the lab admin can manage the entire inventory online with ease. Maker Mate can take similar role but for different use case on demand.
 Learn more
 Services
 Client Web App
 The web app provides services according to privileges of users. Admin can have full access to the inventory system and users signed in can apply for components to borrow from Maker Space. Guest users can access the lab equipment and component documentation.
 Information Console
 A touch screen that provides simple and easy to use user interface to authenticate and unlock the locker. Makers can easily search for tools and components and get to know how to use them and other required details.
 Monitored Pick-up Locker
 A locker which is monitored by a camera when it is opened by a user with authorization. The lock is controlled by a Raspberry Pi, which provides commands to unlock according to authentication.
 Information Console Interface
 Tech Stack
 We use necessary software and hardware tools to develop and test the product
 Wanna know more about Maker Mate?
 Find more descriptive details about Maker Mate
 Learn More
 Our Team
 Thilini
 Member
 Thanujan
 Member
 Madhushan
 Member
 Hashan
 Mentor
 Jaliyagoda
 Mentor
 Dr. Isuru
 Supervisor
 Dr. Mahanama
 Supervisor
 © Copyright Reveal. All Rights Reserved
 Designed by BootstrapMade","3rd year Embedded systems & Software project by @Thilini98, @thanujan96 and @DrMadhushan",E17,Cyber-Physical Systems Projects,https://projects.ce.pdn.ac.lk/3yp/e17/maker-mate/,https://github.com/cepdnaclk/e17-3yp-maker-mate,https://cepdnaclk.github.io/e17-3yp-maker-mate,https://cepdnaclk.github.io/e17-3yp-maker-mate/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/3yp/E17/maker-mate/
52,remote billiard,"Project Page - Remote Billiard Game | Home
 Home
 About
 Solution
 Design
 Solution
 Infrastructure
 Software
 Budget
 Team
 Github Repository
 Remote Billiard
 We have combined
 the live game play of billiard with a online platform. You can experience the Excitement
 of the live billiard game with a opponenent who is not in the same room!
 INTRODUCTION
 ""Remote Billiard"" is a project which provides usual Billiard game experience via online platform.Not all the players could be in the same place to play a billiard game now a days. Project ""Remote Billiard"" solves this problem. Players can play their game physically at their own places individually.
 PROBLEM
 Usually if a pool game is supposed to
 be played, the players should physically be present. But the issue is not everyone has the luxury
 of time to attend a game due to various reasons. In this case, the least thing that they could do is
 play the game online via a mobile or a PC with their friends. But if you ask such players
 whether the experience was satisfying, they would definitely say no! So to avoid this issue we
 have planned to give the player the so called Physical experience at their own comfort zones.
 According to our plan we will be solving a lot of problems such as the issue with time,
 travelling issue, could reduce expenses and will be able to provide the player the real physical
 experience which would be a great chance to enhance their skills at the same time would be
 more fun than online games. Therefore in overall , Remote-Billiard project solves these
 problems.
 OUR SOLUTION
 Simple Solution
 Solution Techniques
 Solution Architecture
 Simple Solution demonstrated between two players.
 Lets cosider two players who are at two different locations as PLAYER A and PLAYER B
 Lets assume PLAYER A makes the first shot.
 When the first shot is made, the ball arrangement of the table of player A is captured by the device.
 This captured image is then processed and sent to the device of player B.
 This processed image is then projected onto the table of PLAYER B.
 Player B then has to arrange the balls based on the projected image and will have to play his shot.
 Techniques used for our solution.
 BALL DETECTION
 First the image of the Player A is captured.
 Next Using QR tags which are already pasted on the 04 corners of the pool table, the captured image is aligned.
 IMAGE PROCESSING
 Then this aligned image is processed.
 This processed image is sent to the device of player B.
 BALL PROJECTION
 If projection happens for the first time, the projecting area of table of plyaer B is calculated. For this,
 first a set of QR tags are projected on to the table, the cordinates of these QR tags are matched with the existing QR tags which were already pasted on the table.
 Once the projecting area is calculated the processed image is processed on to the table of player B.
 Solution Architecture of how two or more players connect.
 The camera module and the projector of one player is connected to the microcontroller.
 The microcontroller of one player is connected to another players microcontroller via the internet which goes accross a web server.
 Mobile applications of each players are connected with each other via the the internet which goes accross the web server.
 DESIGN
 3D MODEL OF OUR PRODUCT
 Solution Overview
 CLOUD DEPLOYMENT
 APPLICATION DESIGN
 MOBILE APP DEMONSTRATION
 SIGNUP and SIGN IN functionalities for multiple clients.
 REGISTERED PLAYERS
 MULTIPLAYER FUNCTIONALITY
 ( Shows only the players who are online at the moment , and if the player is available for a game then a game invitation can be sent.)
 TOSS FUNCTIONALITY
 (As soon as the players connect , they have the ability to toss in and decide who is going to play first. The toss is generated randomly.)
 CALL FOR FOUL FUNCTIONALITY
 (while the game is on going , if a player notices a foul made by his opponnent through the live stream then he can call for a foul. And if it was a foul then the oponnent can either accept it or decline the call.)
 CHOOSE POCKET FUNCTIONALITY
 (Players can choose a pocket before they make their shot. The chosen pocket number will be displayed to the opponent.)
 REAL TIME CHAT FUNCTIONALITY
 (While the game is ongoing the players can chat real time).)
 Flow Chart
 Circuit Diagram
 Data Control Flow
 Flow Chart
 ER Diagram
 Circuit Diagram
 Circuit Diagrom
 SOLUTION INFRASTRUCTURE
 Control Unit
 Projector
 Supporter
 QR Tag
 Control Unit
 Following Components are included
 Raspberry pi 3 model B Microcontroller
 Camera module
 SD card(16 GB)
 Distance sensor
 Cooling fan
 Indicators
 Projector used to project the ball positions onto the table
 Supporter
 The projector and the control unit can be mount with the help of the supporter.supporter is an adjustable one. So that we can adjust the height between the table and the device
 which is really important to focus the device perfectly on to the pool table.
 QR tags are used to detect the perfect alignment of the picture.
 T
 MOBILE APP TESTING
 UNIT TESTING
 Login Validation
 CLIENT AND SERVER CONNECTION TESTING
 INTEGRATION TESTING
 Multiple client connection
 SignUp and Login testing
 Currently online players list
 Displays all registered users
 Sends game invitation to another player who is online
 Random Toss generation
 Foul call
 Pocket selection
 Real-time messaging between players
 EMBEDDED SYSTEM TESTING
 Connecting the Remote-Billiard device (pi module) to the mobile app via bluetooth
 Based on the ball movements the device sends SIGNALS to the opponents mobile application.
 IOT device to IOT device communication TESTING
 Raspberry pi module is taken as one IOT device and the PC is taken as the other IOT device
 Both the devices are subscribed to the same topic.
 Using OpenCV implementations ball movements is detected and once the balls stop movements, Image is captured and the raw image is processed inside pi.
 This processed image is then published to the relevant topic. (Test : Raspberry pi to PC)
 OpenCV Implementations
 1. Supporter adjusment
 With the use of hight adjusting supporter and the aruco markers on the board we can calculate real world height from supporter plate to
 The board.When the height reached required value, a in openCV code a signal will generate. With that flag signal a indicator will light on the microcontroller. This is helpful for the accuracy of capturing the playing area
 and to do the final projection.
 For this height detection function, camera calibration matrix and distortion values are needed as inputs.
 .
 2. Projection area adjusment
 Four aruco tags will be attached near corners of poolboard.
 And their coordinates are taken with their ID. Link to the implementation
 Then another set of aruco tags are projected on to the table, the cordinates of these aruco tags are matched with the existing aruco tags which were already pasted on the table.
 If the projection alignment is right a signal will sent to the microcontroller to light up a LED.
 3.Board area isolation and wrapping
 To get the board area from the captured image. Corners were identified and using corner coordinates image was wraped.
 Link to the implementation
 4.Ball colour detection isolation and generate processed image
 OpenCV trackbars are used to get the accurate colour levels of balls.
 Link to the implementation
 Using those hsv valuse ball location were identified and locations are drawn on a white backbround. Final processed image is ready to sent.
 Link to the implementation
 5.Ball Movement Identification
 To identify whether the balls still moving or stopped moving first the video was captured from OpenCV
 then grabed the video frame by frame. If the previous frame is exact with the current frame this means the video vision is still. Ball moving or not moving signal is genereated using this concept.
 Link to the implementation
 BILL OF MATERIAL
 OUR SUPERVISERS
 Dr. Isuru Nawinne
 Dr. Mahanama Wickramasinghe
 OUR TEAM
 M.I.Rishard
 S.P.D.D.S.Weerasinghe
 A.M.F.Shalha",Remote-Billiard is a project which provides usual Billiard game experience via online platform.Not all the players could be in the same place to play a billiard game now a days. Project Remote-Billiard solves this problem. Players can play their game physically at their own places individually.,E17,Cyber-Physical Systems Projects,https://projects.ce.pdn.ac.lk/3yp/e17/remote-billiard/,https://github.com/cepdnaclk/e17-3yp-remote-billiard,https://cepdnaclk.github.io/e17-3yp-remote-billiard,https://cepdnaclk.github.io/e17-3yp-remote-billiard/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/3yp/E17/remote-billiard/
53,remote keyboard tutoring system,"Remote Keyboard Tutor
 forté
 Menu
 Home
 About
 Architecture
 Design
 Devices
 UI / UX
 Data Flow
 Generation of Data
 Flow of Data
 Software
 Frontend
 Backend
 Other
 Hardware Functionalities
 Security Aspects
 Features
 Research Findings
 Timeline
 Budget
 Contact
 Learn or Teach PianoIn
 The Remote Keyboard Tutoring System is a web
 based system that can be attached to any (electronic) keyboard synthesizer through a MIDI
 connector. Once our system is connected to the keyboard, the user can interactively learn,
 play or teach in combination with the web application that we provide.
 Project Repository
 ×
 Key-press Indication
 Remote key-presses as well as local key-presses (when working offline) are displayed in
 real-time with low latency.
 Interactive Exercises
 Upload exercises for the students as sheet music and let the system handle the rest. The
 system keeps records of student's mistake while practicing.
 Schedule Classes Easily
 Teachers can schedule the classes easily with the web application. Students can also
 easily find teachers near by.
 Offline Helper
 Upload the sheet music that you want to practice and the helper will assist you while
 playing.
 Why do you need a keyboard tutoring system?
 The Problem
 Piano is the most
 admired musical instrument so far. Over 25% of the world population fancy playing it. Yet not
 everyone is fortunate to learn from the best. This has fabricated a vast difference in the skill
 level of people who are playing the piano. The reason would be that most of the experts in the field
 lives in either the western hemisphere or east Asia. Even local experts aren't scattered throughout
 the island. So most of them tend to make use of the online procedures to teach piano. The current
 online methods would be using a platform like zoom and an app that simulates what one plays from his
 instrument. This hasn't shown profound results since the teacher or the student has problems when
 showing what he's playing on the piano in real time. The accuracy of the app isn't acceptable and
 the relationship between the student and the teacher isn't sturdy like in a live class room.
 Our Solution
 The solution we present
 is a web based system that can be attached to
 any keyboard synthesizer through a MIDI connector (Legacy or USB). Thus, the only requirement for
 the keyboard is MIDI support. Once our system is connected to the keyboard, the user can
 interactively learn, play or teach in combination with the web application that we provide. The user
 has to be logged in as either a typical user or an expert (a tutor) depending on the user's needs.
 Users can
 get connected to each other in the network and they can share their music with each other. Users can
 play shared music in their own keyboard and join to experts to learn remotely in real-time. The
 tutor can handle multiple students at a time. When the tutor plays on the keyboard, those notes will
 be displayed and played in the students keyboards simultaneously. Tutor can upload exercises and
 when a student plays incorrectly, the wrong key presses will be displayed to both the student and
 the tutor. Their is also an offline helper which will assist the user while practicing when
 connected to the mobile phone via Bluetooth.
 Solution Architecture
 Our system consists of two devices: the Processing Unit and the Visualizer Bar which is a
 composition of visualizer blocks. The processing unit can be connected to our server via a home router and exchange MIDI data with the
 server or with another remote processing unit if they are connected in a session. In order
 to communicate with the server there'll be a mobile application as well as a web platform.
 Through the web platform, students or teachers can login to the system and use the facilities that we provide in our system. When a student is logged in, he/she can easily find teachers near their area or based on the teacher's ratings and reviews. The students can interactively learn what the teacher plays remotely due to the low latency real-time key press indication.
 Using the offline helper we provide in the mobile application (which can be used when the device is connected to the mobile via BT), students can practice music with the assistance of the offline helper. Students can also upload sheet music and play them in their own keyboard synthesizer. When they make a mistake, it'll be displayed in the mobile app by highlighting the particular note in the sheet music
 as well as in the visualizer bar. They can also record and store their playings to the cloud and show them to the tutor at any time.
 When a tutor is logged in to the system, he/she can create their own courses or classes within the system. They can also schedule their existing classes. During a class, tutors can independently monitor his students' playings and give feedback to them. The tutor can upload exercises and when the students practice them, their progress will be recorded until the student marks them as finished. Even if the student practices
 the exercises offline, the progress will be stored temporarily in the mobile phone and will be uploaded to the tutor when the student logs in again.
 Previous
 Processing Unit
 The Processing Unit handles all the data processing and remote communications. It
 has two MIDI interfaces (one for MIDI-in and the other for MIDI-out) as well as
 two USB-C ports. One USB port act as a MIDI redirect interface which also allows
 the processing unit to be recognized as a USB device when connected to a
 computer. The other USB port is to connect the visualizer bar in.
 The processing unit also consists of inbuilt Wi-Fi as well as Bluetooth via an
 ESP 32 Development board. Through this the consumer can directly connect our
 device to the internet through their home router and send MIDI data to our
 server or another processing unit in a remote location.
 Visualizer Bar
 The visualizer bar is where all the key press information will be displayed at.
 The bar has to be placed on the keyboard near
 the end where the key hinges are. We provide the visualizer bar as
 easy-to-connect blocks in order for our product to fit any
 kind of keyboard and to make our product portable. The three common keyboard
 sizes are 61-keys, 76-keys and 88-keys. Therefore,
 the blocks are such that it covers an octave. Since blocks might not cover the
 whole keyboard, we provide the visualizer bar as a
 whole for the most common keyboard sizes as well.
 The visualizer bar has a USB-C port at the right end to connect in to the
 processing unit. The visualizer blocks have magnetic
 ends to stay still when they're connected. Left end of a block has a female
 USB-C port and the right end has a male USB-C connection. The bar has RGB LEDs
 representing each key and they will lit according to the key presses.
 Next
 UI / UX Design
 Previous
 Web UI
 Once the user creates an account on this apllication, the processing unit can be connected to it. Then the user can,
 Select and enroll in courses.
 Upload score sheets to the account and play them on the keyboard.
 Create score sheets using MIDI files.
 Record what he/she plays on the keyboard.
 Convert recordings into score sheets.
 Remotely play the student's/teacher's keyboard synthesizer.
 More features will be included along with the implementation of the product.
 Mobile UI
 Once the user creates an account on this apllication, the processing unit can be connected to it. Then the user can,
 Select and enroll in courses.
 Upload score sheets to the account and play them on the keyboard.
 Create score sheets using MIDI files.
 Record what he/she plays on the keyboard.
 Convert recordings into score sheets.
 Remotely play the student's/teacher's keyboard synthesizer.
 More features will be included along with the implementation of the product.
 Next
 Generation of Data
 MIDI - Musical Instrument Digital Interface
 MIDI is standard protocol for communication between computers and musical instruments.
 Made up of an 8-bit command byte,
 generally followed by 1 or 2 data bytes.
 MIDI Commands
 Note on - This MIDI command will be sent when a note is played.
 Note off - This MIDI command will be sent when a note is released.
 After touch - This defines the preassure applied to the key.
 You can read more details about MIDI from here.
 Flow of Data
 When some key strokes are played in a keyboard, the keyboard itself will convert those notes to MIDI data. Then those MIDI data are send to our processing unit and from there the MIDI data can be send to our remote server in order to store or if that processing unit is in a session with a another processing unit those data can be directly send to that processing unit via the internet. Even though the processing unit is not connected to the internet it will generate necessary control signals in order to visualize those notes in the local visualizer bar.
 So if the processing unit is in a session with a remote processing unit the two processing units can exchange MIDI data via the internet. The processing units can send received MIDI data to the keyboard and the control signals to the visualizer bar. Thus, the recipient can here as well as see what the remote sender is playing.
 Frontend
 Frontend Technologies
 For our frontend web platform would be implemented using React.js. React will assure a fast and easy development of the frontend since React provides lots of readymade components to use and React uses javascript it will be easier to coordinate with the backend as well. Other than that React apps can be made SEO friendly by making them server-side rendered rather than client-side rendered.
 Our frontend mobile platform will be implemented using Flutter since it also provides a lot of components to use and Flutter performs well in a lot of mobile devices. Since Flutter uses Dart as the programming language we can use the same codebase for both ios and android applications.
 What we have done so far?
 Click here to see what we have done upto now in the web frontend and click here to see what we have done so far in the mobile frontend.
 Backend
 Backend Technologies
 Our backend computing platform would be Node.js along with express as the framework. It'll assist us in hadling multiple online requests and the working environment will be more effective and better-coordinated since javascript laguage is used in both frontend and backend development. MongoDB will be used as our database system. As the number of users for our product increase, it is very much practical to use MongoDB since it is very easy to scale up or down. MongoDB works on all types of cloud platforms. Since we'll be using aws to host our services MongoDB will be a great choice. NGINX is used as a reverse proxy so we can get access through http, https and Let's Encrypt is used to get free SSL certificates. All our backend services will be hosted using AWS services. An AWS EC2 instance is used run all the processes and AWS backup will be used to automate and manage backups.
 click here to see what we have done so far.
 Hardware Functionalities
 The brain of our processing unit is the ESP32 Development board. As shown in the high-level diagram, it's needed to have a MIDI interface in between the visualizer bar and the keyboard synthesizer for communicating with MIDI messages. The processing unit will be connected to Wifi as well as to the mobile phone via bluetooth. We use this BT connection for the offline helper as well as to authenticate to a Wifi router.
 When MIDI messages arrive at the processing unit either from the user's keyboard or from remotely through the server, the microcontroller will process them and send control signals to the visualizer bar appropriately. The communication between the visualizer bar and the processing unit will happen via an USB interface through a wired connection. The visualizer bar will also need to send signals to the processing unit because in order to light up the correct key on the keyboard, the processing unit needs to know the positional information of the particular visualizer block. So, before the user starts using the system he needs to do a very simple calibration procedure: play both the first note and the last note one after the other that the visualizer bar covers when the app prompts and thereafter the processing unit will memorize which block is where. In order to achieve this, the visualizer bar will also require a microcontroller. We will be using an arduino pro micro for this purpose.
 Security Aspects
 Security Aspects
 Cybersecurity of connected embedded system devices has always been important. Since our system also involves sharing confidential information through the internet such as passwords and online payments, we have taken extra care to ensure the security of the system. The diagram to the right shows how we have designed our system to ensure the CIA triad.
 To ensure the confidentiality of the system, all the data stored in the server will be encrypted with AES or hashed. When connecting the mobile application to the device for the first time or after a long time of inactivity, the device will ask for a password pin. The pin will be sent to the server and if it is correct the device will be allowed to connect to the mobile phone. The diagram itself explains the countermeasures we have taken to ensure the other two cybersecurity aspects.
 Our Secure Design Process
 The following diagram shows our secure embedded system design process. The gray color path is the ideal embedded system design process that anyone would follow.
 In addition to that, we have added a security approach that'll happen in parallel with the embedded system design process. Starting from our concept of operations, we will do a threat analysis while collecting the functional requirements to analyze the potential attacks that the system may be subjected to when deployed. The next step is to specify how our demand measures the security requirements of the system. Once we have the security requirements, we will start developing the security approach after some rounds of evaluations.
 Features
 Previous
 Real-time Keypress Indication
 The visualizer bar will display the keys pressed at one end, in the other end with minimum latency. The key presses will be indicated using a light beam directed to each key. What's being played is heard through the speakers of the keyboard synthesizer
 Schedule Classes and Assignments
 Tutors can schedule classes easily after setting up tutors’ profiles on our platform. Then after supplying the relevant information about the class such as the difficulty, number of lessons, course fee and etc., tutors can schedule classes.
 Then tutors can set up assignments for those classes, so the students can easily submit their recordings through our platform.
 Offline Helper
 The offline helper will give students the assistance need while doing their assignments and while practicing. First student has to choose a song (MIDI File) that wants to practice, then the visualizer bar will visualize the keystrokes so the student can practice. Prallaly to that the application will show the incorrect notes, as well as out-of-tempo notes that the students have played.
 In order to this offline helper to function the device (laptop or Mobile device) should be connected via Bluetooth or wifi so the devices can communicate with each other.
 Recorder
 Using the recorder students as well as tutors can record their piano pieces without bothering about the mic quality and the background sound since the recording is happening in the MIDI format directly through the MIDI output of the keyboard. Since MIDI data are light weight users don’t need to worry much about managing the cloud storage space.
 So the tutors can easily provide sample pieces to their to the students recorded and the students can easily record the pieces that needs to be submitted and submit them to their tutors.
 Next
 Research Findings
 Previous
 MIDI to Sheet Music
 Our product will often use MIDI messages to transmit data between the keyboard visualizer bar and the processing unit. But MIDI itself isn't interactive with most of the beginners. They'll be needing score sheets or sheet music to recognize what to play or, what is being played in his/her keyboard synthesizer. Therefor we'll be introducing a feature in a our web and mobile apps to convert these midi files to sheet music. We'll be using BYVoid/MidiToSheetMusic
 opensource project to acheive this.
 Usage of this project can be found here.
 Next
 Timeline
 19th July, 2021
 Project Proposal(Milestone 1)
 Presenting our project proposal was the first milestone.
 The first project proposal presentation can be seen from here.
 Finished: 19th July, 2021
 23rd July, 2021
 CAD Models
 Build the CAD models for both the processing unit and the visualizer bar.
 Finished: 01st August, 2021
 24th July, 2021
 Sheet Music Read / Write
 Trying to understand how to integrate music scores and tabs into the web application using the flat.io API.
 Intended features :
 1. Feed real-time MIDI data to the sheet music display.
 2. Output the sheet music as a MIDI data in real-time.
 3. Edit / Create sheet music in the web platform both online and offline.
 4. Upload / Download sheet music.
 This research is being paused for now.
 Finished: n/a
 06th August, 2021
 Mobile platform Front-End
 Design and implement a mobile platform front-end.
 Finished: n/a
 07th August, 2021
 Web platform Front-End
 Design and implement a web platform front-end.
 Finished: n/a
 12th August, 2021
 Testing the ESP32 Board
 Wifi and BLE testing.
 Finished: 28th August, 2021
 25th August, 2021
 Database Schema
 The database schema (ER diagrams) for the system has been designed.
 Finished: 29th August, 2021
 29th August, 2021
 Designing backend APIs
 Implementing backend APIs with authentication and authorization.
 Finished: 02nd September, 2021
 29th August, 2021
 Redesigning the high-level diagrams
 The high-level diagrams for the system are being redesigned with the protocols and technologies being used.
 Finished: 28th August, 2021
 30th August, 2021
 Building Schematics
 Designing schematics for the processing unit and the visualizer blocks.
 Finished: 31st August, 2021
 03rd September, 2021
 Progress Review 1(Milestone 2)
 Criteria to be met before the presentation:
 1. Block diagrams.
 2. Circuit diagrams.
 3. Database schemata.
 4. Algorithms / Flow Charts.
 5. UI Designs.
 6. Performance, Power, Security requirements.
 7. Failiure handling.
 8. Sensors and actuatos.
 9. Controller platforms (programming, memory, available
 interfaces, connectivity, speeds, data-rates, built-in units,
 power, security, cost, etc.).
 10. Network technologies and protocols (interfacing, medium,
 bandwidth, security, availability, reliability).
 11. Back-end technologies (programming, storage, accessing,
 backups, security, cost, 3rd party services)
 12. Front-end technologies (programming, data visualization,
 security).
 The presentation for Progress Review 1 can be seen from here.
 Finished: 03rd September, 2021
 04th September, 2021
 Mobile App / ESP32 BT Test
 Test Bluetooth communication and WiFi SSID, Password extraction inside the mobile application.
 Try to send encrypted WiFi SSID and Password to the ESP32 via Bluetooth.
 Finished:
 17th September, 2021
 15th October, 2021
 Testing
 Testing the API, front-end and security testing.
 Finished: n/a
 25th October, 2021
 Progress Review 2(Milestone 3)
 Criteria to be met before the presentation:
 1. Completete back-end and front-end software.
 2. Cloud deployment.
 3. Ability to provide a clear overview of the system.
 4. Ability to clearly explain features and functionalities (including reliability, scalability and security aspects)
 5. Ability to clearly explain implementation details
 6. Attention paid to enhance the user experience of software/hardware components and of the overall product
 7. Details of three or more tests carried out on the software components (what was tested?, why is it important?, how was the test done?, results and findings)
 8. Designs for embeded node hardware.
 Finished: n/a
 Not started yet.
 MIDI Read / Write Test
 Build and test a circuit to read and write MIDI messages using Arduino. An
 introduction to what MIDI is can be found here.
 Finished: n/a
 Not started yet.
 Implement gRPC services for ESP32
 Implementation of gRPC services in the backend for the embedded hardware.
 Finished: n/a
 Not started yet.
 Implement ESP32 gRPC client
 Implementation a gRPC client in the ESP32 and test with the gRPC supported backend.
 (gRPC direct usage w/HTTP2, and not the gRPC transcoding -> REST API)
 1. Connect ESP32 to HTTP/2 Server.
 1. Efficient IoT with Protobuff
 Finished: n/a
 Not started yet.
 Migrating from JSON to Protobuff
 Change the data serialization format from JSON to Protobuff in the backend.
 Compare the performance difference between the two versions.
 Finished: n/a
 Budget
 Our Budget
 Item Name
 #Units
 Per Unit Price (Rs.)
 Price (Rs.)
 ESP32 Development Board
 2
 1, 550
 3, 100
 Arduino Pro Micro Development Board
 3
 1, 130
 3, 390
 5mm LEDs(12 per block)
 36
 4
 144
 RGB LEDs
 2
 15
 30
 3.7V 4000mAh Li-Po Rechargeable Battery
 2
 800
 1, 600
 DIN 5 pin Connector (Female)
 4
 175
 700
 MIDI-to-MIDI Cable
 2
 400
 800
 TP4056 Charger Module
 2
 65
 130
 LTC3440 IC
 2
 130
 260
 6N139 Optocoupler
 2
 34
 68
 BC212 Transistor
 2
 5
 10
 Resistors (220Ω, 680Ω, 2.7k, 6.8k, 10k, 20k, 1M, 200k)
 8 packs of 40 pcs
 40
 320
 Capacitors (100μF, 470μF, 0.1μF, 22μF, 10μF)
 6
 2
 12
 10μH Inductor
 1
 5
 5
 Latching Push Button Switch
 2
 50
 100
 DPST Switch
 2
 30
 60
 Push Button
 2
 5
 10
 PCB Printing Cost
 _
 _
 2, 000
 3D Printing Cost
 _
 _
 2, 000
 TOTAL
 14, 739
 The above BoM is considering two processing units and three visualizer blocks, one processing unit and one visualizer block for tutor's end demonstration and the rest for the student's end demonstration.
 Our Team
 Dinura Dissanayake
 E/17/072 Developer
 Sathira Silva
 E/17/331 Developer
 Shamal Weerasooriya
 E/17/380 Developer
 Our Advisors
 Dr. Isuru
 Nawinne
 Dr. Mahanama
 Wickramasinghe
 Remote Keyboard Tutor
 The Remote Keyboard Tutoring System is a web based system that can be attached to any
 (electronic) keyboard synthesizer through a MIDI connector. Once our system is connected to
 the keyboard, the user can interactively learn, play or teach in combination with the web
 application that we provide.
 Have a Questions?
 +94 77 416 1509
 (Dinura)
 +94 77 600 7404
 (Sathira)
 +94 76 468 5395
 (Shamal)
 bitlasagna.mail@gmail.com
 Or you can put your general Comments / Questions
 here.
 University of
 Peradeniya
 Faculty of Engineering
 Department of Computer Engineering
 Copyright © Bitlasagna","The Remote Keyboard Tutoring System is a web-based system that can be attached to any (electronic) keyboard synthesizer through a MIDI connector. Once our system is connected to the keyboard, the user can interactively learn, play or teach in combination with the web application that we provide.",E17,Cyber-Physical Systems Projects,https://projects.ce.pdn.ac.lk/3yp/e17/remote-keyboard-tutoring-system/,https://github.com/cepdnaclk/e17-3yp-remote-keyboard-tutoring-system,https://cepdnaclk.github.io/e17-3yp-remote-keyboard-tutoring-system,https://cepdnaclk.github.io/e17-3yp-remote-keyboard-tutoring-system/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/3yp/E17/remote-keyboard-tutoring-system/
54,remote medical diagnostics,"Remote Medical Diagnostics System
 Projects - Department of Computer Engineering
 University of Peradeniya, Sri Lanka
 Remote Medical Diagnostics System
 Team
 E/17/134, Kavindu Jayasooriya, e17134@eng.pdn.ac.lk
 E/17/318, Udith Senanayake, e17318@eng.pdn.ac.lk
 E/17/207, Pasindu Marasinghe, e17207@eng.pdn.ac.lk
 Table of Contents
 Introduction
 Solution Architecture
 Hardware & Software Designs
 Hardware Components
 Circuit Designs
 Software Tools
 UI Designs
 3D Prototypes
 Algorithms
 Testing
 Hardware
 Software
 Detailed budget (tentative)
 Project Timeline
 Conclusion (will be available after testing)
 Supervisors
 Links
 Introduction
 People have to face a lot of challenges when they want to see a doctor, from having to waste time on the road full of traffic, waiting in long queues for hours to being in hospitals full of patients with contagious diseases. We aim to minimize these problems by introducing a platform where doctors and patients can meet online and a convenient diagnostics device to go with it; eliminating the need to go to a hospital for most of the common medical conditions and get diagnosed in the comfort of your own home.
 While there are some solutions already available in the market trying to solve some of these problems like E-channeling, audio and video conferencing they don’t provide a good way for the doctor to monitor the patient’s condition easily. We aim to design a cheap yet convenient and effective tool to make everybody’s life easier by taking patient’s measurements in real-time.
 What’s available in the system:
 Making appointments online
 Video Consultation.
 Diagnostics device that reads commonly needed measurements
 Heartbeat sensing (clear audio with little to no background noise)
 Temperature sensing ( 0.5°C tolerance)
 Glucose levels in the blood
 Blood pressure
 Ability to handle the device via software and hardware
 Real-Time measurements.
 Getting valid prescriptions from the doctor
 Verified authenticity of the doctor
 Privacy and Confidentiality
 Patient history and other analytics
 User experiences and reviews for a doctor
 Who is it for?
 As an individual patient one simply have to sign up and log on to our system to meet verified doctors with audio conferencing; for a more accurate diagnosis, our medical device can be used. Hospitals can use our system to manage doctor-patient communication remotely by getting the platform set up along with their existing systems. Individual doctors can use our system by going through a verification process that assesses the validity of their license to practice medicine ( Doctors, patients who are associated with a hospital that uses our system can directly use this platform )
 Solution Architecture
 This diagram shows how components in our system connect with each other. The device that assists the diagnostics takes two main measurements; heart/lung sounds and temperature. Support for additional peripherals that are used to measure glucose levels in the blood, blood pressure is available. These are the information a doctor usually takes to diagnose a patient initially, most of the medical centers do not have complex and expensive machinery with them unless it is a fully-fledged hospital that treats inpatients. This is because they are not needed for most of the common sicknesses that patients take medicine every day. Our device takes these common measurements, therefore, saves the vast majority of hospital trips people need to take. There is some portion of diseases that require laboratory test results and inner body images which our device does not support. But for reviewing those lab results and blood works this system can be used easily.
 The online platform provides userfriendly interfaces that include the following functionality for each user type:
 For a doctor:
 Schedule Sessions
 Interact with the Patient
 See Patient’s Medical History
 Make Notes
 Write the Prescription
 Control the Device
 For a patient:
 Make Appointments
 Interact with the Doctor
 Make Payments
 Obtain the Prescription
 Get Notified About the Appointments
 Rate and Give Feedback to the Doctor
 Information about patients like their medical history, NCDs (non-communicable diseases: heart disease, stroke, cancer, diabetes, and chronic lung disease), allergies to medication (Penicillin and related antibiotics, Antibiotics containing sulfonamides (sulfa drugs), Anticonvulsants, Aspirin, ibuprofen and other nonsteroidal anti-inflammatory drugs (NSAIDs), Chemotherapy drugs) or food and lifestyle are going to be stored and made available to the doctor who treats that particular patient; this can be useful for the diagnostician and is not usually accommodated in the conventional way of seeing a doctor.
 About the doctors, information about their medical license, specialty, and available times are stored and shown to the users. Functionality for reviewing and feedback is also planned to be implemented.
 Apart from the information about the users, scheduling times, metadata, and statistics will be stored.
 Hardware and Software Designs
 3D Prototype
 The device is completely wireless, WiFi is used to do data communication (can connect with a smartphone/computer without manual configuration).
 Powered with Li-ion rechargeable batteries. There are two inbuilt sensors; a temperature sensor and a microphone (stethoscope).
 The microphone is controlled by the doctor over the internet. (microcontroller is signaled to initiate reading and transmitting data)
 The temperature sensor is activated when pressed against the skin. (continuous measurements are not needed)
 An on/off switch is available to power down the device
 Hardware Components
 esp32
 Relatively high sampling rate
 12 bit ADC bit depth
 Wifi capabilities
 LM35 Temperature Sensor
 0.5°C typical accuracy
 Low-Cost
 Linear scale
 Calibrated Directly in Celsius
 Less Than 60-μA Current Drain
 4V - 30V (operating voltage)
 Amplifier
 To increase signal-to-noise ratio and also as a unity gain buffer
 Features excellent power supply rejection ratio (112 dB)
 Excellent common-mode rejection ratio (126 dB)
 Cell Fuel Gauge
 To measure battery level
 Accurate battery level as a precentage
 Uses I2C protocol
 Circuit Designs
 The following diagrams show the proposed designs for the prototype device:
 Schematic Diagram
 LM53: temprature sensor
 CAO106: electret condenser microphone
 NJM5532: low noise operational amplifier
 PCB Layout
 3D Circuit Model
 Design Decisions
 I2C interface is utilized to allow connectivity for most of the common sphygmomanometers (blood pressure monitors)
 The built-in capacitive touch sensor capability is used to trigger the temperature sensor
 A bandpass filter is used to filter out the unwanted frequencies
 The low pass filter is set to lower frequencies than usual to better suit internal body sounds
 Software Tools and Technologies
 React
 Virtual DOM feature that allows rendering only the changed UI components (avoiding simple changes at the top level from causing huge ripples to the user interface)
 Provide high performance making complex apps run extremely fast
 Node.js
 Load balancing and the capability to handle a huge number of concurrent connections
 Possible to create a separate microservice for any functionality
 Data is divided into small chunks that are sent to the front end piece by piece (Good for video conferencing)
 AWS
 Highly scalable
 Flexible in choosing OS, programming languages, database, and other services
 The pay-as-you-go pricing (pay only for the exact amount of resources used)
 MongoDB
 A non-relational database that is favorable to the data in the system.
 It is a natural form to store data (human-readable)
 Structured and unstructured information can be stored in the same document
 Dynamic schema; adding fields or leaving a field out is possible
 UI Designs
 Click here to see all UIs.
 UI Prototypes
 Patient’s UIs
 Doctor’s UIs
 Algorithms
 When a patient adds an appointment
 When patients join a session
 Testing
 Hardware Testing
 Prototypes with different hardware implementations are planned to be used to test the system. Data generated by our device will be compared against freely available heart/lung sounds measured by the state of the art equipment.
 Software Testing
 Importance of test-driven development (TDD)
 With TDD we identify the bugs right in the development stage. It is impossible to test each component manually when the project grows. And if done by hand it would cost
 a lot of time.
 Unit Tests
 The idea behind Unit Testing is to test every single part of the program separately and show that the individual parts are correct. In unit testing, we test the business logic of a function or a component. The number of test cases for a unit will depend on all the different execution paths.
 For front-end unit testing, we render component trees in a simplified test environment and assert their output
 Integration tests
 The idea behind Integration testing is to combine small units in the application and test as a group to see that
 they are working fine together.
 Integration tests involve database queries and network requests most of the time.
 We use integration tests to prevent bugs from reaching the production stage.
 End to end tests
 The main purpose of End-to-end (E2E) testing is to test from the end user’s experience by simulating
 the real user scenario and validating the system under test and its components for integration and data integrity.
 We use the selenium framework to test the web application.
 Tests in this project
 We have automated unit tests and integration tests using Github workflows. This helps us to refactor with confidence
 and make sure that new pull requests pass all the test cases before they are merged into the main branch.
 Front-end related tests
 Back-end related tests
 End-to-end tests
 Detailed budget
 All items and costs according to the current plan: (might change in the future)
 Item
 Quantity
 Unit Cost (Rs.)
 Total (Rs.)
 ESP32
 1
 1500
 1500
 LM35 Temperature Sensor
 1
 110
 110
 3.7V Li-ion Rechargeable Battery
 2
 200
 400
 Max17048 Cell Fuel Gauge
 1
 500
 500
 AMS1117-3.3 Voltage Regulator
 1
 10
 10
 Logic Level Converter
 1
 135
 135
 Condenser mic CA0106
 1
 20
 20
 Stethoscope
 1
 840
 840
 Others
 2000
 TOTAL
 5515
 Project Timeline
 Supervisors
 Dr.Isuru Nawinne web page
 Dr.Mahanama Wickramasinghe web page
 Conclusion
 Links
 Project Repository
 Project Page
 Survey Results
 Department of Computer Engineering
 University of Peradeniya
 Back to top",An online platform combined with a convenient medical diagnostic device where patients can meet doctors and get diagnosed at the comfort of their home.,E17,Cyber-Physical Systems Projects,https://projects.ce.pdn.ac.lk/3yp/e17/remote-medical-diagnostics/,https://github.com/cepdnaclk/e17-3yp-remote-medical-diagnostics,https://cepdnaclk.github.io/e17-3yp-remote-medical-diagnostics,https://cepdnaclk.github.io/e17-3yp-remote-medical-diagnostics/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/3yp/E17/remote-medical-diagnostics/
55,remote proctoring system,"Remote Proctoring Device
 Home
 About
 Solutions
 Progress
 Timeline
 Designing overall system
 Frontends Implementations
 Students' UI
 Proctors' UI
 Administrators' UI
 Backend Implementation
 Hardware Implementation
 Testing and Integration
 Features
 Budget
 Team
 Contact
 Connexa
 REMOTE PROCTORING DEVICE
 Project Repository
 SEE OUR PRODUCT VIDEO
 Web Application
 Explore the Connexa space
 What is the Remote Proctoring Device?
 A single device which integrates the hardware and software components needed to conduct an examination in the currently implemented system, which will provide a seamless process for the proctors and students involved in an examination.
 why
 Why do we need a seperate device?
 When conducting examinations where the skills of the students should be evaluated in a limited timeframe, it's crucial to manage the external factors affecting the performance of the students at a satisfactory level. However it could be
 challenging to manage these factors in an online environment.
 Currently, multiple hardware devices and software tools are used to conduct online examinations while ensuring the quality of the examinations. We have noticed that this method can cause a lot of distractions and unnecessary
 burdens to students as well as proctors.
 System Overview
 What Are The Technology We Use
 System
 The device on the student's side is capable of capturing the video and audio stream from students continuously even incase of a power failure. The proctor will be able to monitor the video and audio streams captured
 from all the students facing an examination through the browser application in the proctor's side.
 High-Level System Overview
 Method used to ensure secure video/audio streaming:HTTPS protects the user from “Man-in-the-Middle” attacks where hackers can use vulnerabilities in public networks to steal data transmitted to the viewer. Using HLS
 encryption to mask a user's connection with the website can prevent this sort of attack.
 Technology Stack
 The system consists of three main endpoints...
 Web browser in Proctor's end
 Desktop application in Student's end
 Database and server application hosted on the Cloud
 Timeline
 The completion of the product will undergo 5 phases
 Phase 1
 Phase 2
 Phase 3
 Phase 4
 Phase 5
 Phase 1
 2 Weeks : Determine the design architecture and connections of overall infrastructure.
 System overview
 High-Level System Overview
 Technology Stack
 Aproximate Budget
 See more >>
 Phase 2
 7 Weeks : Develop the frontend (Both web and desktop apps)
 Students' User Interface
 Proctors' User Interface
 Admins' User Interface
 See more >>
 Phase 3
 8 Weeks :
 Develop the backend and server side programs
 ER-diagram
 See more >>
 Phase 4
 5 Weeks : Hardware implementation
 Proctoring device circuit
 Power supply unit circuit
 See more >>
 Phase 5
 6 Weeks : Testing and Integration
 Software testing plan
 Hardware testing plan
 See more >>
 Progress
 Our Progress So Far
 Student App Unit Testing
 See more details >>
 Electron application unit testing
 Spectron and Mocha
 Proctors' Web App Unit Testing
 See more details >>
 React application unit testing
 React testing libraries and Jest
 API testing
 See more details >>
 API request testing
 Authentication testing
 Access control testing
 Jest and Postman
 Your browser does not support HTML video.
 Students' Desktop App
 See more details >>
 See course details
 See exam schedule
 Join examination
 Upload locally saved videos
 Your browser does not support HTML video.
 Admin Portal
 See more details >>
 Add proctors to the system
 Add students to the system
 Add courses to the system
 Add exam schedule using master sheets
 Remove exam schedules
 Your browser does not support HTML video.
 Proctors' Web App
 See more details >>
 See course details
 See exam schedule
 Invigilate an examination
 See disconnections of students
 Get the link of the students saved videos
 API call documentation
 See more details >>
 Authentication
 Access control
 Student App Unit Testing
 See more details >>
 Electron application unit testing
 Spectron and Mocha
 Proctors' Web App Unit Testing
 See more details >>
 React application unit testing
 React testing libraries and Jest
 API testing
 See more details >>
 API request testing
 Authentication testing
 Access control testing
 Jest and Postman
 Your browser does not support HTML video.
 Students' Desktop App
 See more details >>
 See course details
 See exam schedule
 Join examination
 Upload locally saved videos
 Your browser does not support HTML video.
 Admin Portal
 See more details >>
 Add proctors to the system
 Add students to the system
 Add courses to the system
 Add exam schedule using master sheets
 Remove exam schedules
 Your browser does not support HTML video.
 Proctors' Web App
 See more details >>
 See course details
 See exam schedule
 Invigilate an examination
 See disconnections of students
 Get the link of the students saved videos
 API call documentation
 See more details >>
 Authentication
 Access control
 Student App Unit Testing
 See more details >>
 Electron application unit testing
 Spectron and Mocha
 Proctors' Web App Unit Testing
 See more details >>
 React application unit testing
 React testing libraries and Jest
 API testing
 See more details >>
 API request testing
 Authentication testing
 Access control testing
 Jest and Postman
 Your browser does not support HTML video.
 Students' Desktop App
 See more details >>
 See course details
 See exam schedule
 Join examination
 Upload locally saved videos
 Your browser does not support HTML video.
 Admin Portal
 See more details >>
 Add proctors to the system
 Add students to the system
 Add courses to the system
 Add exam schedule using master sheets
 Remove exam schedules
 Your browser does not support HTML video.
 Proctors' Web App
 See more details >>
 See course details
 See exam schedule
 Invigilate an examination
 See disconnections of students
 Get the link of the students saved videos
 API call documentation
 See more details >>
 Authentication
 Access control
 Demonstration
 User Interface Implementation
 Your browser does not support HTML video.
 Security
 Security is a key concern in our product since we deal with highly sensitive data related to examinations and personal security.
 We ensure security using 3 main methods.
 Isolation
 Encyrption
 Authorization and authentication
 Security
 Data Encryption
 Encryption in Server and Database
 The secure MongoDB, Atlas cluster ensures secure data encryption both at transit and rest using TSL and AES-256 standards
 Encryption in Jitsi-Meet server
 Jitsi-Meet provides end-to-end and hop-to-hop encryption for multiparty conferences.
 The meeting rooms are password protected and ephemeral.
 Encrypting locally stored videos
 To ensure that students or any other third party cannot tamper locally stored recordings, they are encrypted using AES-256.
 Node.js's built-in cipher class File-encrypter is used to encrypt the recordings.
 AWS Key Management Service is used to control and manage the encryption keys.
 Features
 Check Our Amazing Features
 All
 For The Proctor
 For The Student
 video/audio feed
 Get video/audio feed from multiple students
 One-on-one interaction
 One-on-one interaction with students
 Detect disconnections
 Detect disconneted students immediately
 Detect unauthorized activities
 Detect unauthorized activities by warning messages
 Notify examinations
 Notify students about up coming examinations
 Capture the video & audio
 Capture the video & audio stream from student's environment
 Record and Upload
 Continuously record the video in case of a power failure and store it locally
 Backup Power
 Continuous power supply even incase of a power failure
 Easily mountable
 can easily mountable on a surface
 Display remaining time
 See the remaining time on the screen
 Budget
 Bill of Materials
 COMPONENT
 PRICE (Rs.)
 PLACE
 FOR THE MAIN CIRCUIT
 Rasberry pi board 3B+
 9500
 microchip.lk
 Rasberry pi power cable and adapter
 800
 microchip.lk
 Touch screen 5' with integrated speaker
 6350
 tronic.lk
 Speaker
 50
 microchip.lk
 PAM8406 Digital Amplifier
 350
 microchip.lk
 Cooling Fan
 260
 microchip.lk
 Camera module
 1800
 microchip.lk
 USB Microphone
 485
 tronic.lk
 Flash drive (SD card) 32gb
 1800
 microchip.lk
 Total for the RPI
 21395
 FOR THE UPS
 Lithium Battery 18650 1200mAH
 200
 microchip.lk
 LM2596 Buck Converter
 650
 microchip.lk
 BMS 18650 balanced charger with protection
 350
 microchip.lk
 Capacitor 220 microF
 10
 12V power inout barrel jack
 15
 Micro USB jack
 40
 Nilambara Electronis
 Miscellaneous
 500
 TOTAL
 23160
 Our Team
 Developers
 Isuri Devindi
 E/17/058
 Sashini Liyanage
 E/17/190
 Savindu Wannigama
 E/17/369
 Advisors
 Prof. Roshan Ragel
 Professor
 Dr. Isuru Nawinne
 Senior Lecturer
 Dr. Mahanama Wickaramasinghe
 Senior Lecturer
 Contact
 Contact Us
 Email:
 connexa.info@gmail.com
 Call:
 Isuri- +94713713686
 Sashini- +94713585988
 Savindu- +94776259252",A smart proctoring device which can be controlled remotely by an authorized person.,E17,Cyber-Physical Systems Projects,https://projects.ce.pdn.ac.lk/3yp/e17/remote-proctoring-system/,https://github.com/cepdnaclk/e17-3yp-remote-proctoring-system,https://cepdnaclk.github.io/e17-3yp-remote-proctoring-system,https://cepdnaclk.github.io/e17-3yp-remote-proctoring-system/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/3yp/E17/remote-proctoring-system/
56,smart apartment security system,"Smart Apartment Security System
 Home
 Introduction
 Architecture
 Design
 Budget
 Team
 SAFENET
 SMART APARTMENT SECURITY SYSTEM
 Introduction
 For years, the need to protect one’s property has become one of people’s
 main concerns. The sense of security and protection is one of those feelings that makes us
 comfortable and complements quality living. Although there are existing solutions for home
 security like video surveillance cameras, alarms etc., they are very expensive, and the
 installation process is also not easy. Most of the existing security system solutions
 address the needs of the people who are living in private houses. But people who live in
 apartments in urban areas have some added needs to be satisfied with a particular security
 system. Therefore, addressing all these issues along with some added unique features we have
 come up with the idea of a Smart Apartment Security System. Smart Apartment security system
 is a system that secure entry points, like doors and windows, as well as interior spaces in
 an apartment from a burglary or a fire by notifying the owners and security officer at the
 apartment gate whenever a threat has been identified through a mobile application.
 Solution Architecture
 Mobile Application
 An Android mobile application is available for
 SafeHome security system to allow users to remotely monitor and control the security system.
 Users are able to arm and disarm the system, get mobile alerts and contact the security
 officer whenever a threat has been identified by the security system. Users can log in to
 the mobile app anytime to view the status of sensors, activity logs for door locks and more
 as well. The app is designed using Flutter.
 Security System
 The security system is used to identify
 threats in the entry points and interior spaces of an apartment through sensors. It will
 consist of wireless sensors(door and window sensors, motion sensors, smoke detector), a
 siren and a smart door lock with fingerprint access for added security. ESP 32 development
 board is used as the microcontroller of the door lock and ESP-01S WiFi modules are used for
 wireless sensors.
 Web Server
 AWS server is used as the web server for the
 SafeHome Security System. Signals from sensors are passed to the mobile application as well
 as control information from mobile applications are passed to the smart door lock and
 response mechanisms through this server. Database which is used to store sensor statuses and
 logs of the system is hosted at the server.
 Highlevel
 Diagram
 Infrastructure
 Technologies
 Frontend Development
 As the front end development we use flutter as our front
 end development software in this development process the
 expectations are to provide user friendly and
 interesting interface with enhanced UI and UX experience
 to the client.Other than that cross platform support is
 required for us to maintain the target clients by giving
 them access to our services independant their mobile os
 type.
 Backend Development
 In the backend server requires to handle a larger amount
 of requests. Therefore the scalability and
 sustainability if uncaught error exceptions happend are
 critical points under choosing a optimum backend server.
 For those requirements, Node js would be the best
 solution for this project. Therefore the backend server
 side requests managements are done by the NodeJs.
 Cloud Deployment
 Due to financial issues met in the process of developing
 the physical server of maintaining cost. The team sole
 decision and the academic guidance as well based on
 cloud services. Therefore the cloud server deployment
 was decieded. In that case the EC2 instance of AWS
 server support is selected.
 Database Development
 For storing data puroposes' requirements were the
 structured schema with the same type of data that have
 the ability to contain all the details of the target
 market when the product able to cover the whole market
 itself. Also the data base should be able to scalable
 from a smaller database. For these requirements are
 fulfil when the chosen selection was Mysql. Because the
 mysql have the ability contain 1000 different tables and
 all our reqiurements are fulfilled under 1000 tables.
 Also a table can record 21 million records with the
 space under 1GB. The maximum row table will be our user
 table but it will not exceed 21 million because the
 targeted ordience is less.
 Circuit Diagrams
 Door Circuit Unit
 Flame and PIR Sensor Unit
 Window Sensor Unit
 Schematic Diagrams
 Hardware Design
 Diagrams
 ER Diagram
 Relational Schema
 Detailed Budget
 Interfaces of Mobile Application
 User Login Interface
 The user can login here. He has to enter his email address
 and password. Two factor authentication is used here.
 Therefore, an OTP is sent to the relevant email address. If
 the user doesn't have an account, he can sign up to the
 system.
 Security Officer Login Interface
 The security officer can login here. He has to enter his
 email address and password. Two factor authentication is
 also used here.
 Therefore, an OTP is sent to the relevant email address. The
 registration of security officer is done by the
 Administrator.
 User Home Interface
 The user can change the mode of house. If the user select
 'Home Mode', the notifications wouldn't get when the motion,
 door and winsow semsors are triggering.
 There are 6 buttons in the home screen and user can go into
 the relevent option.
 User Registration Interface
 The users can register here. The email address is verified
 by sending OTP. Only the owner of the house can register to
 the system.
 Other members of the house can login with his user
 credentials.
 User Details Interface
 The owner details can be seen here. The name and the phone
 number can be changed
 here.
 Sensors' Details Interface
 All the sensor details can be seen here.One button is to
 deactivate the sensors and
 other button is to give access to the
 security officer to enter the house when there is a security
 threat.
 Layout Interface
 The user can find the places of the sensors in the house
 using the unique ID of
 them.
 Notification Interface
 The notifications when the sensors are triggering, can be
 seeen here. The date and
 time are also there.
 Contact Details Interface
 The emergency contacts can be seen here. By pressing the
 button, the user can calling to the
 relevant person.
 Security Officer Home Interface
 There are 4 buttons. Using 'Front Door Sensors', SO can
 find the front door of houses which he can access.
 Sensor Details(SO) Interface
 The status of all the sensors' of the apartment are showing
 here.
 The activate sensors of the apartment can be seen in red
 color.
 User Awaymode Interface
 When the awaymode is activated, background color of the interface is changed blue to green and the sensors are not triggering
 Interfaces of Administrator Web Application
 Previous
 Next
 AWS Deployment
 Previous
 Next
 Security
 Security is the most important thing in the IT world right now.
 In our solution, we have deeply considered the security point of view of our system.
 We have developed our system with the help of the following techniques to enhance the
 application security in our solution.
 JSON Web Tokens (JWT)
 Our application use JSON Web Tokens (JWT) to allow the client to indicate its identity for
 further exchange after authentication. It is a self-contained way for securely transmitting
 information between parties.
 User Input validation
 This prevents improperly formed data from entering an information system. Because it is
 difficult to detect a malicious user who is trying to attack software, applications should
 check and validate all input entered into a system specially in Resgistration and Login.
 Input validation as a mitigation strategy for both SQL injection and XSS.
 Two Factor Authentication
 Used in Authentication is used in the Registration of the Mobile Application. When Email is
 provided, OTP is sent to email address and user has to Enter that OTP for the registration.
 Password Hashing
 Password hashing is used in our application to verify the integrity of passwords, sent
 during login, against the stored hash so that actual passwords never has to be stored. A
 salt is added to the hashing process to force their uniqueness, increase their complexity
 without increasing user requirements, and to mitigate password attacks like hash tables
 AWS EC2 Security Groups
 A custom security group is assigned to the instance where our application is deployed which
 allows only http/tcp connections on port 80. Security groups offer protection at the ports
 and protocol access level.
 Testing Plan
 Project Timeline
 Link to Demonstrations
 Our Team Members
 Dananjaya Morais
 E/17/212
 Ishini Udara
 E/17/312
 Kanishka Dilhan
 E/17/065
 Our Advisors
 Dr.Isuru Nawinne
 Dr.Mahanama Wickramasinghe
 University of Peradeniya.
 Phone: +94 81 239 33 00
 Email: vc@pdn.ac.lk
 Web-site: http://www.pdn.ac.lk/
 Faculty of Engineering.
 Phone: +94 81 239 33 02
 Web-site: http://eng.pdn.ac.lk/
 Computer Engineering Department.
 Phone: +94 81 239 39 14
 Web-site: http://www.ce.pdn.ac.lk/
 Github repo
 Find more informations and
 codes details.
 Copyright 2019 All Right Reserved By Department of Computer Engineering/FOE/UOP",Smart Apartment Security System is a security system which can be used for houses in an apartment.,E17,Cyber-Physical Systems Projects,https://projects.ce.pdn.ac.lk/3yp/e17/smart-apartment-security-system/,https://github.com/cepdnaclk/e17-3yp-smart-apartment-security-system,https://cepdnaclk.github.io/e17-3yp-smart-apartment-security-system,https://cepdnaclk.github.io/e17-3yp-smart-apartment-security-system/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/3yp/E17/smart-apartment-security-system/
57,smart garbage collection,"Smart Garbage Collector
 TrashCube
 Home
 About
 Architecture
 Features
 Data Flow
 Data Generation
 Flow Chart/ Algorithms
 Diagrams
 Design
 Circuit
 3D Model
 UI /UX
 Progress
 Timeline
 Budget
 Team
 Contact
 Smart Garbage Collector
 For A Proper Garbage Management For Smart Cities
 Project Repository
 why
 Why do we need a Smart Garbage Collector?
 Proper garbage collection & disposal has been a continuous struggle. Main issue that we identified is that there is no way to get an overview of fill levels of the bins. Therefore, authorities in charge of collecting garbage do not
 know when and where to collect garbage. This leads to inefficiency in assigning garbage collectors. And also time and fuel consumption are hight when collecting garbage, due to lack of a proper system. People tend to burn plastic/polythene
 because of the lack of a proper household garbage collection system and it gives rise to severe environmental polution. Furthermore, overflowing garbage bins which can be seen very often in public areas affect on public health
 negatively and scenic beauty of the environment is also destroyed.
 Solution Overview
 What is Smart Garbage Collector?
 Smart garbage collection system provides a platform to manage garbage collecting in a large area with proper coordination between the responsible authorities and the workers assigned to collect garbage while utilizing the available
 resources effectively. And also the people who are in need of a proper system to dispose garbage will be benefited by the system. The system can be implemented in cities, grounds, parks and any large public areas and it will make
 a high positive impact on public health & environment as well.
 Architecture
 Solution Architecture
 The main device of our system is the Smart Garbage Bin.
 Inside the Garbage bin the main controller is ATmega328p microcontroller. From this controller,
 we implement three main features. They are User Alerting system, Fill Level Detection System
 and Compaction mechanism. Considring compaction mechanism, it has seperate security system to
 detect the compation mechanism. The main power supply of the system is solar panel unit. There
 is also a bin location identification system inside the smart garbage bin.
 The Smart Garbage Bin can be connected to the AWS server
 via the GSM module and GPS module. In order to communicate with the server, there will be a web
 application as well as a mobile application. When garbage is put in to the bin, the LED indicators in
 Fill Level Detection Unit will be turned on according to the fill level. And, if the Garbage Bin is
 already full, Bin sends the fill level information to the server. Then, the server is responsible for
 updating details in both mobile application and web application. According to the compaction process
 details in the web server, the garbage bin will be automatically compacted. Otherwise, web application
 will send the request to Garbage Bin Collector through the mobile application. He/ she can view all
 the locations of assigned bins using the Google map in the mobile application.
 Features
 Check Our Special System Features
 FEATURES OF GARBAGE BIN
 Fill Level Detection
 Can decide on the priority to collect garbage
 LED Indicators
 Users can know whether to dispose or not & night visibility
 Compaction
 Reduce number of times garbage should be collected, max space utilization
 FEATURES OF WEBSITE
 Get Overview
 Can decide on priority, can stop overflowing bins efficiency
 Set Parameters
 Parameters to detect fill levels change as prefered
 View on a Map
 Easy to keep track of the procedure
 Add/Remove Bins
 Expanding the system with more units
 FEATURES OF Mobile App
 Handle Requests
 Can cancel or accept requests based on availability
 Get Overview
 Can get an idea on how much garbage is there to be collected
 Locate on a Map
 Can easily find the best route to collect the garbage bins assigned, increased efficiency
 Website
 Used for administration purpose by the adminintrator
 Mobile App
 Used by the garbage collectors
 Garbage bin
 Used by the general public
 Data Flow
 Data Generation
 Smart Garbage Collector system's block diagram consists of following four units
 Input Unit
 Controlling Unit
 Processing Unit
 Output Unit
 The input unit acts as the power supply unit in the system. Here the solar panel
 collects power from the sun’s rays.
 It is a monocrystallic panel, containing solar photovoltaic cells which convert the sun
 rays into electricity. The Dc power from the solar panel is then stored in the 12 volts battery
 used to operate all the electronic components in the controlling unit and processing unit.
 Then the controlling unit consists of relay switch [5V dual channel], ardino UNO [ATmega328 8-bit], ultra- sonic sensor
 and LED indicators.
 These components are used to control the complete processing unit. The ultra- sonic sensor senses the garbage
 accumulated in the container bin at the constrained level (say about 5cm from the top of the container bin) and sends signal
 to the micro chip. The micro chip is programmed to actuate the GSM module which sends a message to the administrator
 when the constrained level of garbage is accumulated in the garbage bin. It also controls the relay switch to actuate the DC motor.
 And considering the fill level information, LEDs are turned on by the micro chip.
 Next, the processing Unit has the DC motor. The Dc motor is connected to relay switch which
 actuates the scissor mechanism to compact the garbage in
 the garbage bin at constrained level. The relay switch with a H-bridge arrangement is used to control the Dc motor. This
 relay switch as two switch s1 and s2. When s1 is ON’s, the motor terminals with a positive polarity, thus the direction of
 rotation of the lead screw connected to the motor is in clockwise direction to extend the mechanism, similarly when s1 is
 off and s2 is actuated the lead screw will rotate in counter-clockwise direction to retract the mechanism. Next, the output unit
 consists of a GSM module, when the ultra-sonic sensor senses the level of garbage in the container bin, therefore actuating the
 scissor mechanism for compaction of waste and when the garbage
 reaches about 3cm at top of the container bin, the ultra sonic sensor sends signal to the GSM module via mocro chip,
 thereby it sends the message stating that the bin is going to be filled and need to dispose the wastes. Tha data and control flow can be observed in the diagram.
 Block Diagram
 ×
 Data and Control Flow
 ×
 Flow Diagram
 Firstly, the GSM module and other ports are initialized.
 Then, we check whether garbage bin is used by someone. If it is used, the fill height is sensed
 and fill level is calculated. If the fill level is lower than 50%, Green colored LED is turned on,
 if the fill level is in between 50% - 80%, yellow LED is turned on. Otherwise, red LED is turned on.
 If red LED is turned on, it s checked wheth bin needs to be compacted or not by using the number
 of compaction processes. If number of compaction processes are lower than 3, then the compaction
 process is automatically turned on. When compaction is going, if someone tries to put garbage
 in to the garbage bin, it will be automatically stopped. If it is not, compaction process will be
 completed. After that the number of compaction processes will be updated.
 ×
 Algorithm
 Automation Algorithm
 For What?
 How Implemented?
 Initial Condition
 To stop assigning
 same location to two Garbage Collectors.
 Check the database whether another bin in the unit related to
 bin in which the request is sent has been assigned to another garbage collector.
 Assign the request to that same Garbage Collector.
 First Criteria
 To distribute the workload evenly among Garbage Collectors.
 Define an amount of tasks for one round.
 Store the number of active tasks of every garbage collector in the database.
 Find round number for each collector using amount of active tasks and tasks per round.
 Select collector who has the lowest round number.
 Second Criteria
 To assign the nearest collector to the bin.
 Get smallest distances from collectors to bins
 using locations.
 Select collector who has the smallest distance to the bin.
 Third Criteria
 When the Garbage Collecting starts, everyone can start from the same round and same location.
 To assign the bins next to each other in the same zone to a collector by Preventing two bins
 that are too far apart from, being assigned to the same collector.
 Every red bin must be assigned to relevant collector in the zone.
 Access database and find any relevant collector in the zone corresponding to the bin
 where the request came from.
 Diagrams
 Check our Database System
 This is the Conceptual Schema of the relational databse of our system.
 Each entity can be explained as given below.
 ADMIN - The administrator of whole Garbage Colllection and Disposal System can register to
 our system and they will be provided an unique id.
 COLLECTOR - The Garbage Collector also can register to the system and he/she will be provided an
 unique id which is given for only garbage collector.
 ZONE - A zone has units and it has a n unique id. The Collectors are allocated to zone.
 SYSTEM - The Garbage Collection system settings can be changed by the administrator. The administrator
 can change bin height and fill level details (Green range, Yellow range and Red range)
 BIN - A Smart Garbage Bin is registered to the system by using an id and the category,
 battery capacity, compaction cycles and fill level details are the main attributes of the Bin.
 UNIT - One Unit consists of bins. Each unit has an id and the location details.
 Conceptual Schema
 ×
 Logical Schema
 ×
 Circuit Designs
 Check our Circuit Diagrmas
 POWER SUPPLY UNIT
 Solar panel absorbs sunlight with the help of photovoltaic cells which convert the sun rays into electricity.
 Battery Charge controller is used to control the rate at which electri current is added from electric batteries and it prevents overcharging.
 Then electric power stored in the battery which is used to operate all the electric components.
 View Circuit Diagram
 FILL LEVEL DETECTION UNIT
 The ultrasonic sensor senses the waste in the bin at the constrained level and send signals to the microcontroller.
 0-50% of fill level - on Green LED
 50-80% of fill level - on Yellow LED
 Greater than 80% of fill level - on Red LED
 View Circuit Diagram
 COMPACTION UNIT
 Microcontroller turn on the relay switch, once the bin is full and compaction times are less than 4 to actuate the DC motor.
 DC motor activates the scissor jack mechanism to compact the waste in the bin.
 When the compaction begins, buzzer will be turned on and User activity can be monitored using IR sensor.
 View Circuit Diagram
 3D Model Designs
 Check our Smart Garbage Bin Designs
 UI /UX Design
 Web Apllication UI Designs
 Once the administrator creates an account and login to
 this application, he/she can,
 Get a full Overview of the entire Bin Systemwith the fill levels and other Details.
 Search certain Units by Unit ID.
 Toggle between the views(Table View and Graph View).
 Add/ Remove Bins to the System.
 Change System parameters(Bin Height, Fill Level Ranges).
 View accepted/declined requests by Garbage Collectors.
 Mobile Apllication UI Designs
 Once the Garbage Collector creates an account and login to
 this mobile application, he/she can,
 Accept/Decline all garbage collecting requests.
 Accept one/more than one request.
 See all locations of accepted Garbage bins.
 Filter requests.
 Previous
 Next
 Previous
 Next
 Security
 System Security Concerns
 JSON Web Token(JWT) - JWT is used to securely transmit messages
 of communication between
 Garbage bin and Administrator and communication between Garbage Collector and Administrator. By using JWT, the messages
 can be verified and trusted as they are digitally signed.
 Password Hashing - When Administrator login to the website by entering username and
 password, we use passwoord hashing to verify the integrity of the password. From this the actual password would never
 be stored in the database.
 AWS RDS/EC2 Security Features - We use SSL/TLS to communicate with AWS resources. And we setup the API
 and logging details using AWS CloudTrail.
 Progress
 Web App Completed Video
 Mobile App Completed video
 Cloud Deployment
 AWS Cloud Deployment
 AWS Deployment of the system is done by according the given AWS architectute. All the steps are given below.
 And We connect the backend of our system to AWS and tested it through POSTMAN tool.
 Testing
 Check Our Testing
 View Our Testing Codes
 Timeline
 Our Project Timeline
 05th July, 2021 - 19th July, 2021
 Presenting the Project Proposal
 Thinking innovative ideas for the project and presenting one specific idea with
 a project proposal.
 20th July, 2021 - 27th July, 2021
 Frameworks and Technologies
 Getting familiarized with the frameworks and technologies.
 27th July, 2021 - 10th August, 2021
 Front End Development
 Website Basic Front End Development.
 Mobile App Basic Front End Development.
 10th August, 2021 - 24th August, 2021
 Designing 3D Model
 For Smart Garbage Collector, the 3D model is designed. See 3D model from here.
 24th August, 2021 - 03rd September, 2021
 Progress Review - Milestone 2
 Presenting detailed soultion architecture.
 Presenting Technology familiarity.
 Demonstrating progress of software applications.
 Presenting testing methods which are used in system development.
 03rd September, 2021 - 17th October, 2021
 Back End Development
 Website Back End Development.
 Mobile App Back End Development.
 17th October, 2021 - 20th October, 2021
 Software Testing
 Web and Mobile App UI Testing by using Appium.
 REST API Testing by using Postman.
 Server Loading Testing by using JMeter.
 20th October, 2021 - 25th October, 2021
 AWS Deployment
 Storage and backups - Amazon S3
 Database - Amazon RDS MySQL
 Hosting - Amazon EC2
 25th October, 2021
 Progress Review II
 Presenting System Functional completeness.
 Presenting the software testing plan.
 Presenting 3D Model.
 Our Budget
 Bill of Materials
 Component
 Quantity
 Unit Price (LKR)
 Price (LKR)
 ATMEGA328P Microcontroller
 1
 600
 600
 GSM Module
 1
 1950
 1950
 GPS Module
 1
 1250
 1250
 Solar Panel (20W, 12V) with Polycarbonate shield
 1
 6000
 4500
 PWM Charge Controller(12V, 20A)
 1
 4200
 4200
 Battery(12V)
 1
 2499
 2499
 Ultrasonic Sensor
 2
 190
 380
 IR Sensor
 1
 110
 110
 LED
 3
 11
 33
 Buzzer
 1
 35
 35
 DC Motor(12V)
 1
 1000
 1000
 Dual-Channel Relay Module
 1
 250
 250
 Outer Cover with Scissor Jack
 1
 2000
 2000
 Total
 18772
 Our Team
 Developers
 Isara Tillekeratne
 E/17/352
 Hashini Wijerathne
 E/17/398
 Vidurangi Kalpana
 E/17/148
 Advisors
 Dr. Isuru Nawinne
 Senior Lecturer
 Dr. Mahanama Wickaramasinghe
 Senior Lecturer
 Smart Garbage Collector
 Call Us
 Messsage Us
 Quick Links
 Smart garbage Collector is an IoT system which facilitates the management of Garbage Collection and disposal.
 +94 77 764 8682 (Isara)
 +94 77 269 3120 (Hashini)
 +94 70 327 2396 (Vidurangi)
 e17352@eng.pdn.ac.lk
 e17398@eng.pdn.ac.lk
 e17148@eng.pdn.ac.lk
 University of Peradeniya
 Faculty of Engineering
 Department of Computer Engineering","Smart garbage collection system provides a platform to manage garbage collecting in a large area with proper coordination between the responsible authorities and the workers assigned to collect garbage while utilizing the available resources effectively. And also general public who are in need of a proper system to dispose garbage will be benefited by the system. The system can be implemented in cities, and any large public areas and it will make a high positive impact on public health & environment as well. ",E17,Cyber-Physical Systems Projects,https://projects.ce.pdn.ac.lk/3yp/e17/smart-garbage-collection/,https://github.com/cepdnaclk/e17-3yp-smart-garbage-collection,https://cepdnaclk.github.io/e17-3yp-smart-garbage-collection,https://cepdnaclk.github.io/e17-3yp-smart-garbage-collection/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/3yp/E17/smart-garbage-collection/
58,smart home,"Smart-Home
 Smart Home
 Introduction
 Solution Architecture
 Hardware
 Hardware model
 Circuit Diagram
 Software model
 Others
 Budget
 Timeline
 Team
 Future Home for everyone
 Everything is on mobile, why not home?
 Github Repository
 Features of our smart devices
 Smart-bulbs
 Smart Switches
 System
 Introduction
 Smart-Home is meant for convinence and efficecy. Controlling and monitoring
 your home by your smart-phone make things easier. Smart-bulbs,smart-switches and
 smart-bliends
 make your home more smarter.
 Problem
 In the Non-smart-Homes or the traditional homes, so much energy waste
 is due to improper management.
 Monitoring the full home appliances is insane. When the peak of the electric bill,
 we don't know what eats the power so much. It is so inconvenient some time for example,
 when you forgot to turn off the light before sleeping or going far away and forgot to turn
 off
 your bulbs or switches..
 Solution
 Solution is simple. Using concepts like IOT, HCI and AI(future plan)
 we can implement smart devices for the homes. They can be control using our mobiles and they can
 semi-automated. And also you can monitor your home power consumption details by using and
 smart-switch
 embeded with
 voltage and current sensor. For this project we going build smart bulbs and the
 switches.
 Solution Architecture
 Our solution Architecture contains two types of devices, they are
 Smart-Bulbs & the Smart-Switchs. They both devices, sensors like motion
 sensor,
 AC current sensor, AC voltage sensor and the Rellay are directly connected with the Central
 Unit.
 Our mobile application and the central unit connectedto the cloud via internet. So they can
 communicate
 with them-self. The central unit will use the MQTT protocol to communicate with the
 server.
 The energy consumption monitoring, this is a feature embeded with all the switches to
 calculate the
 energy consumption and update it to the cloud. This is can be implemented using Two sensor
 AC Current
 sensor
 and AC voltage sensor.
 Motion Sensing, Motion sensing is another feature built-in with the bulbs, The motion
 sensors are
 connected
 to the central unit. When a motion detected or the central unit wil turn on the bulb and if
 no
 motion/human detected
 the CU will turn off the bulb.
 Mobile applications communicate with the central unit through the cloud or diretly using
 wifi router.
 The User settings, user data, and the reports will be stored in the cloud/server.
 UI DESIGN
 ER Diagram
 Hardware Model
 This our Hardware model of our project.
 These are the hardware componets we planned to use:
 NodeMCU32
 PIR sensor
 Relay switch
 AC Dimmer
 AC Current
 Sensor
 AC
 Voltage
 sensor
 NodeMCU32S
 NodeMCU32S is development board which is embeded with the esp32 micro
 controller which
 so small but powerfull. It's the perfect controller for our project because it's
 cheap, it already
 comes with the wifi module and it has many features.
 Feature of the ESP32:
 18 Analog to DC converter
 3 SPI interfaces
 3 UART interfaces
 2 I2C interfaces
 16 PWM output channels
 2 DAC
 2 I2S interfaces
 10 Capacitive sensing GPIOs
 Referense
 Link
 Hardware Model
 PIR Sensor
 Passive Infrared Sensor commanly known as Proximity motion
 sensor. PIR sensors are
 used to detect the movements. It functioning by absorbing the IR rays emits
 from the objects.
 Humans and animals emit the IR radiation other than that the hot objects
 also emits the IR rays so
 IR sensor detect the movements of the objects. Many variety of sensors in
 the market they vary
 with price, sensitivity and range. For this project we HC-SR501. Because it
 mid range wide, angle,
 good sensitivity and cheap
 Referense Link
 Hardware Model
 Current Sensor
 ACS 712 Current sensor used to measure the current using the
 Hall-Effect princile.
 Many current sensors are in the market but we selected this because of it's
 size and accuraccy.
 it's so small but it can measure up to 30A and the energy wastage is
 negligible.
 30 A module
 5V Operating Voltage
 Scale factor 100 mV per Amp
 Referense Link
 Hardware Model
 Voltage Sensor
 ZMPT101b is a voltage sensor which accurate and small in size
 and best suited for
 the
 IoT developments.
 Measure up to 250 V
 Operating Voltage: DC 5V-30V
 Output Signal: Analog 0-5V
 Referense Link
 Hardware Model
 3-3.3V Relay
 Relay is an electrically operated switch. It consists of a set of input
 terminals for a
 signal or multiple control signals, and a set of operating contact
 terminals. The
 switch may have any number of contacts or multiple contact forms, such as
 make
 contacts, break contacts, or combinations thereof.
 Relay are used where it is necessary to control a circuit by an independent
 low-power
 signal, or where several circuits must be controlled by one signal.
 Referense
 Link
 Hardware Model
 AC Dimmer
 The AC Dimmer is designed to control the alternating current voltage,
 which can transfer current up to 400V/8А. In most cases, Dimmer is used to
 turning the power ON/OFF for lamps or heating elements, it can also be used
 in
 fans, pumps, air cleaners, e.t.c. Lately, Dimmer has become an often-used
 decision
 for smart home systems. For example, when you need to smoothly change the
 light brightness.
 Power : up to 400V/600V (8A~24A)
 Operating Voltage 0 - 3.3 V
 Current signal > 10mA
 Referense Link
 Hardware Model
 ❮
 ❯
 CAD MODEL
 Circuit Diagram
 Circuit Diagram of the Smart Plug.
 Here the led denotes the Load.
 Circuit Diagram
 Circuit Diagram of the Smart Bulb.
 Here the led denotes the Bulb.
 Software Model
 Flutter
 Node.js
 MongodB
 MQTT
 AWS-Cloud
 TESTING DEMO
 Budget of our Project
 This is the budget of our project
 Timeline
 Team Members
 Arshad MRM   E/17/015
 Nishankar S   E/17/230
 Varnaraj N   E/17/358
 Supervisor
 Dr. Isuru Nawinne
 Dr. Mahanama Wickramasinghe
 Step in to the future
 App
 store Google play
 © Copyright digitalHuT. All Rights Reserved
 Designed by Students of UOP",,E17,Cyber-Physical Systems Projects,https://projects.ce.pdn.ac.lk/3yp/e17/smart-home/,https://github.com/cepdnaclk/e17-3yp-smart-home,https://cepdnaclk.github.io/e17-3yp-smart-home,https://cepdnaclk.github.io/e17-3yp-smart-home/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/3yp/E17/smart-home/
59,smart shopping cart,"Smart Shopping Cart
 Smart Shopping Cart
 Home
 About
 Solution Architecture
 Implementation
 Team
 Supervisors
 More..
 budget
 Timeline
 Mobile App
 Desktop App
 Testing
 Get Started
 Better Experiance with Smart Shopping Cart
 Make your shopping more easy
 Get Started
 Watch Video
 Introduction
 Smart Shopping Cart is an innovative consumer purcjasing product that is designed to help shoppers' fast-track their shopping experince. The concept of this smart cart will revolutionize the purchasing experience
 of every buyer.
 Problem
 Long waiting queues
 Waste time on searching products
 Forget to buy products
 Safety and health during a pandemic
 Difficult to display offers
 Lot of labour for scanning and billing process
 Our Solution
 Customer can purchase the product by just scanning the barcodes printed on the products before adding it to cart
 Mobile app keeps adding the items in list and the total amount is updated accordingly
 Customers can scan and remove any item
 LCD will show the improved bill at each instance
 Billing will be done automatically
 Customers can do the bill payment through their preferred payment method
 Customers will be able to view their digital receipts via app
 Solution Architecture
 Solution based on our project idea
 Data Path
 In our system have many data transfer process this is the data transfer flow in software, hardware and server
 Overview
 These are our implementations for our project
 Mobile App
 Mobile Application developed for the Customer who uses our smart shopping cart
 Web App
 Web Application developed for the Admin and staffs they can manage the activity and transactions.
 Hardware
 Our Hardware system will be placed on the shopping cart
 Testing
 Software and Hardware tesing
 BUDGET
 Timeline
 Our project plan timeline
 Timeline
 We planed to improve the project in this time flow
 Team
 We are the members who develop this projects
 Rilwan M.M.M
 E/17/292
 Studing at university of peradeniya
 Kavinaya Y
 E/17/159
 Studing at university of peradeniya
 Piriyaraj s
 E/17/256
 Studing at university of peradeniya
 SUPERVISORS
 The supervisors who give guidance for the project
 Dr. Isuru Nawinne
 Senior Lecturer
 Dr. Mahanama Wickramasinghe
 Senior Lecturer
 © Copyright SMART SHOPPING CART. All Rights Reserved
 Designed by
 KRP ROCKERS","Many people like shopping but the main problem is the time take for the billing. We like to solve this problem. In this case, we like to introduce a device that can do all billing work so that there is no need to wait for billing. they can buy things and pay themselves. developed by @Piriyaraj, @Rilwan292, and @Kavinaya12 https://cepdnaclk.github.io/e17-3yp-smart-shopping-cart/",E17,Cyber-Physical Systems Projects,https://projects.ce.pdn.ac.lk/3yp/e17/smart-shopping-cart/,https://github.com/cepdnaclk/e17-3yp-smart-shopping-cart,https://cepdnaclk.github.io/e17-3yp-smart-shopping-cart,https://cepdnaclk.github.io/e17-3yp-smart-shopping-cart/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/3yp/E17/smart-shopping-cart/
60,agribot,"AGRIBOT
 Menu
 About
 introduction
 Architecture
 Design
 Team
 budget
 Autonomous Agricultural Robot
 AGRIBOT
 Find More
 About
 Overview
 Agriculture sector has always performed as a major economic force in Sri Lanka, making a significant contribution to the national economy, food security and employment. At the same time agriculture is the livelihood of the majority in the rural sector and plays a key role in alleviating rural poverty. This has been well recognized from the time of independence and there has always been a cabinet portfolio set aside for the agriculture sector.
 Problem
 Lack of laborers, the difficulty of finding labourers or can't afford daily wages for them are some of the main problems that today's farmers are facing. Not only that less knowledge about environmental conditions and pests also a problem faced by farmers.
 Solution
 The solution to the problem will be an automated robot to automated the seeding process as well as to identify the environmental conditions.
 INTRODUCTION
 Why AgriBot?
 Smart Farming is a widely growing area. In smart farming user can monitor their field via smart
 device and control the watering, fertilizing autonomously. With this concept, people tried to develop into the next level. They want to use robots into the field to reduce the labour work. There are robots which can do seeding, cropping, identify diseases and literally everything. So now people are not just monitoring the field, they can maintain the whole field and labour cost is minimum.
 So you will ask if there are already machines which can seeding, why do we need an AgriBot? The answer is normally there are some machines which consume a lot of power because they have heavy machinery components. They have a lot of disadvantages like, they have huge drilling components which turn upside down the field area, not good for the soil creatures and soil structure, sound and air pollution, not good for small seeds. In that case, AgriBot works really well, it can map the whole field and seeding. Agribot is the only drill point where we need to put seed. Also cost-effective, lightweight and eco friendly. This is the best way to seeding small seeds in big areas. Another thing is very easy to operate through a user-friendly mobile app.
 AgriBot for Greenhouse farming
 AgriBot is the best solution for modern Greenhouses because those are full covered areas which have sensitive sensors all over the place. Because in greenhouses every condition which plant will depend, is measured and controlled accurately and another thing is there are tap
 lines all over the place. For areas like this, you can’t use heavy machinery or drones to seed plants and AgriBot is the perfect solution.
 Solution Architecture
 Mobile Application
 Android mobile application is available for the AgriBot. The app is designed using Android Studio. Mainly the app is used to configure the robot while temperature and humidity readings can get through the mobile app. Both these procedures happen through AWS servers.
 Automated Robot
 The robot is used to plant seeds over a farming area.We use ESP32 SIM8000I as the microcontroller of use robot.An MPU6050 and encoders are used to navigate the robot in the farming area. An ultrasonic distance sensor is used to identify the obstacles in the path of the robot.A drill bit is used to dig the ground and put seeds. Motors are used to drill the land and control the drill bit.
 Web Server
 AWS server is used as the webserver for the AgriBot. Initial parameters which we will take by mobile app are passed to the robot through this server. Firebase database is used for store the credentials details as well as the specific data of the Robot.
 Highlevel Diagram
 Design
 Click on the image to see details of each sub-sections.
 Mobile Application
 Overview
 Frontend UI
 Testing
 Server Backend
 Overview
 Deploy
 Database
 Agribot
 Overview
 3D Model
 Hardware Components
 Circuit Diagram
 Navigation
 Communication
 Our Team
 ""Alone we can do so little; together we can do so much."" – Helen Keller
 Maneesha Randeniya
 E/16/313
 Nipun Dewanarayane
 E/16/360
 Denuke Disanayake
 E/16/089
 Our Advisors
 “Education is the passport to the future, for tomorrow belongs to those who prepare for it today.” – Malcolm X
 Dr.Isuru Nawinne
 Dr.Ziyan Maraikar
 Dr.Upul Jayasinghe
 Budget
 Full detailed estimated budget according to market price in Sri Lanka.
 Github repo
 Find more informations and codes details.
 Visit
 University of Peradeniya.
 Phone: +94 81 239 33 00
 Email: vc@pdn.ac.lk
 Web-site: http://www.pdn.ac.lk/
 Faculty of Engineering.
 Phone: +94 81 239 33 02
 Web-site: http://eng.pdn.ac.lk/
 Computer Engineering Department.
 Phone: +94 81 239 39 14
 Web-site: http://www.ce.pdn.ac.lk/
 Contact Us
 Get in touch with us.
 Send Message
 Copyright © Agribots 2021
 Overview
 Overall design
 Objective:Navigate through the farm area and do the seeding.
 Height :15-20 cm
 Length : 20-25 cm
 Width : 15-20 cm
 Weight : 1 - 1.5 kg
 Speed : 0.5 ft/s
 Average work time : 60 - 90 min
 Close
 3D MODEL
 3D model views of the Agribot
 3D model was implemented by using Tinckercad which is available for free online tool. Below pictures shows some views of the agribot 3D model which is taken from different angles.
 Get the obj file from here
 Close
 Hardware Components
 Hardware components which will use for implement agribot
 Microcontroller
 Temperature & Humidity sensor
 DHT11 Temperature & Humidity Sensor
 Take measurements every 30 mins. Measure the humidity and temperature of the current environment
 Use the DHT11 sensor module because the module will have a filtering capacitor and pull-up resistor inbuilt.
 The sensor is factory calibrated and hence easy to interface with other microcontrollers.
 Humidity measurement range: 20% ~95%
 Humidity measurement error: ±5%
 Temperature measurement range: 0℃~50℃
 Temperature measurement error: ±2 ℃
 Operating voltage:3.3 V~5 V,
 Operating current: 0.3mA
 Relay module
 SRD-3VDC-SL-C Relay Module
 The drilling operations of the drill bit is operated using an SRD-3VDC-SL-C Relay Module.
 Sealed typed
 Coil nominal voltage - 3 v
 Nominal current - 120 mA
 Power consumption of coil - abt. 0.36W
 Motor Driver
 Driver Model: TB6612FNG H-Bridge
 Motor supply voltage of 2.5 to 13.5 volts DC.
 Logic supply voltage of 2.7 to 5.5 volts DC.
 Output current of 1.2 amperes continuous, 3.2 amperes peak.
 Built-in thermal shutdown.
 Efficiency 91-95%
 Small module, no heat sink required
 Servo Motor
 Model: SG90
 Two servo motors to work as a valve of our seed container and another one to place the drill bit in the correct position
 Operating voltage - 3.0V~7.2V
 Working Frequency - 50Hz
 Motor Type - Brushed DC Motor
 Gear Type - Plastic Gears
 Close
 Circuit Diagrams
 This Diagrams shows how components wired.
 Close
 android application
 This is the overview of Android mobile application
 Android mobile application is available for the AgriBot. The app is designed using Android Studio. Mainly the app is used to configure the robot while temperature and humidity readings can get through the mobile app.
 Overview
 Start of the app user have to enter the Product Id to Login
 Using that ID, App will subscribe to corresponding topics
 App will display the sensor data and connection state of the Robot
 Also user can publish initial plantation instructions to Robot
 And essential control signals to autonomous process
 Scalability
 Firebase Database can easily increase storage and entries.
 New hardware devices can be added
 App can simply connect with any Device for a given ID.
 In the broker, support the Asynchronous process
 Can handle multiple topics at a time.
 Reliability
 Responsive UI
 Connect with any device simply entering the device ID
 Unique topics to isolation
 Check the connectivity of the device
 No need to have steady internet connection to work
 Close Project
 frontend ui
 This images shows the user interface of the Android mobile application.
 This is the 1st impression of user. When user run the app, appliication will start with this splash screen.
 This is the login screen UI. User need to enter credential which will be provided by seller for specific robot. Each robot has unique username and password.
 After user logged in, he/she can see the configuration fragment UI. This is the UI which user will use to configure the robot. As well as, it will display whether robot is connected with server.
 This UI will display the weather report. This report include data which was send by the robot. Currently robot implemented with temperature and humdity sensors only. Therefore, app will fetch only temperature & humidity.
 This UI includes the details of the purchesed robot. Data will be fetched from firebase database along with login. Each data are unique with the login credentials.
 When user clicks the logout in bottom navigation bar, this confirmation box will appear. User may choose yes or no to logout or keep stay login.
 Close
 Testing
 Tests that need to be done during the App building
 Testings were done during the development of the app to identify the problems with the mobile app. ( Connectivity, User Inputs etc… )
 Used test cases to check the expected values come as the output.
 Easier than manually testing each case over and over again.
 1. Subscribe the Broker
 Checked the subscription - connectivity between the Mobile app and the EC2 Broker.
 This is important because through the Broker only mobile app get the readings from the Robot.
 Though an IP is correct, invalid topic can be subscribed. But this won’t happen in the app because topics are assigned when user successfully login to the app ( Using correct ID and the Password).
 Subscription is tested using mosquitto broker(Representing Sensors of the Robot) also; using Temperature and Humidity values.
 2. Publish to Broker
 Checked the Publish - connectivity between the mobile app and the EC2 Broker
 It is important to check whether only the correct values are delivered to the Broker from the mobile app.
 Terminal is used as the Robot to check the received values are correct.
 3.	User Inputs
 Most critical place the Robot can go wrong is with User Inputs.
 Important to check whether correct values are input by the User to the app.
 4.	UI test with espresso
 Additional test to check the functionality of the login page using espresso with test cases.
 Summary report
 Close Project
 overview
 Overview of the cloud deployment,MQTT and Technologies.
 Overview
 Create AWS EC2 instance and install Mosquitto MQTT Broker
 Using EC2 IP address and port, mobile app and Robot can connect to the MQTT broker
 MQTT broker will handle the publications and subscriptions
 We created firebase database and entries for each Robot
 Using the database app will connect with the correct device
 Cloud Deployment
 Create EC2 instance, Ubuntu virtual machine
 Install mosquitto MQTT brokerr
 Used IP of VM to connect the application
 Setup security rules of EC2
 Give access only to port numbers 1883 and 22 (MQTT, SSH)
 MQTT Setup
 Used Mosquitto broker to handle mqtt clients (App,Device)
 Set Password protection to the broker
 Not anyone can publish or subscribe to broker and increase the traffic
 when we want to publish or subscribe we have to use the password
 Used unique topics for each device
 to identify whether Robot is connected or not
 Set MQTT Last Will message
 Publish QOS 2 messages
 Data send only one time.
 Highlevel Diagram
 Close
 Deploy in AWS
 Steps to follow deploy in AWS server.
 The Internet is composed two types of machines: a server or a client. A server provide services to you while the client request for the service. To ensure that our MQTT broker(the service) can be accessed using other computer or electronic devices anytime, we need to install the broker to a server machine that is always turned on and connected to the internet. To do this, we rent a virtual machine on AWS that functions like a computer.
 Go to AWS Management Console
 & Select EC2.
 Select Launch Instances
 Select the ubuntu server.
 Select “ Free tier Eligible” Option & Click “Next: Configure Instance Details”
 In “Configure Security Group” Tab add above rules using “Add Rule” option. Then Click “ Review and Launch”
 Click “Launch”
 “Create a new key pair” option & Give a “Key pair name” ,Then Download the key pair
 Newly Created instance is Running there in instances.
 Details of the instance. “Public IPv4 address” is used to connect with in the instance.
 Start, Stop or Connect the Instance can be done by write clicking on the instance.
 When select “Connect” this dialog box is displayed.
 Close
 database
 Overview about Firebase database
 Overview
 Firebase realtime NoSQL database.
 There is a one entry for every agribot device
 Used for Login process
 Using that data, app will assign unique client id
 Because of that, only one app can connect to a device at the time
 Using that data, app and device share data via unique topic
 Useful to isolate other devices data
 No write access for users
 Easy to retrieve and scale the database
 Close
 Deploy in AWS
 Steps to follow deploy in AWS server.
 Use this area to describe your project. Lorem ipsum dolor sit amet, consectetur adipisicing elit. Est blanditiis dolorem culpa incidunt minus dignissimos deserunt repellat aperiam quasi sunt officia expedita beatae cupiditate, maiores repudiandae, nostrum, reiciendis facere nemo!
 1. Go to AWS Management Console
 & Select EC2.
 Close Project",Autonomous Agricultural Robot,E16,Cyber-Physical Systems Projects,https://projects.ce.pdn.ac.lk/3yp/e16/agribot/,https://github.com/cepdnaclk/e16-3yp-agribot,https://cepdnaclk.github.io/e16-3yp-agribot,https://cepdnaclk.github.io/e16-3yp-agribot/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/3yp/E16/agribot/
61,automated railway ticketing system,"Automated Railway Ticketing System
 PROJECT BLOG
 About
 Design
 System Info
 Circuits
 Software
 Testing
 Demonstration
 Documents
 Benefits
 Budget
 Contact
 Automated Railway Ticketing System
 Travel Easy
 Get Started
 Problem
 In srilanka people waste there valuable time by spending in long queues in railway stations. Eventhough people do not get better experience when travelling. This problem effects badly on every aspects in day to day life of passengers.
 And another thing is using cardboard tickets for entrances increases environmental pollutions and keeping several officers for ticket issueing increase the government expenses.
 Solution
 Introducing fully automated system which consisting automatic gates and realtime web services with a one card for every travel.
 This reduces the time wastage as passengers have several entrances and no unefficient human involvements for ticketing. No environmental pollution any more as no tickets in the system. This will make the life easier.
 Solution Architecture
 Every passenger should enroll to the department and when he/she enroll will get a plastic card which has a key to his/her account. Passenger can recharge their account at the cashier or using their bank accounts. When he/she enters to the platform he/she only
 have to swipe the card and the starting point will be added to the database.
 Then they can seated anywhere he/she like Ticket checker will swipe their card again and class(1,2,3) will be added to the database. Finally passenger should swipe their card at the destination station. Then the cost will be deducted from their account.
 The gate at the destination will be closed until this procedure for each passenger.
 All money transactions and travelling history can be tracked and a database can be managed. All of the details related to passengers account can be managed by using app or website.
 Physical Design
 CAD Designs
 Design for handheld device
 Previous
 Next
 Design for Gate device
 Previous
 Next
 System Flow
 Initially a passenger should register to our system by purchasing a swipe card from the railway department. Then that person will be elligible to use our traveling system by using their card. Card owner can recherge their account on their own by using their bank accounts. Otherwise
 Payment methods are added to recharge from the counter at the railway station. When the person who travels, should swipe their card at the entrance gate. Then by checking theri account status gate will be activated.
 If there is no error and if there is no any illegal activity at his entrance, then he can enter to the train. When entering to the train his/her account will be holded
 until he/she leaves destination after swiping their card. In the train, ticket checker checks
 about the passenger class & will update the person's class(class 1, class 2). Any persons' default class will be ""class 3"".
 Then at the exit, passenger should swipe the card and system releases the holded accuont. At that time all the calculations will be happened and travel cost will be reduced from his/her account.
 and if the passenger don't swipe
 the card then their account will be transfered in to a freezed state. In the freeze state what happens is, he can't enter again from an entrance gate without paying his last travel cost.
 All these activities will be recorded in to a database. If a passenger wants to see their travel details, that person can register to our web application using his/her user id. That user id will be provided when that person
 initially register to our travelling system.
 Entity Relationship Diagram
 Circuit Diagrams
 Gate Circuit
 Handheld Device Circuit
 Previous
 Next
 Gate controlling Software
 This is the software used for the physical opertions of the system. This contain the software handlers at the gates(entrances and exits).
 This software system is also conneted with the central server and is running on the stations PCs mounted in each station. This is capable of all the
 functionalities needed.Entering, Exiting, Authenticating & Error checking are some such funtions.This is written in Java and some hardware functionalities
 are also displayed using the GUI.
 User Web application
 This web application can be used by both admins(officers incharged by railway department) and the users. Users can get smart facilities like
 account balance details checking, Rechgarge their account, travel details checking etc. Admins can do better performances using this web application. All the security actions
 are taken here to enhance the better performances. Front end consists of html, css, java script where use react.js and backend consists of java script
 where use node.js.
 Hands on experience
 To use as a user
 User ID: user001
 Password: user001
 To use as an admin
 User ID: admin001
 Password: admin001
 click here to tryout!
 Database Point of View
 Both Gate Controller Software & Web Application connected to a central server written in javascript where node.js, express is used. As the Database MongoDB Atlas cloud used
 due to security aspects and scalability. There are more things offering us by Mongo. Specially it takes less time to read/write to the database.
 Testing
 several main functionalities of the web application was tested.
 For the testing purpose we used unit testing on the middlewares and some integration testings.
 Testing gives the clear overview of outcomes of the functionalities.It is very helpful in debugging and diagnosing.
 so these are helpful to enhance the code. Doing these test with
 development saves the cost and enhance the security aspects.
 It automatically uplift the product quality and ensure the customer satisfaction.
 For testing the backend(Nodejs) of our server, we used Mocha, Jest and chai.They give the clear understanding about the details about
 the end points in out api. The main usage was using unit testing we were able to find the bugs in the authentication purpose of the admin and users.
 The full details about our testing results were given on the github repo of out project.
 #
 Test Type
 What Tested
 Importance
 Way of test done
 Results & Findings
 1
 Unit testing
 Middleware :
 Post method for user logging
 Correctness of the user name was tested
 Someone can enter any password and can enter to the system using several bruteforce checkings.
 Our system always gives security for the incorrect user names.
 Using mocha,chai,supertest
 Tools
 Incorrect user names were given as json objects and system gives correct outputs as we expected.
 What we expected was given and sometimes expectation and output was differ and using those tools able to identify the code statements needed to be modified.
 2
 Unit testing
 Middleware :
 Post method for user logging
 Correctness of the user password was tested
 Using bruteforcing someone can enter the correct password.
 System works corrently for the password issues.
 Using mocha,chai,supertest
 Tools
 Incorrect passwords were given as json objects and system gives correct outputs as we expected.
 Here password formats and other types were changed and tested and system worked correctly.
 What we expected was given and sometimes expectation and output was differ and using those tools able to identify the code statements needed to be modified.
 3
 Unit testing
 Middleware :
 Get method for admin authentication
 By changing admin usernames and passwords
 System gave authentication fails warning correctly
 Using mocha,chai,supertest
 Tools
 Incorrect passwords and admin names were given as json objects and system gives correct outputs as we expected.
 Here password formats and other types were changed and tested and system worked correctly.
 What we expected was given and sometimes expectation and output was differ and using those tools able to identify the code statements needed to be modified.
 4
 Unit testing
 Middleware :
 Get method for user details cheking after logged as an admin
 Giving different usernames try to get user details. Have to have authenticate the admin correctly otherwise this didn’t work. Have to have correct tokens.
 Try to acces user details without logging as admin system gives authentication fails warnings. Tried different tokens gave authentication fails.
 Expected was given and sometimes expectation and output was differ and using those tools able to identify the code statements needed to be modified.
 5
 Unit testing
 Middleware :
 Post method for user logging
 Check correctness of tokens
 System generate unique tokens and it is the one responsible for activities done on the server after logging. If someone can cheat then security fails.
 Change the tokens and try different aspects in the web and try do things inside the web
 Gives authentication fails warnings and some functionalities were accessed so were able to identify them.
 Some Test Results
 Full Demonstration of the System
 Download Documents from here👇
 Design Manual
 User Manual
 Benefits for the man kind
 Best passenger experiance
 User friendly Interactive App
 Save more Time to User
 More Security Provided
 Future Enhancements
 We hope to extend our payment methods far beyond.
 Introduce our cross platform mobile app
 Maximize performances with the help of Artificial Intelligence & Machine Learning
 For more details
 Visit our github project repository
 Visit Now
 Connect With Us
 Deshan L.A.C
 Madushan K.H.G.H
 Madushanka H.M.K
 deshanch9678@gmail.com
 hasindumadushan325@gmail.com
 kavindumadushanka972@gmail.com
 +94 71 1204836
 +94 76 4825922
 +94 77 2894172","The project is to replace currently available manual methods by an 'Automated Method'. To check the entrance & exit, an automatic gate & a swipe card has introduced. Class will be updated using introduced 'Handheld Device'. In this project we have introduced online banking facilities as well. we hope to develop this project further. To see all the activities, the 'User Portal' has introduced.    ",E16,Cyber-Physical Systems Projects,https://projects.ce.pdn.ac.lk/3yp/e16/automated-railway-ticketing-system/,https://github.com/cepdnaclk/e16-3yp-automated-railway-ticketing-system,https://cepdnaclk.github.io/e16-3yp-automated-railway-ticketing-system,https://cepdnaclk.github.io/e16-3yp-automated-railway-ticketing-system/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/3yp/E16/automated-railway-ticketing-system/
62,automatic fish tank control system,"Fish Tank Control System
 e16377@eng.pdn.ac.lk
 +94 77 88 75 74 7
 Fish Tank Controller System.
 Home
 Introduction
 Hardware
 Mobile App
 Software Testing
 Design
 Team
 Budget
 Contact
 Welcome to Fish Tank Controller System
 We are team of talanted Engineering Undergraduates making the world a better place
 Get Started
 Check our Mobile App
 Demonstration
 Connect
 Connect to your fish tanks remotely and control it
 Keep Track
 Keep a log about the fish varieties in the fish tanks
 Feed on time
 Feed them as you need.
 Clean It
 Renew the water when the tank get polluted
 Introduction
 What Are We Going To Do
 Many fish are dying in aquariums due to lack of food or polluted water. We found a solution for that.
 We have invented a Fish Tank Control System
 So you can control multiple fish tanks from anywhere in the world.
 Read the pH value and Temparature inside the fish tank.
 Through the sensors you can check the pH value and temparature inside the fish tank.
 Control your fish tanks from anywhere
 You can set the feeding times and how much mass you are gonna feed to the fish. And also you can
 log the fish types and count of each fish type using the mobile app.
 Depend on the water quality, the water renew system will work automatically.
 Planing 100%
 User Interface Designing95%
 Back End Developing 75%
 Hardware Implementation 20%
 Mobile App 30%
 Total Progress 55%
 5
 Milestones
 1
 Project
 2
 Semesters
 3
 Hard Workers
 Hardware
 Check our Hardware Technologies
 We are using several Hardware Technologies
 ESP32-WROOM
 GPIO pins + WIFI module inbuilt
 18 Analog-to-Digital Converter (ADC) channels
 16 PWM output channels
 Dual-core 32-bit LX6 microprocessor, up to 240 MHz
 448 KB ROM for booting and core functions
 520 KB SRAM for data and instructions
 4MB Flash
 Supply Voltage : 3.3V
 XH-M603 Charger module
 Input Voltage : DC 10-30V
 Can Control Input Voltage
 Inbuilt Display
 12V lead acid battery
 Very Durable
 Rugged Construction
 7.2AH Capacity
 Ph meter sen-0161
 pH signal conversion board + pH probe
 Supply Voltage : 3.3V~5.5V
 Accuracy : ±0.1 at 25℃
 Temperature Range : 5~60 ℃
 Response Time: < 1min
 Probe Life : 1 year
 DS18B20 Waterproof Sensor
 Communicate using 1 wire method
 Operating Voltage: 3V to 5V
 Temperature range : -55~125 ℃
 Programmable
 0.5 ℃ accuracy
 G1/2 plastic Solenoid valve
 Normally Closed
 Thread : G1/2”
 Rated Voltage : 12V
 Current 300mA
 Applicable fluid Temperature : 0~55 ℃
 Applicable Water pressure : 0.02-0.8MPa
 Assembly Design of Final Overview
 Front View
 This is the front view of our tank that we willing to gain at as final overview.
 Final Overview 1
 Feeding Controller
 This is the feeding controller design which will attach to the tank of user.
 Final Overview 2
 Sensor Panel
 This is the sensor panel that can easily paste to user tanks.
 Final Overview 3
 Controller PCB Designs
 Controller Circuit
 Controller circuit consists of four regelators that helps to manage the voltage levels required for the components .
 Controller Circuit layer 1
 Controller Circuit
 This contains the bottom layer of our controller circuit .
 Controller Circuit layer 2
 Power Circuit
 This power circuit contains a transformer and a rectifier,which helps to connect with our charger module and LED ACID battry
 Power
 Circuit layer 1
 Power Circuit
 This contains the bottom layer of our power circuit .
 Power
 Circuit layer 2
 Final Overview of
 Component Placement
 Overview of Component Placement
 This view shows the final hardware component placement in our device.The device consists of a plastic cover .
 Battry Display
 The Display indicates the volatge level of our led battry.
 Side overview of Device
 Package of Device
 Package that covered with nice wrapper.
 Hardware Designs
 Demonstration of Outputs using LED
 In here as we have limited hardware components we have demonstrated outputs using LED bulbs. There is a buzzer for alerts, water inlet, water outlet, feed motor
 Sensors
 Two sensors are immersed in water here for testing. Temperature is measured using DS18B20 waterproof temperature sensor. Also there is a PH sensor.
 Power supply
 Power is supplied after converted into 12V DC. Then through voltage regulaters.
 Testing on Hardware
 Many tests have been done for hardware.
 Hardware Tests
 Following tests have been done
 1.
 Authentication test (Integrated
 security test)
 This is important for the security of the app and user accounts. In Fish Tank Control System security is far more important as someone can kill all the fish in seconds
 2. Data Mapping testing
 Database testing helps in protecting the most important component of the app which is data. The correct Structure is very
 import for the whole process of renewing and feeding
 3. Stored Procedures(Black box testing)
 Perform an operation from the front end (UI) of the application and check for the execution of the stored procedure and its results.
 4.
 Device compatibility testing
 Users will use android and ios with different versions and screen size
 Mobile App was tested across various mobile devices to confirm its compatibility.
 5.
 Wi-Fi Connection test
 Microcontroller connects to the server using Wifi. Therefore we done few test to make sure the communication happened perfectly.
 Download the test report here!
 Special Features
 Whatsapp Integration
 We are sending you a message if anything is out of ordinary
 Using Whatsapp API we have developed the app to send you an error message
 Security is the most important thing!
 If the PH value goes wrong
 You will get an error message saying that PH is not in a good range with the tank number.
 If the Temparature value goes wrong
 You will get an error message saying that Temparature is not in a good range with the tank number.
 Mobile App
 We built an app compatable with both Android and ios
 We are using several software Technologies like flutter, python etc.
 Mobile App View
 Spread the word!
 Close
 Spread the word!
 Close
 Spread the word!
 Close
 Spread the word!
 Close
 Spread the word!
 Close
 Spread the word!
 Close
 Spread the word!
 Close
 Spread the word!
 Close
 Spread the word!
 Close
 Spread the word!
 Close
 Spread the word!
 Close
 Spread the word!
 Close
 We have done some testing testing
 These tests have done only for the software parts
 TEST NAME
 PURPOSE OF THE TESTING
 METHOD
 RESULTS
 CONCLUSION
 Signup route testing
 1.A user can enter duplicate or malicious email
 2.User can send wrong data types
 3.User can send a request with missing fields
 We used five python scripts with sign up requests
 1. Request with a wrong email address
 2. Request with a already signup email
 3.Request with wrong data types
 Ex number for the name
 4.Request without the email field
 5.Correct Request
 Case 1
 Response with
 Status code 422
 Case 2
 Response with status code 406
 Case 3
 Response with
 Status code 422
 Case 4
 Response with status code 422
 Case 5
 Response with
 Status code 200
 All test cases pass
 Login route testing
 1.User can enter wrong credentials
 2.User can send a request missing field
 3.A black hat user can try to sql injection
 4.Correct request
 5.User can send wrong data types
 We used four python scripts with login requests
 1.Request with wrong email
 2.Request with a wrong password
 3.Request that use password as “pass OR 'x'='x’”
 4.Request with correct credentials
 5.Request with a number as email
 Case 1
 Response with status code 401
 Case2
 Response with status code 401
 Case 3
 Response with status code 401
 Case 4
 Response with status code 200
 And user details
 Case 5
 Response with status code 422
 All test cases pass
 Control route
 1.Control route for feed particular tank fishes
 2.Control route for renew the water of particular tank
 We use dummy mqtt client to subscribe the tank id and looking for the mqtt message
 1.Send a feed
 Signal with valid token
 2.Send a feed signal with invalid token
 3.Send a feed signal with expired token
 4.Send a renew
 Signal with valid token
 5.Send a renew signal with invalid token
 6.Send a renew signal with expired token
 Case 1
 Response with status code 200
 Case  2
 Response with status code 401
 Case 3
 Response with status code 401
 Case 4
 Response with status code 200
 Case 5
 Response with status code 401
 Case 6
 Response with status code 401
 All test cases pass
 Tank routes
 1.Send data , tank to server
 1.Data with wrong tankid
 2.Data with correct request
 Case 1
 Response with
 s=0
 Response with status code 200
 All test cases pass
 Tank routes
 2.Get stable temperature of tank
 1.Send a request with tank id
 Case 1
 Response with a temperature
 Test Case pass
 Server reliability
 We need to check whether server can function under bulk of requests
 1.We use 3 python scripts that contain “for” loop with 1000 login requests
 Then we run that there python scripts at the same time
 After few time request got response code as 500 as server error
 Test case failed
 Solution :
 Earlier we used only one thread after this we increase it to 20 threads then we test again and test was successful
 Rate limit test
 We need to check rate limiting of our server
 .We use 4 python scripts that contain “for” loop with 1000 login requests
 Then we run that four python scripts at the same time
 Response with status code 200 also last request took some time to response
 Rate limiting functions properly
 App data view routes
 We check adding a new tank functionality
 1.Add tank request with wrong token
 2.Add tank request with expired token
 3.Add tank request with a not valid data type
 4. Add tank request with correct data and valid token
 Case 1
 Response with status code 401
 Case 2
 Response with status code 401
 Case 3
 Response with status code 422
 Case 4
 Response with status code 200
 All test cases passed
 Python Code Samples of  our Test Cases
 Test Results
 Swagger Ui Testing
 Also we use an inbuilt swagger uI tester that supplies by Fast API,to test and confirm our functionalities of our every router. This UI supplies an api caller for every route ,we have coded with a fast api object.Also it provides us with good documentation that contains required data type and requested responses.
 Some screenshots are given here
 Swagger UI is a good api testing platform with visual documentation that makes backend testing easier (Documentation for the link is given here https://swagger.io/tools/swagger-ui/)
 Also this comes as a inbuilt flatform in FastApi package(Details of fastapi with swagger https://fastapi.tiangolo.com/advanced/extending-openapi/#check-it)
 Unit Testing
 We did unit testing in front end code .In here we checked the graph viewing function that written in dart.First we make a dummy object of data that contains temperature and pH.Then we inject it in to our function and looking for the view showing in our mobile device
 Test Result: Visualize the graph correctly
 Time taken for the visualization is higher
 Modification:Increase the visualize time using flutter graph package
 Back end testing conclusions
 every end point validate the data
 Wrong data types give an error code and message
 No duplicate emails can create an account
 Login validation checking
 Every route checking through the swagger UI
 with 20 threads server gives superior efficiency
 ...
 Dr. Upul Jayasinghe
 Advisor
 Dr. Asitha Bandaranayake
 Advisor
 Dr. Suneth Namal Karunarathna
 Advisor
 Dr. Isuru Nawinne
 Advisor
 Dr. Ziyan Maraikar
 Advisor
 Dr. Upul Jayasinghe
 Advisor
 Dr. Asitha Bandaranayake
 Advisor
 Dr. Suneth Namal Karunarathna
 Advisor
 Dr. Isuru Nawinne
 Advisor
 Dr. Ziyan Maraikar
 Advisor
 Dr. Upul Jayasinghe
 Advisor
 ‹›
 Design
 Check our Designs
 This is a brief explanation about our design
 All
 Hardware
 Overall
 User Interface
 3D Design
 Drawn using tinkercad
 Circuit Diagram
 Designed using easyEDA
 NODE MCU ESP32 WROOM
 Pin Diagram (30pin version dev kit)
 Flow
 How nodes connected to each other
 UI Design
 User Interface of Login page
 Circuit Diagram
 Power Supply Circuit
 ER Diagram
 ER diagram of mongoDB
 Fighter Fish
 User Interface Design
 Team
 Our Hardworking Team
 Vindula I B S
 E/16/377
 Harshana P Y S
 E/16/127
 Samaraweera A A R V
 E/16/332
 Budget
 Estimated Budget According to the market price
 F.A.Q
 Frequently Asked Questions
 These are some of the freequently asked questions
 How much will this cost?
 Around 18000LKR. Roughly 97.5 USD.
 Is it easy to control?
 You just need to download the app. This will be the easiest way you ever tried. Trust Us!
 What happen if the power goes down?
 This has inbuilt backup power which is enough for 2-3 Days. So it will be very much useful in household
 fish tanks as well as larger scale aquariums, companies etc.
 Contact
 Contact Us
 Fell free to contact regarding any matter
 Address
 University of Peradeniya, Peradeniya
 Email Us
 e16377@eng.pdn.ac.lk
 Call Us
 +94 77 88 75 74 7
 Loading
 Your message has been sent. Thank you!
 Send Message
 Github Repo
 Visit our project Repository
 github repo
 Pre-Order
 Enter the E-mail. One of our team member will contact you
 University of Peradeniya.
 University of Peradeniya
 Peradeniya
 Sri Lanka
 Phone: +94 81 239 33 00
 Email: vc@pdn.ac.lk
 Web-site: http://www.pdn.ac.lk/
 Faculty of Engineering.
 Faculty of Engineering
 University of Peradeniya
 Peradeniya
 Sri Lanka
 Phone: +94 81 239 33 02
 Web-site: http://eng.pdn.ac.lk/
 Computer Engineering Department.
 Computer Engineering Department
 Faculty of Engineering
 University of Peradeniya
 Peradeniya
 Sri Lanka
 Phone: +94 81 239 39 14
 Web-site: http://www.ce.pdn.ac.lk/
 Designed by Group 4
 Home
 Introduction
 Hardware
 Mobile App
 Software Testing
 Design
 Team
 Budget
 Contact",This is a third-year project by a group of students. Here we are developing a fish tank controlling device with a web app.,E16,Cyber-Physical Systems Projects,https://projects.ce.pdn.ac.lk/3yp/e16/automatic-fish-tank-control-system/,https://github.com/cepdnaclk/e16-3yp-automatic-fish-tank-control-system,https://cepdnaclk.github.io/e16-3yp-automatic-fish-tank-control-system,https://cepdnaclk.github.io/e16-3yp-automatic-fish-tank-control-system/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/3yp/E16/automatic-fish-tank-control-system/
63,chessMATE,"chessMATE | e16-3yp-chessMATE
 e16-3yp-chessMATE
 chessMATE
 Smart Chess Platform
 Back to our Repository
 Group Members:
 Isurika Adikari
 : E/16/012 : e16012@eng.pdn.ac.lk
 Damsy De Silva
 : E/16/069 : e16069@eng.pdn.ac.lk
 Chaminie De Silva : E/16/070 : e16070@eng.pdn.ac.lk
 Table of Contents
 Problem
 Solution
 About Product
 Vision of the Product
 Product Overview
 High-Level Architecture
 3D Model of our Product
 User Interfaces for Mobile Application
 Cloud Architecture
 Mobile App Demonstrations
 PCB Designs
 Test Summary
 Test Results
 Mobile App Testing
 Server & Database Testing
 Embedded System Testing
 System Test
 Demonstration
 Budget of the Product
 Advising Lecturers
 Links
 Problem
 Chess is one of the most popular and oldest board games played by millions of people worldwide. But still there are some difficulties chess players face which limits them to enjoy this game to its fullest.
 When chess players have trouble finding competent opponents in their locality, they try online chess on a mobile or desktop application. We found out that most of the professional as well as casual chess players are more likely to play chess game on a physical chess board rather than on a mobile or desktop screen.
 Many chess players have stated that they have trouble in focusing and attacking aggressively during games played through mobile or desktop applications. And also, they have confessed that when playing using the physical chess board, they get to touch the pieces as they make a move, and this really draws them into the game.
 Solution
 Our solution is an IOT platform which will provide the grand usual chess board experience to whom that need online chess.
 About Product
 Vision of the Product:
 The vision of the chessMATE is to add a cool online chess game experience on everyday lives of people. Our endeavour is to give people more human experience with the new next generation technology.
 Product Overview:
 Our product consists of two main sections; an electrically powered chess board (Smart chess board) and a mobile app.
 In order to start a game, first you need to connect the board with the mobile app. Next you have to connect to an opponent who is registered on our platform via the mobile app. Then you can start the game.
 When you make a move on your chess board, that move is sent to the chess board and mobile app of the opponent and the path of the move is displayed along with the start and end squares on the chess board owned by the opponent using the LEDs on the board. Then the opponent is required to manually place the specific chess piece moved by you on the correct end square in order to continue the game.
 The main game mode we offer to our clients is the Board Vs Board game mode. Further the Board Vs App and App Vs App game modes can be experienced by the chessMATE clients.
 High-Level Architecture
 Given below diagram shows the high-level architecture of our solution.
 3D Model of our Product
 Given below is the 3D overview of our chess board.
 Following shows the 3D overview of the inner section of our chessboard. There are 64 compartments where each compartment being used by a square.
 User Interfaces for Mobile Application
 These are our currently implemented user interface designs.
 Cloud Architecture
 Mobile App Demonstrations
 Here we have demonstrated how the Sign-In and Login functionalities work for multiple clients.
 The following demonstration shows how a new game is started between two players and how the movements are being sent and received by the players successfully.
 PCB Designs
 PCB design for Main Unit
 PCB design for a compartment unit.
 Test Summary
 Test Results
 Mobile App Testing
 Server and Database Testing
 Client Connection Establishments
 Get all available online users
 Check multiple games between multiple pairs of players
 Client 1 initiates a new game with Client 2
 Client 3 initiates a new game with Client 4
 Moves are sent and received by the respective clients in the parallely conducted games without resulting any conflicts.
 Client 1 and Client 2
 Client 3 and Client 4
 Check database access and queries
 When a new player sign-in into the platform a new record will be created in the database
 When a player log into the platform his login information will be checked in the database
 Embedded System Testing
 LED Panel Test
 What is the test?
 Whether the opponent’s move is correctly shown on the led panel
 Why is it important?
 Ensures the correctness in indication of opponent’s move
 Establishment of connection (Bluetooth) between Mobile app and ESP32
 How was the test done?
 Algorithm Test
 Test 01
 Test 02
 System Testing
 Demonstration
 App vs App Game Demonstration
 Game Streaming Demostraion
 System Demonstration
 Part 01
 Part 02
 Budget of the Product
 Advising Lecturers
 Dr. Isuru Nawinne
 Dr. Ziyan Maraikar
 Links
 Department of Computer Engineering
 Faculty of Engineering
 University of Peradeniya","The project “chessMATE” provides the grand usual chess board experience through an online platform. Chess is one of the most popular and oldest board games and most chess players are more likely to play chess on a physical chessboard rather than on a mobile or desktop screen. Project chessMATE resolves the main issues faced by the online chess players by providing an electronically powered chessboard and mobile app so that distant players can connect and play on actual chessboards. Game live streaming option and chess tutorials are included in the mobile app. Board vs board, app vs board and app vs app are the available game modes in chessMATE.",E16,Cyber-Physical Systems Projects,https://projects.ce.pdn.ac.lk/3yp/e16/chessMATE/,https://github.com/cepdnaclk/e16-3yp-chessMATE,https://cepdnaclk.github.io/e16-3yp-chessMATE,https://cepdnaclk.github.io/e16-3yp-chessMATE/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/3yp/E16/chessMATE/
64,computerized timetabling and attendance marking system,"Attendance Marking System
 Home
 Solution
 Services
 Design
 website
 app
 hardware device
 Cloud Deployment
 Testing
 Software Testing
 Hardware Testing
 Demonstration
 Budget
 Future Developments
 Team
 Contact
 Computerized Timetabling And Attendance
 Marking System
 In this project,our goal is to develop a
 modern attendance marking system that is suitable for
 today's world.In parallel with the attendance marking system,a fully-fledged time table
 managing
 and lecture reminding system is developed.
 Read
 More
 How things work?
 On the registration day of the students,fingerprints and student details are taken and added to the
 database using the website.A RFID token is registered for a student and given to them.Student can do
 the registration via the website or mobile app.Before a particular class starts,corresponding
 student details and fingerprint details are loaded into the sd card and fingerprint device with the
 help of nodemcu & backend server for student identification purposes.when a particular student put
 their attendance using fingerprint device and RFID sensor student details are displayed in the LCD
 display.At the same time,new attendance data is sent to the backend.All timetable data is added to
 the database and lecture reminding is automatically done by the server.Student and teachers are able
 to analyze and visualize attendance data via the website or mobile app.
 Services
 Attendance marking System
 Identifying the students in a lecture will be done by a hardware device which is in the
 lecture rooms by fingerprints and rfid cards.Students can see they were identified correctly
 in hardware device itself by seeing his information in the lcd screen.During the lecture
 time attendance details will be sent to the server and attendance will be updated.
 Lecture Notifications
 Before a lecture of a particular course students who had registered to that course will get
 notifications regarding lecture,lecture time,lecture duration and lecture room
 Student Registration
 Student will registered to the system via the website.This process will done by an
 admin.Student details including fingerprint will be taken into the system.
 Course Registration
 Beginning of the each semester registered students can do the course registration via the
 website or the app
 Automated Timetable
 Admin will initially enter the time table details (date,time,course,lecture room,lecturer) to
 the system via website.After that before each lecture students will get notifications via
 the mobile app
 Attendance Review
 Students can see their attendance percentage,daily attendance details via the mobile
 app.Lecturers can see their students' attendance for lab sessions and lecture sessions via
 the website.
 UI Design - Website
 UI Design - App
 Final Hardware Device
 Power On
 Power Off
 Keypad
 Setting the course code
 Selection Menu
 Selecting modes such as course registration,attendance marking for the
 device
 RFID sensing area
 Fingerprint Scanner
 LCD display
 Displaying student details after making the attendance
 Hardware Components Used
 1. ATmega328P microcontroller
 Operating Voltage: 1.8V-5.5V
 Flash Program Memory: 32 kbytes
 EEPROM Data Memory: 1 kbytes
 I/O Pins: 23
 SPI and I²C Master and Slave Support
 USART Support
 External Oscillator: up to 20MHz
 2. ESP8266 microcontroller
 Operating Voltage: 3.3V
 Flash Memory: 4 MB
 EEPROM Data Memory: 1 kbytes
 Digital I/O Pins : 16
 SPI and I²C Support
 USART Support
 PCB Antenna
 Integrated TCP/IP Protocol Stack
 External Oscillator: up to 80 MHz
 3. R307 Fingerprint Module
 Interfacing
 UART -> for microcontrollers/ARM processors
 USB 2.0 ->for computers
 Software serial -> to use other digital pins
 Reliability
 Flash memory for saving fingerprint templates -> Avoids data loss in
 power failures
 Power Usage
 Operating on 3.3v or 5v
 Sensing circuitry consumes 5µA(very low)
 4. RC522 RFID Module
 Operating voltage: 2.5V to 3.3V
 Communication : SPI, I2C protocol, UART
 Maximum Data Rate: 10Mbps
 Maximum Read Range: 5cm
 Current Consumption: 13-26mA
 5. Keypad & Port Expander(PCF8574P)
 4x4 keypad -> 8 pins(wastage of pins)
 Port expander
 Pin usage reduces to 2 pins
 Uses I2C protocol
 6. 16x2 LCD Dispaly & Port Expander(PCF8574P)
 LCD module - > 11 pins(wastage of pins)
 Port expander
 Spi -> i2c Conversion
 Pin usage reduces to 2 pins/li>
 7. Rechargeable battery(Li-Po)
 Output voltage - > 12v
 Capacity - > 2500mAh
 Circuit Diagrams
 PCB design
 Protocols
 Software Testing
 Integration Testing
 Integration testing is a level of software testing
 where individual units / components are combined and
 tested as a group. The purpose of this level of testing is to expose faults in the interaction
 between
 integrated units. Test drivers and test stubs are used to assist in Integration Testing.
 Spring Boot Application Architecture
 Spring Boot Application has a 3 Tier Architecture with
 Controller, Service and Persistence Layer.
 When we talk about integration testing for a spring boot application, it is all about running an
 application in ApplicationContext and run tests. Spring Framework does have a dedicated test module
 for integration testing. It is known as spring-test. If we are using spring-boot, then we need to
 use spring-boot-starter-test which will internally use spring-test and other dependent libraries.
 Example :- Adding a Lecture room
 Results
 Unit Testing - API
 UNIT TESTING, also known as COMPONENT TESTING, is
 a level of
 software testing where individual units / components of a software are tested. The purpose is to
 validate that
 each unit of the software performs as designed.
 Example :- Lecture Attendance Percentage Update
 Results
 UI Testing - Mobile App
 In     UI TESTING, test runs on a device or an emulator.
 In the background, your app will be installed and then a testing app will
 also be installed which will control your app, lunching it and running UI
 tests as needed
 Example :- User Login
 First Student will enter the username .
 Then he/she will close the keyboard.Next, student will enter the password.
 After that, he/she will close the keyboard again.
 Finally, student will preform click button.
 Programme
 Result
 After Running the Test, Mobile app's Login UI will redirect to corresponding user's home page.
 Performance Testing - Website
 website performance, was measured using
 Web.dev
 site.Performance details are as follows.
 Results
 Summary Report-Software Testing
 Feature / Unit
 Functionality
 Tools
 Results
 1
 Backend Validations
 Registration number and Email were tested for proper pattern with different inputs
 JUnit
 Given Registration number and Email should have proper pattern for user registration
 2
 Frontend Validations
 Frontend forms were tested with different set of inputs.
 Jest and Enzyme
 Forms can not submit without proper inputs
 3
 Joins in the database
 After Course registration of a student, Course and Student and Attendance Collection
 details were tested.
 Course and Lecturer collections was tested
 Course and TimeTable collections were tested
 JUnit
 Attendance details were join correctly with Course and the Student
 Lecturer collection joins correctly with course collection.
 Course collection joins correctly with Timetable collection
 4
 Lecture Attendance
 Tested whether correct present days,absent days and attendance percentages of student is
 calculated after adding attendance of a lecture by hardware device
 JUnit
 Attendance details are correctly calculated and stored in the database.
 5
 UI Testing in Mobile App
 Mobile App UI s were tested with different user inputs.
 Button clicks were checked with different pressed times to see output
 Expresso
 Forms can not submit without proper inputs
 Only registered students can log in
 No matter which time duration button was pressed.It gives the correct function after button was released.
 6
 Performance Testing in website
 Test the performance of the website by metrics such as
 first paint and time to interactive to determine lag.
 Test the HTTPS usage to correct image aspect ratios.
 Check for best practices to ensure site is discoverable.
 Check for common issues that may prevent users from accessing website content.
 Web.dev
 performance is quite low the following reasons may be possible
 Usage of unencoded images
 Bad aspect ratios of images
 Bad API response time
 Accessibility and Best practices are at a good level
 SEO is very high because website is hosted in AWS and AWS is take care of SEO
 Hardware Testing
 Unit Testing
 Every Component (Fingerprint,RFID module,SD card module,
 ...) should be tested for proper functionality.
 Sensor
 What will be Tested
 Importance
 1
 RFID sensor
 Test the results according to the environment in which the tag is being read.
 Test the distance range and the orientation of the tag.
 Accuracy of identifying the tags should be high
 2
 Fingerprint sensor
 Ability to scan a fingerprint.
 Test the tolerance level of the sensor.
 Accuracy of identifying the fingerprints should be high
 3
 Push Buttons
 Debouncing of push buttons.
 To improve UX with debouncing of push buttons
 4
 LCD Display
 Test the screen clearness of the LCD display under different light conditions
 LCD display screen must be clear to use the hardware device.
 Letters and numbers must be clearly visible
 5
 SD Card module
 Test the ability to create files in unique names
 Test the ability to write and delete files
 Test the ability to delete files
 To properly handle the connection failure or power failure conditions
 Testing Power Supplies
 After soldering testing power supplies one by one
 Main power input(Battery)
 check ripple voltages (using oscillator)
 Is noise acceptable
 Micro Controller circuit
 checking connectivity (using Oscilloscope or Logic probe)
 check the basic functioning
 Sensors
 checking connectivity
 check the basic functioning
 Cloud Deployment
 Demonstration
 Website
 Mobile App
 Hardware Device - Part 01
 Hardware Device - Part 02
 Budget
 Future Developments & Plans
 develop mobile app for ios platform
 Introduce the product into ,
 Universities
 Post graduate institutes
 Technical schools
 Team
 We are 3rd year Computer Engineering Students of University of Peradeniya
 Eranadana Wijerathna
 E/16/399
 Computer Engineering Undergraduate
 Saubhagya Munasinghe
 E/16/242
 Computer Engineering Undergraduate
 Nuwan Piyarathna
 E/16/286
 Computer Engineering Undergraduate
 Visit Our Github Repository
 Visit Now
 Contacts
 Nuwan Harsha
 nuwan.harshamatrix@gmail.com
 Saubhagya Munasinghe
 sm201211d@gmail.com
 Erandana Wijerathna
 erandanawijerathna@gmail.com
 Home
 Solution
 Services
 Design
 website
 app
 hardware device
 Cloud Deployment
 Testing
 Software Testing
 Hardware Testing
 Demonstration
 Budget
 Future Developments
 Team
 Contact","This project is about creating a modern attendance marking system that is suitable for today's world. In parallel with the attendance marking system, a fully-fledged timetable managing and lecture reminding system is developed. The system consists of a website, hardware device, and mobile app. The website is for the students, lecturers, and administration. Key features of the website are timetable management, attendance, course registration, and registration.  The hardware device is used to mark the attendance with fingerprint and RFID cards and setting the lecture room. The mobile app is used by the students to see lecture reminding and attendance marking notifications, timetable, and attendance.",E16,Cyber-Physical Systems Projects,https://projects.ce.pdn.ac.lk/3yp/e16/computerized-timetabling-and-attendance-marking-system/,https://github.com/cepdnaclk/e16-3yp-computerized-timetabling-and-attendance-marking-system,https://cepdnaclk.github.io/e16-3yp-computerized-timetabling-and-attendance-marking-system,https://cepdnaclk.github.io/e16-3yp-computerized-timetabling-and-attendance-marking-system/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/3yp/E16/computerized-timetabling-and-attendance-marking-system/
65,digital signage based user targeted advertising,"Digital Signage Based User Targeted Advertising
 PROJECT BLOG
 About
 Design
 Progress
 3Dmodels
 Services
 Places
 Contact Us
 Digital signage based user targeted advertising
 A novel approach...
 Find Out More
 Problem and Solution
 Most of the advertisements displayed in the digital screens are not relevant for the targeted audience. So,ninety percent of the advertisements displayed using the currently existing digital signages are not very effective. Also the provided digital signage solutions are expensive to afford for most of the SMEs(Small and Medium Enterprises).
 To overcome these kind of issues our proposed solution consists of an upgraded version with additional hardware/software which can do User Targeted Advertising. Here our aim is while showing content that changes dynamically, when a particular crowd (male/female/children) is identified in front of the screens it will change the current content flow and show some relevant,interesting advertisement for the targeted audience that might catch their attention.
 The buyer(shop owner) of the digital signage unit can upload their advertisements using the provided user authenticated web application or the mobile application. Also add some specific advertisements categorized by gender and age group. When a user in front is detected specific content will be visible to the targeted audience.
 Watch Product Video
 Design Architecture
 Design Technologies
 The digital signage control unit consists of raspberry pi 3 which acts as the heart of the system.
 The digital display is connected to raspberry pi using HDMI and the user detecting and identifying unit is attached to the digital display. This user detecting and identifying unit mainly consists of a Raspberry Pi camera module
 Firebase is used as the back-end and cloud firestore NoSQL database to store,sync and query data.
 Flutter is used as the mobile app framework for the front-end development. Dart is used as the programming language where the same code base can be used to develop both android and ios applications.
 As an additional improvement, a smart power supply unit which can control the digital screen on/off through the mobile application will be implemented.
 ER Model - Database Structure
 Main Entities Of The Database are ADMINISTRATION, CUSTOMER(Shop owner who is willing to buy signage unit), ADVERTISEMENT and WORK_TIME. Main relationships are REGISTERS, UPLOADS and SCHEDULES.
 ADVERTISEMENT entity has a unique id as the primary key. Advertisements can be in three types.
 -Image,
 Ticker(Text),
 Video
 CUSTOMER table has a composite key, Customer_id along with Device_id. Having a composite key here is useful when same customer is buying more than one signage unit. Here,Device_id refers to the MAC address of the NIC of digital signage controlling unit.
 1:n relationship exists between ADMINISTRATION and CUMSTOMER, beacause a customer only has one ADMINISTRATION and ADMINISTRATION has many customers. And we can see total participation in both sides because everyone should be registered to use the system.
 As digital screen can be turned on and off only within the working hours because of our smart power supply unit, customer can schedule the time slots for each day. That data is stored in WORKING_TIME entity. It is a weak entity that does not have a primary key. Here, we have got a weak key 'Day' and same key can be repeated.
 Simplified Circuit Diagrams
 Components Needed
 Digital Signage Controlling Unit
 Raspberry Pi - $ 35
 RPi Power Supply - $ 9
 SD card - $ 10
 Cooling Fan - $ 4
 HeatSink - $ 1
 LED Push Button - $ 4
 Cover - $ 7
 User Identifying and Detecting Unit
 Raspberry Pi - $ 35
 RPi Power Supply - $ 9
 SD card - $ 10
 Cooling Fan - $ 4
 HeatSink - $ 1
 Camera Module - $ 10
 LED Push Button - $ 4
 Cables - $ 5
 LEDs, Resistors(220Ω) - $ 0.5
 Cover - $ 7
 Smart Power Supply Unit
 NodeMCU esp8266 - $ 5
 Relay Module - $ 3
 Rocker Switch - $ 5
 Plug socket & Top - $ 5
 Cables - $ 3
 LED, Diodes, Resistors(100kΩ) - $ 1
 Cover - $ 7
 Flow Chart
 Data Flow
 User is supposed to upload the advertisements using the mobile app.
 Those advertisements are uploaded into the selected google slides page according to age and gender. Default advertisement lists are uploaded to the RPi SD card.
 Image of the person who is infront of the screen is captured by the camera module and processed in the RPi to extract the face by running haar-cascade classifiers
 That image is processed with pre trained deep learning model to predict the gender and age group of the person.
 The age,gender details are updated in firestore database in realtime and signage control unit listens and query these new data.
 According to the received results, matching google slides presentation is selected and displayed on the digital screen.
 Software Architecture
 Data Flow
 User information and shop details are stored in Firebase(in Cloud Firestore database).
 Users can upload advertisements using the mobile app after login into their account.
 Advertisements are uploaded as google slides which are stored inside google cloud. Because of using google slides, we could minimize the cloud storage of our system. And users can easily create and edit advertisements using google slides.
 Detected persons age and gender prediction values are realtime updated in firebase firestore.
 Screenly API (in RPi) fetches the matching(to age and gender) google slides presentation link and display the advertisements in the screen.
 User Detecting & Analysing Unit
 Real Implementation and Design
 This unit mainly consists of a RaspberryPi board and a camera module.
 Image of people in front of the screen
 is processed inside the RPI for predicting the gender and the age.
 Haar Cascade classifier is used to detect faces from the captured images.
 To obtain results, OpenCV libraries are used with a trained cnn model.
 Predicted age and gender information is updated real-time in firebase.
 Trained Data Model
 Age and gender detection in our system is mainly done with Convolutional Neural Networks.
 We have used the CNN models trained by Gil Levi and Tal Hassner (two Israel researchers) using caffe framework and we have used the OpenCV’s dnn package which stands for “Deep Neural Networks”.
 To detect faces from images,webcam and to import neural network trained models, opencv libraries such as haarcascade,dnn packages are used in our implementation for predicting gender and age.
 CNN consists of 8 values for 8 age classes (“0–2”, “4–6”, “8–13”, “15–20”, “25–32”, “38–43”, “48–53” and “60-100”) and two values for two gender clases (""Male"" and ""Female."")
 As you can see in these pictures, this model accurately predict the gender and the age group.
 Link for Github Files
 Test To Check Accuracy When Multiple Faces Appears
 Test
 Check if image predicted correctly when there is more than one face appearing.
 Test Type
 Unit Testing(We did Black Box Testing)
 Tool
 Unit Test Library In Python
 Results
 Smart Power Supply Unit
 Overall Design
 Components and Procedure
 MQTT messaging protocol is used to design the smart power supply unit.
 Mobile app (MQTT client) publishes user given messages (ON or OFF) to a unique topic for that power supply unit.
 Mac address of the Node MCU is used as the topic to make the topic unique for that power supply unit.
 Node MCU(ESP8266) subscribes to the certain topic and recieves user given message through the broker.
 EMQX broker is used in our implementation.
 After recieving the user given message, a signal is given to 5V relay module by the NodeMCU. According to the message,the relay triggers and screen will turn on or turn off.
 Hardware Design
 Demonstration Video
 Safety Factors
 5A fuse is included in the plug to protect the relay module from high currents.
 Heat sink is added to protect the device if it becomes overheated.
 Earthing is used here to protect the user from an electric shock.
 Special Concerns
 If a wifi failure happens, you can disconnect the smart power supply unit by the switch and switch on or off manually.
 If there is a power failure, you don’t have to worry. It will execute the last message given by the user.
 Using WifiManager library, ESP8266 can be connected to Wifi router without hardcoding router SSID and password in the code.
 Digital Signage Controlling Unit
 Actual Implementation & Design
 Demonstration Video
 This unit mainly consists of a Raspberry Pi Board.
 SD card is inserted into the Raspberry Pi board.
 Screenly OSE is installed in RPI.
 Screenly API fetches the correct google slide according to the gender and age of the person.
 When
 more than one person is appeared, generic advertisements are displayed.
 App Frontend Design...
 1. Login/Authentication
 2. Dashboard/Drawer Bar
 3. Signage Control/Add assets
 4. Create/Edit Advertisements
 5. Power Supply Control
 6. Customize Profile
 App Features and Functionality
 1. Login/Authentication
 Two factor authentication for login
 First login through PhoneAuth then Google Sign-In
 Provide personal and shop details for initial profile setup
 Shared preferences are used to save credentials after inital login for easy access to app
 2. Dashboard/Drawer Bar
 Drawer bar consists of Dashboard, User Targeted Signage, Smart Power Supply and Profile pages
 Dashboard contains current playlist previews of the actual digital screen
 Customer analytics are included for strategies
 3. Signage Control/Add assets
 Choose Signage device to control from the drop down menu and rename features included
 Select age and gender category to select a specific advertisement(asset) list
 Add assets and Watch Previews
 User targeting turn on/off as per the users choice
 4. Create/Edit Advertisements
 Create,Edit assets through google slides by accessing from add assets button
 Latest advertisment updates are realtime synced and displayed
 Preview advertisment playlists anytime inside the app through google slides
 5. Power Supply Control
 Add multiple power supply units and rename feature to enhance user friendliness.
 Choose a power supply to control from drop down menu
 Digital screen on/off button for easy control of the power supply
 6. Customize Profile
 Maintain user profile and preview details
 Customize details as per the users choice
 Log in through different account by logging out of current account.
 Unit testing for app and backend
 Results and Findings
 cloud firestore mocks and flutter test dependencies are used for several testing in both app and the backend firebase
 User entry validations performed to provide a better user experience. Invalid user inputs captured and informed properly. Added mac addresses by users are realtime synced to the menus and testing done for device adding and deleting procedures in app.
 Firestore is used as the backend databse and collections are used to organize data structure of users and devices.
 Mac addresses are used for document topics in firestore to identify each device. Testing perfomed with this structure for correct retrieval and deletion of devices through app by the user.
 Results from tests optimized the structure of the firestore for categorizing users and also providing analytics by realtime counts of people captured for user targeting in each gender category.
 Test Summary
 Digital Signage Control Unit Designs...
 Outer view
 Inner components
 Inner design
 Outer back view
 Top view
 Vertical view
 User Detecting & Identifying Unit Designs...
 Outer view
 Inner components
 Inner Design
 Stand front view
 Stand back view
 Stand design
 Smart Power Supply Unit Designs...
 Outer view
 Inner components view
 Inner design
 Outer back view
 Cover and Box
 Outer/Inner view
 Design Decisions for Fabrications
 Digital Signage Control Unit
 Maximum space utilization within the box
 Holes to get air into the fan are designed such that small objects are not get into the box
 User Detecting and Identifying Unit
 The length between jaws can be easily adjusted according to the table
 Adjustable hand to change the angle of the camera
 Fits properly with the RPi cover
 Smart Power Supply Unit
 Compact structural design (box dimensions - 17.5cm x 8cm x 4cm)
 All components screwed for solidity and easy replacement.
 Air vents added to optimize airflow
 Clearance space for connecting wires and cables
 What we provide...
 Dynamic real-time content changes
 upload content to display anytime
 Easy set up and User friendly
 register with an account to discover features
 Upload advertisements through mobile app
 display promotions,sales,new arrivals
 Shows relevant content to audience
 detects person in front and show interesting,eye catching ads
 Where we can see digital signage...
 Shopping malls
 Outdoor
 Restaurants
 Supermarkets
 Clothing-retail
 Airports
 Find more details from our github project repository
 Visit now!
 Get In Touch!
 Feel free to visit our github accounts or email us for further details!
 Viraj Dhanushka
 Sumudu Lakmali
 Hans Thisanke
 smviraj@gmail.com
 sumuduliyanage888@gmail.com
 hansthisanke@gmail.com
 End of page",,E16,Cyber-Physical Systems Projects,https://projects.ce.pdn.ac.lk/3yp/e16/digital-signage-based-user-targeted-advertising/,https://github.com/cepdnaclk/e16-3yp-digital-signage-based-user-targeted-advertising,https://cepdnaclk.github.io/e16-3yp-digital-signage-based-user-targeted-advertising,https://cepdnaclk.github.io/e16-3yp-digital-signage-based-user-targeted-advertising/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/3yp/E16/digital-signage-based-user-targeted-advertising/
66,full body motion tracking system,"Full Body Motion Tracking System
 Home
 Design(current)
 Deployment
 Testing
 Budget
 Team
 Game Console
 The game console is a collection of sensors and wireless modules that can track the motion of arms, legs, and body.
 The collected data use as input for a multiplayer first-person shooter game.
 The smartphone is used to display the graphics and play the sound of the game.
 Motivation
 People spend more time on online gaming and they do not get enough exercises for their body.
 In urban places not enough space for exercising.
 VR gaming devices give solution for both problems.
 The game console market was valued at 34.27 billion USD in 2019**.
 The game consoles are influenced by new technologies like VR, AR, and voice recognition.
 Consumers are increasing continuosly.
 Related Links
 Department of Computer Engineering
 Faculty of Engineering
 Univerity of Peradeniya
 github
 © Copyright 2020 Full Body Motion Tracking System",This is a online multiplayer first person shooter game that combines traditional multiplayer shooter gaming experience with virtual reality to create an ultimate immersive gaming experience.,E16,Cyber-Physical Systems Projects,https://projects.ce.pdn.ac.lk/3yp/e16/full-body-motion-tracking-system/,https://github.com/cepdnaclk/e16-3yp-full-body-motion-tracking-system,https://cepdnaclk.github.io/e16-3yp-full-body-motion-tracking-system,https://cepdnaclk.github.io/e16-3yp-full-body-motion-tracking-system/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/3yp/E16/full-body-motion-tracking-system/
67,gas level indicator and leakage detector,"GAS MATE
 University of Peradeniya 3rd year Students Project
 Goto
 HOME
 Description
 Team
 SERVICES
 CONTACT
 Further
 Presentation
 Blog Post
 GitHub Repo
 More
 Gas Level Indicator and Leakage Detector
 Group 18
 PROCEED
 THE REAL LIFE PROBLEM & SOLUTION
 Gas tank users face difficulties because of not having a precise way to
 find the remaining gas amount. To over come those problems with some protection. We are introducing gas level indicator and leakage detector.Gas Level Indicator and Leakage Detector is a system aimed on LP gas tank users and distributors, consisting 3 main areas
 Gas level indicator
 The device display and the mobile/web application will be updated in real-time, displaying the current existing gas percentage in the tank.
 A delivery system for gas renewal
 When a gas tank is finished, the user will be notified and will be asked whether they need delivery of a gas tank or not. According to their selection, a new gas tank will be delivered to the user.
 Gas leakage detector
 If a gas leakage happens around the gas tank, a gas sensor will detect the leakage. As soon as the leakage is detected, a buzzer will alarm the user about it while sending a notification to the mobile/web application.
 Contents
 01. Solution Overview
 02. Technologies
 03. Hardwares Required
 04. Circuit Diagram
 05. UI Design
 06. Container Diagram
 07. Bill of Materials
 08. Unit and Integration Testing
 09. Embeded System Testing
 10. Unit and Integration Testing Summary Report
 11. Security Report
 12. Embeded System Testing
 13. 3D designs of the Product
 14. Demonstration
 15. Team Profiles
 16. Services
 17. Contact
 18. Advisers
 Solution Overview
 In order to answer the above mentioned problems. As a solution we are introducing a hardware module able to detect the level of gas amount and interpret to a user using a display. Also for security measures the device have the capability to notify any imminent danger if there is a gas leakage happend.These are the services clients could obtain from the hardware end.
 Also in order to available data about the tank usage, current amounts and the status of the tank if there is a leakage in device. The user had to reach the hardware always. As a solution for that difficulty using a mobile application will be the optimal solution for the client. Therefore We provide you user friendly mobile application for the ease of usage.
 In order to connect these mobile application and the hardware module, there is a requirement of a backend server. Therefore cloud based backend server is used because of the neccessity of the current situation. Otherwise using a physical server will be finantailly disadvantagious. The choice over physical server will be discussed further under technology section as well.
 Technologies
 As the front end development we use flutter as our front end development software in this development process the expectations are to provide user friendly and interesting interface with enhanced UI and UX experience to the client.Other than that cross platform support is required for us to maintain the target clients by giving them access to our services independant their mobile os type.
 In the backend server requires to handle a larger amount of requests. Therefore the scalability and sustainability if uncaught error exceptions happend are critical points under choosing a optimum backend server. For those requirements, Node js would be the best solution for this project. Therefore the backend server side requests managements are done by the NodeJs.
 For storing data puroposes' requirements were the structured schema with the same type of data that have the ability to contain all the details of the target market when the product able to cover the whole market itself. Also the data base should be able to scalable from a smaller database. For these requirements are fulfil when the chosen selection was Mysql. Because the mysql have the ability contain 1000 different tables and all our reqiurements are fulfilled under 1000 tables. Also a table can record 21 million records with the space under 1GB. The maximum row table will be our user table but it will not exceed 21 million because the targeted ordience is less. Overall the total storage space if the project was a success as purposed the overall database will not be exceed 2GB of storage from the web server space.
 In the deployment it is required to consider the following facts. Those facts are what will be the platform the deployment will be happend. There are two suggestions one is assembling a physical server and the other one is the deploy the project on cloud as well. Due to finantial issues met in the process of developing the physical server of maintaining cost. The team sole decision and the academic guidance as well based on cloud services. Therefore the cloud server deployment was decieded. In that case the EC2 instance of AWS server support is selected. Under that there was free tier eligible service which will be provided 8Gb amount of virtual PC for the establishment of the project.
 Hardware
 As see on the image left, ESP86266 module is gonna use as the main controller and the connection establisher of the device. Since building connection between the cloud and the device is a requirement convensional arduino and a wifi module is not a convinient solution. That was the main reason behind selection of the Node MCU module rather using the wifi module with a arduino device.
 Using a li-ion power supply is for the usage of the battery even there is a power shortage if there is a leackage with a power shortage as a contengency plan. But the device can be maily powered by using a power supply from the location it self the cable requirements are all handled by the device it self.
 Gas sensor is there for detect any gas releases from the tank to the premises from the tank. For those requirements Using an implimented gas sensor for detecting natural gas is convenient.
 For making weight measurements for the to the system using a weight sensor with strain gauge technology is making the weight measurements more reliable because this method is sensitive to smaller weights changes also.It is very useful for keeping track of the gas usage.
 Pieso Buzzer is for when there is an identified threat imminent. For notify the close people to take necessary actions. Before been late and loosing control of an emegency situation. The main emergency the device will notify you if there is a leackage there and a short term notification using the buffer if it is not connected to the main power source and further the device is low on battery.
 LCD screen is using to graphically interpret the current state of the device such the remaining amount of gas in the device. The user interface for adding a tank and removing tank
 Circuit Diagram
 This circuit diagram developed using fritzing. This implementaion is describe how the hardware components are connceted to each other as well. The support for simulating the device quite complex therefore please consider the overall operation of the device may contain bugs. Since all the units of the system when developing should be tested.
 User Interface
 The following video shows the current progress on the software mobile application implemented using flutter and basic functions of the application is at an final staged but the user experience and the user interfaces are need to be improved further.
 Your browser does not support the video tag.
 Container
 The data flow overview is displayed as follows.
 Bill of Materials
 The bill of materials can be displayed as follows
 Testing
 In Testing the following tests are done.
 Up to the third milestone The testings are done according to the following. Methods of testing. The first testings are done under unit testing. In unit testing, the backend and the front end separate in to smaller units. In this scenario the units were login,sign in, add new device, add new tank such single processes independantly.
 Following tests are done due to following reasons.
 Checking multiple users access the same device - Since the devices are only registered per customer. Therefore the access only grants per customer
 Session Expiration - After Signed in the user initialize a session. once the session time expired the user have to log in again.
 Session Maintaining - The session time defines here.
 Check the input device an existing device - In order to function the mobile application the device should be a valid device. Therefore the validation testing is done under this testing.
 Validation Testing on the front end.
 Embedded System Testing Plan
 The Embedded System Testing Plan can be displayed as follows. In this scenario our plan was to test the embedded system according to the given testing plan. In this testing procedure followings are tested.
 Firmware Testing
 There may be containg erronous values in the firm ware may harms the itself. Also avoiding overflows, detect and avoid typos is required before uploading the codes in to the flashes.
 Board bring up related testing.
 In board testing we were looking at the soldering issues. Power failures due to loss of connection issues in linking each components to each other. Checking section by section in the hardware designs. Each node may not containing any communication to other nodes. Check weather where the overheated surfaces may occure while soldering.
 Sensor indicating and hardware protocol testing.
 Ensure the identified any malfunctioned components. Ensure the protocol functionality.
 Pressure Testing
 Hardware Structure stability are tested in this. And Find the rigidness of the failures.
 Hardware Designs
 Complete User Interface Design
 Inside View of the User Interface design
 Final View of the product with the gas Tank
 Caster Wheel Design
 Interior design of the product including weight sensor and other components
 Demonstration Plan
 Mobile Application Demonstration video Link
 Your browser does not support the video tag.
 Final Mobile and Hardware Demonstration video Link
 Your browser does not support the video tag.
 Hardware Demonstration videos Link
 OUR TEAM PROFILE
 Sudam Kalpage
 Thilini Deshika
 Hashan Eranga
 Services
 Supportive Mobile Application
 A mobile application that shows the current gas level and
 detect any leakages.
 Built in Level Detector on the hardware
 The current gas level will appear in the hardware
 Notify any gas leakage quickly
 If there is any leakage buzzer in the device as well as a notification will notify the owner
 Easily Refilling Plans
 You will be connected to any Gas suppling company instantly.
 Contact
 Sudam Kalpage
 Thilini Deshika
 Hashan Eranga
 Advisers
 Dr.Isuru Nawinne
 Dr. Ziyanm Marikkar","LP gas tank users face various difficulties because of not having a precise way to find the remaining gas amount in the tank. To overcome those problems with some protection, we are introducing the gas level indicator and leakage detector. Gas Level Indicator and Leakage Detector is a system aimed at LP gas tank users and distributors, consisting of two main areas; a gas level indicator and a gas leakage detector. To indicate gas level, the device display and the mobile/web application will be updated in real-time, displaying the current existing gas percentage in the tank. For gas leakage detection, a buzzer will alarm the user about it while sending a notification to the mobile/web application.",E16,Cyber-Physical Systems Projects,https://projects.ce.pdn.ac.lk/3yp/e16/gas-level-indicator-and-leakage-detector/,https://github.com/cepdnaclk/e16-3yp-gas-level-indicator-and-leakage-detector,https://cepdnaclk.github.io/e16-3yp-gas-level-indicator-and-leakage-detector,https://cepdnaclk.github.io/e16-3yp-gas-level-indicator-and-leakage-detector/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/3yp/E16/gas-level-indicator-and-leakage-detector/
68,obstacle bots for swarm robots,"Swarm Robotics Platform | e16-3yp-obstacle-bots-for-swarm-robots
 e16-3yp-obstacle-bots-for-swarm-robots
 Swarm Robotics Platform
 Team
 Members
 Thushara K A R
 :: E/16/369 :: e16369@eng.pdn.ac.lk
 Thilakarathna D M D U
 :: E/16/366 :: e16366@eng.pdn.ac.lk
 Dissanayake D M T H
 :: E/16/088 :: e16088@eng.pdn.ac.lk
 Supervisor
 Dr. Isuru Navinna
 Mr. Ziyan Marikkar
 Prof. Roshan Ragel
 Dr. Upul Jayasinghe
 Related links
 Faculty website
 Department website
 Web Application Home Page
 Swarm Dash Board
 Git Hub Repository
 Design Manual
 User Manual
 TABLE OF CONTENT
 OVERVIEW
 GOALS
 SPECIFICATIONS
 SOLUTION ARCHITECTURE
 HARDWARE
 WEB INTERFACE
 ALGORITHM
 TESTING
 BUDGET
 OVERVIEW
 In swarm robotics the major barrier is that the researchers have to do a lot of hardware implementation prior to their projects. In this particular project we are going to come up with obstacle bots for a swarm robotic arena which is a part of a swarm robots platform project which eventually solves the above mentioned problem.
 GOALS
 Automated obstacle bots monitored by an overhead camera setup.
 Bots can move to the desired positions with a user friendly interface.
 SPECIFICATIONS
 Obstacle robot swarm is capable of moving every individual obstacle robot to their own destination with the consideration of
 collision
 avoidance.
 With the help of collision avoidance algorithms, obstacle robots can be placed in certain positions which allows the researcher to make
 various obstacle shapes on the arena made out of obstacle robot combinations .
 Obstacles can be programmed to be static or dynamic. Dynamic obstacles can model scenarios that have a motion in the obstacle.
 Each robot has its own radio module
 which uses a 433Mhz radio band. These modules can be used to communicate with the base station.
 Each robot has two independent wheels that can perform forward, backward and turning operations. With the help of inbuilt gyroscope robots can perform accurate turning operations.
 SOLUTION ARCHITECTURE
 HARDWARE
 REAL HARDWARE
 3D MODEL
 3D MODEL DEMO VIDEO
 SAFTY MEASURES
 PCB DESIGN
 PCB DEMO VIDEO
 WEB INTERFACE
 #### Web Interface
 WEB INTERFACE DEMO VIDEO
 #### 3D Interface
 3D INTERFACE DEMO VIDEO
 #### Platform PC Operator GUI
 3D INTERFACE DEMO VIDEO
 ALGORITHM
 TESTING
 #### Algorithm Deployment
 Algorithm Deployment Video
 #### Web Interface Authentication And Authorization Testing
 Web Interface Testing Video
 #### Hardware Testing
 Hardware Testing Video
 BUDGET
 FUNDING
 This project was funded by the Prof. Suhada Jayasuriya Project Support Fund through .
 FINAL DEMO VIDEO
 Final Demo Video
 FUTURE IMPROVEMENT
 We plan to run all the algorithms first and then send the data to the robots.
 We have to further tune the parameters
 to get a smooth process.","Obstacle bots for the existing swarm research project of the  Department of Computer Engineering, University of Peradeniya. Robots are localized with an overhead camera set up in order to create very challenging patterns using movements in a self-created potential surface with collision-avoiding algorithms on optimized stochastic gradient descent and particle repulsion theory.",E16,Cyber-Physical Systems Projects,https://projects.ce.pdn.ac.lk/3yp/e16/obstacle-bots-for-swarm-robots/,https://github.com/cepdnaclk/e16-3yp-obstacle-bots-for-swarm-robots,https://cepdnaclk.github.io/e16-3yp-obstacle-bots-for-swarm-robots,https://cepdnaclk.github.io/e16-3yp-obstacle-bots-for-swarm-robots/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/3yp/E16/obstacle-bots-for-swarm-robots/
69,qurantine tracker,"Qurantine Tracker
 Quarantine Tracker
 A third year students project University of Peradeniya
 Home
 Team
 Introduction
 Solution Architecture
 Designs
 Testing
 Budget
 Conclusion
 Team Details
 Team Members :
 Nimashi Uthpala
 E/16/039 e16039@eng.pd.ac.lk
 Sachini Dissanayaka
 E/16/087 e16087@eng.pd.ac.lk
 Tharushi Suwaris
 E/16/364 e16364@eng.pd.ac.lk
 Advisors :
 Dr. Isuru Nawinna
 Dr. Zian Maraikkar
 Department of Computer Enginnering
 Faculty of Enginnering
 University of Peradeniya
 Introduction
 Backgound
 The COVID19 pandemic, also known as the coronavirus pandemic, is an ongoing global pandemic of coronavirus disease 2019 (COVID19), caused by severe acute respiratory syndrome coronavirus 2 (SARSCoV2). The outbreak was first identified in December 2019 in Wuhan, China. The World Health Organization declared the outbreak a Public Health Emergency of International Concern on 30th January 2020 and a pandemic on 11th March. The first case of the virus was confirmed in Sri Lanka on 27th January 2020, after a 44-year-old Chinese woman from Hubei Province in China was admitted to the National Institute of Infection Diseases.
 As of 16 August 2020, 2,890 confirmed cases have been reported in the country with 11 deaths. On 3rd March 2020, the first reported case involving a Sri Lankan origin outside Sri Lanka was reported in Italy. As of 23 March, forty-five quarantine centers have been built in the country by the Sri Lanka Army as a preventive measure to tackle the coronavirus pandemic. Nearly 3,500 people have been under quarantine in 45 quarantine centers which also include 31 foreigners from 14 countries.
 As of 25 March 2020, Sri Lankan authorities have tracked down over 14,000 people who had contacted the identified patients and had ordered self-quarantine for such people.
 Motivation
 When putting a person under self-quarantine, medical officers and the police will come to the house where that person lives and advise that person not to go out. Even though the person under self-quarantine is advised not to go out the medical officers cannot be guaranteed that the person will not go out because they do not have a proper method to monitor the people under self-quarantine. If people under self-quarantine do not obey the rules and go out of the home and interact with other people, there will be a risk of exposing other people to the virus if those self-quarantined people are infected
 Real world Apllication of the Quarantine Tracking Device
 The device is designed to be used by the medical authorities to track if the person who is under quarantine (wearing the device) is breaking the law and also to track his/her health; to check if he/she would implementing the symptops of the illness
 using thier smart phone and reduce the spreading of the virus as much as possible.
 Solution Architecture
 As a solution we present a band which is wearable by the person who are unde quarantine such that the corresponding data such as body temperaure, the movements of the person and the location
 will be gathered and send to the mobile app which is used by the medical authority. so they can track and moniter the person keeping their distance and as well as improve the quality of their job.
 Features of the device
 Location tracking
 Can set a virtual fence
 Detect and monitor the body temperature
 Unauthorized removal notifications
 Low battery notification
 Hardware And Software Designs
 Hardware Design
 Components
 Node MCU 12e module
 ublox NEO-6M GPS md0153 Module
 LM 35 temperature module
 MAX30100 Pulse Oximeter and Heart-Rate Sensor
 NiMH rechargeable batteries
 3V mini buzzer
 LEDs
 Power Requirement
 The node mcu works with 3.3 v so we supply power of 7.4v(power supply) from the betteries to the Vin pin of the node mcu.
 since the node mcu has an in built voltage regulator we can supply a voltage range of 7-12Vs to Vin
 Circuit Diagram
 PCB Design
 Product view
 Product Dimension : 59x65x20 mm (WxLxH)
 Software Design
 Mobile Application And Website
 User Interfaces of Website
 Technology front end : HTML,CSS,Javascript
 Technology back end : Nodejs
 User Interfaces of Mobile app
 Technology : React Native
 Additional Libraries : React NAtive Vector icons, Animated API, Custom Fonts from Google fonts
 For navigation: React navigation library
 Stack navigation: For login and signup screens
 Drawer navigation : All the other screens
 For better user experience two kinds of navigations have been used to navigate though the different tabs of the mobile app.
 Data Handling
 Database
 Technology : MongoDB
 Testing
 Data validation testing and UI testing is done in frontend of the website in manual basd procedure.
 User is only allowed to enter data within the given restrictions. They are all indicated to the user when they use
 the website.
 In validation of data, the limitations are as follows:
 All kinds of Names, ID numbers only should be 25 of character length in maximum.
 One line an address can be 30 in character length maximally.
 Contact number should be exactly 10 characters long
 Password should be more than 8 characters
 These values were selected based on the database configuration in Backend, making sure the intergrity of data.
 In frontend it is tested that the user enters the data as expected and do not allowed to proceed through otherwise.
 During the tests, nearly 5 test cases were used t ckeck in individual case.
 The summery of the tests conducted as follows :
 UI testing were conducted testing performance of the website, mainly checked whether the buttons of the pages are working properly and also the links are directed as they are supposed to be.
 Budget
 Component
 Price (LKR)
 Supplier
 NodeMCU
 665
 SKYTRONIKS(PVT)LTD
 Neo 6m GSP Module
 1350
 SKYTRONIKS(PVT)LTD
 LM 35 temperatuer sensor
 110
 SKYTRONIKS(PVT)LTD
 MAX 30100 pulse rate sensor
 450
 SKYTRONIKS(PVT)LTD
 3V mini buzzer
 20
 SKYTRONIKS(PVT)LTD
 TOTAL
 3535
 Conclusion
 Github repo link","The idea is about to provide a comfortable and easy tracking option for a user to track a person under home quarantine due to the current pandemic situation by tracking their location. The user will be marking a geo fence when they register a new person under quarantine so the user will be notified if the fencing rules are violated. Apart from this the device will be measuring the body temperature of the person, so if he start showing symptoms(fever) of Covid 19, then the user can take necessary actions regarding the person's health. ",E16,Cyber-Physical Systems Projects,https://projects.ce.pdn.ac.lk/3yp/e16/qurantine-tracker/,https://github.com/cepdnaclk/e16-3yp-qurantine-tracker,https://cepdnaclk.github.io/e16-3yp-qurantine-tracker,https://cepdnaclk.github.io/e16-3yp-qurantine-tracker/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/3yp/E16/qurantine-tracker/
70,smart door lock,"SmartDoorLock - Responsive HTML5 Template
 Toggle navigation
 Home
 About
 Solution Architecture
 Hardware
 Software
 Testing
 User Experience
 Budget
 Contact
 SmartDoor Lock
 A project of third year students developing a Smart door lock for the entrance
 All Features
 View Plans
 Problems at the entrance
 People who use doors for the entrances face different types of problems when entering. The security level, accuracy and efficiency of the current lock system at the entrance are some major causes for the user problems.
 Therefore by this project we are developing a smart door lock including the following main features.
 Face Recognition
 Finger Print detection
 RFID card reader
 -In addition to them following features will be included in the smart door lock.
 - Unlock record of the current user at the door
 - Information display of the current user
 - Allowing only one person to enter at a time
 - Web Application to control access
 - Access at night only for specified persons
 Solution Architecture
 user data can be taken from the Finger print sensor, RFID card module and camera module.Those data can be compared with the data stored in the firebase realtime database.
 Hardware components communicate with the firebase database using get and post requests of HTTP protocol. Realtime database is connected with the web application and read
 and write data from it. Firebase Hosting services are used for cloud deployment of the web site.
 Product Overview
 Acoording to the diagram camera is placed in the height of an average
 person for the handiness of the door lock. IR sensors are kept at a corner of the door to captue the object(person) efectively.
 Other two modules, Fingerprint sensor and RFID reader are placed near the lcd display . So that while enterring the credintails
 use can read the lcd display information easily.
 Hardware design
 1 / 3
 Circuit Daiagram
 2 / 3
 Full Pcb Design
 3 / 3
 ❮
 ❯
 Main Hardware components
 Raspberry Pi 3 Model B
 As we are doing Face recognition we have choosen Raspberry Pi module becuase it
 has 1GB RAM to process and Micro SD port for loading operating system and storing data.
 Also python libraries like OpenCV can use with it.
 There is a CSI camera port in the module for connecting a Raspberry Pi camera.
 Raspberry Pi Camera Module
 Finger Print Sensor
 RFID module
 LCD display
 3D Model of the product
 1 / 6 --Try different views by clicking below
 2 / 6 --Try different views by clicking below
 3 / 6 --Try different views by clicking below
 4 / 6 --Try different views by clicking below
 5 / 6 --Try different views by clicking below
 6 / 6 --Try different views by clicking below
 ❮
 ❯
 Software Implementation
 Web Application
 Web appliaction is designed to control access to the system. In our web application there are three types of administrator roles.They are,
 - To monitor database
 - To add/update/delete personal data through web application
 - To view all door users' access history, admin sign in history to the web site and notifications about unauthorized access
 Speciality of this roles is that, each of these administrators able to handle only the given role. It makes the web application more secure. Moreover, Only these persons can login to the web applcation.
 For the authentication purpose we have added 2 factor authentication.
 - By using email and password
 - Send OTP to predeteremined mobile phone number.
 FrontEnd technology
 ReactJs has been used for the implementation of the web application.
 Why ReactJs ?
 Create components
 virtual DOM and server side rendering
 BackEnd technology and cloud deployment
 Firebase realtime database, which is a NOSQL database has been used as the database.
 Why Firebase ?
 Reliability
 Reliable data tranfer in firebase allow to frequent state syncing and low latency in data transfer
 Realtime data transfer is important as hardware components needs data from firebase
 to allow the access. Also in an entering, it helps to monitor the door access records through web application in
 real time
 Scalability
 Due to the high
 performance and scalability of firebase, when there is any data change, Firebase helps in the calculation of the
 minimum set of updates needed to keep all the clients synchronized.
 Data can be synchronized in real time eventhough there are multiple doors.
 Multiple users can use the web Application (concurrent connections )
 Authentication
 With the use of Firebase auth services,two
 factor authentication has been used for the web application.
 firebase Email and password authentication
 firebase phone authentication
 UI designs
 Visit our web application
 UI designs
 Software Testing
 Firebase Security Rules
 The Firebase Local Emulator Suite consists of individual service emulators built to accurately mimic the behavior of Firebase services.It enables app to directly connect to these emulators to perform integration testing.Through this we checked whom can access to the data as we set security rules to deny permission for unauthorized access.
 Check UsedID validity
 Always the
 IDs stored in the database are compared with the ID which is taken by the hardware. So it is important confirm that that process is correctly processing if not the unlocking records will be inacurrate.
 Data encryption and Decryption function testing
 In the database, userID of each employee and the security status of a Admin is considered as most sensitive data stored in the database as userID is directly connected with hardware components which gives the access to the doors and admin privilages are depend on the security status of a admin. So these data is stored in the database after encryption. We have implemented AES encryption decryption functions.
 All the hardware configurations are planned to do in python language and Reactjs is used for implementation of web Application.So tests are done to verify that both python and JavaScript functions give the same result in encryption and decryption process.
 Test 5 - Time taken for encryption and decryption
 Data encryption and decryption may cause the delays in the reliable data transfer to the frontend and hardware components. So we have checked the time taken for that. Actualy
 it takes few microseconds. So the effect of delay can be negligible. Following values may slightly change according to the data we use
 - for encyption = 0:00:00.000782
 - for decryption = 0:00:00.000019
 Click here to know more details about software testing >>>>
 Hardware Testing Plan
 01.
 Check each unit separately
 Fingerprint sensor
 - Ability to scan a fingerprint
 - Validate the fingerprint accuracy - Test after updating new fingerprints
 - Test with valid user when,
 Finger is dipped with water
 Finger is dipped with any color
 Sketch mark is on finger
 Finger is covered with transparent gloves
 Different angle of the finger is used
 Rfid card reader
 - Test the environment in which the tag is being
 read (need to check whether this is being too vulnerable to environment factors)
 - Test different orientations of the tag
 Face recognition
 - Test the three important stages of face recognition process
 I.	Face detection
 Using different lighting conditions
 Using different times of day
 II.	Face capture
 III.	Face match
 Using different lighting conditions
 Using different times of day
 Using different moods and emotional states
 IR sensors
 - How the sensors can be detected (Can use a small circuit of led blinking)
 - Test if two people get detected and check that message goes to buzzer
 02.
 Check whole combined unit
 - After updating new user details check how the entire system works
 - Power Supply
 - Check the connectivity of whole system ( Using blinking an led or oscilloscope and logic probes)
 Performance Testing
 A. 	Test the performance of each unit and whole system for allowing a valid user
 the time taken by the each unit and whole system to allow the
 B. 	Test the performance of each unit and whole system for not allowing an invalid user
 the time taken by the each unit and whole system to identify an invalid user
 C. 	Test the time of the entry of the user is recorded to the system after been authorized
 How to use our product
 As we mentioned in the introduction there are several reasons
 to prove that tradition door
 lock systems at the entrance are inefficient. But in this product we have overcome all those problems and give a
 quick access to the door user while providnig important addtional services such as take access
 records and giving realtime update about the access using a lcd display
 Register Users
 In order to register an user,
 administrator should fill the form provided in the website
 with uniquely provided userid. Here, the corresponding userid is registered for one specific person.
 Access Given
 First contact the RFID card/tag to the RFID reader. Then place the finger on the fingerprint sensor and look at
 the camera to capture a photo of the user.
 Then the access will be granted if all there steps are correctly passed by that person. LCD display will
 display the information whether access has been given or not. All the accesses will be recorded on the website.
 Identify Unauthorized Aceess
 If any unauthorized access is detected i.e. using an unregistered card/tag or trying to access more than one person
 into the room using one card/tag, access will not be granted and the buzzer will make a loud sound. LCD display
 will display the information whether access has been given or not. These accesses will be recorded on the website.
 Demostration of user experience of admin
 who can view history of the access to the door and the web site
 Click here to get the user manual >>>>
 Advising Lecturers
 Dr.Isuru Nawinne
 Dr.Ziyan Maraikar
 Links
 Visit our github repository
 Visit our web app
 University of Peradeniya
 Faculty of Engineering
 Department of Computer Engineering
 Project Contributors
 Name
 Virajani Dharmathilaka
 Tharushini Jayathilaka
 Chanika Madushanki
 Contact Details
 virajanidharmathilaka@gmail.com
 tharushinithiwanka@gmail.com
 cmkariyawasam10@gmail.com
 End of page","This is a solution which can be used by the advanced security system users at their entrances. This Smart door lock consists of three main security features, RFID sensor, Fingerprint sensor and Face recognition. The collected data is sent to a database and the updated unlocking records will be displayed at the door. In addition, two IR sensors are placed on either side of the door to check the entrance of only one person at a time. All the records of authorized/unauthorized data will be available on the web application and three different admin roles are introduced to manage the web application.",E16,Cyber-Physical Systems Projects,https://projects.ce.pdn.ac.lk/3yp/e16/smart-door-lock/,https://github.com/cepdnaclk/e16-3yp-smart-door-lock,https://cepdnaclk.github.io/e16-3yp-smart-door-lock,https://cepdnaclk.github.io/e16-3yp-smart-door-lock/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/3yp/E16/smart-door-lock/
71,smart infared shooting sport,"e16-3yp-smart-infared-shooting-sport | The purpose of this project was to design, build, and test an infrared shooting sport from the ground up to shoot farther in low cost and most importantly make it smart and updatable
 e16-3yp-smart-infared-shooting-sport
 Team
 Group Members
 E/16/320 e16329@eng.pdn.ac.lk
 E/16/319 e16319@eng.pdn.ac.lk
 E/16/126 e16126@eng.pdn.ac.lk
 Supervisor
 * Dr. Isuru Navinna
 * Mr. Ziyan Marikkar
 * Prof. Roshan Ragel
 * Dr. Upul Jayasinghe
 Related links
 * Faculty website
 * Department website
 ### IMAGE
 ## TABLE OF CONTENT
 OVERVIEW
 GOALS
 SPECIFICATIONS
 SOLUTION ARCHITECTURE
 HARDWARE
 MOBILE APP.
 BACKEND
 TESTING
 BUDGET
 OVERVIEW
 X-tag is a Smart IR shooting sport.In currrent related product are,
 very expensive
 Companies with large indoor environments charge up to $10 for a single game.
 Not enough game modes/options.
 Not smart enough.
 Not updatable
 Therefore In this Project we build a tag system with a central server and a mobile application.In that manner we were able to
 Create a updatable ,smart IR tag system.Also the cost of our system will be very lower compare to current products in the market.
 GOALS
 Shoot father up to 100ft
 And make cheaper to produce
 And most importantly we want to make our system smart.
 SPECIFICATIONS
 There are mainly 3 parts in this project .Xtag gun and the headband to shoot and receive IR,Xtag mobile application to
 Choose game modes,initialize the game and a firebase server for communicate with in the game .
 Every gun has a cababilty of shooting more than 30m and many features like LCD screen,LED lights,vibration motors.
 The headband is connected to gun with a wired connection and it receive the IR signals shoots by other players.
 Also Headband has 3 LEDs to indiacate the Team colour and Another LED to blink when get shotted.Those IR receivers and LEDs are locate in every
 4 sides in the gun.
 Inizialize the gun before the match,selecting the team,battle mode are done with the help of the Xtag mobile application.Gun is connecting to the mobile using Bluetooth.
 Finding the shooter,giving scores,join to a battle are done with the help of the firebase server by connecting to it through the Xtag mobile application.
 Thanks to our system artchtecture this system is updatable(Can be added more modes more option without changing the hardware) and a smart one.
 SOLUTION ARTCHTECTURE
 Every player has a head to reserve IR shots and IR gun to shoot IR.
 when the platyer shoots he shoot with some data.They are,
 Team ID
 Damage
 Player ID
 Every players gun is connected to the mobile application through blutooth.
 Every phonr is connected to the our server.
 Using mobile application,
 gun is initilized.
 game mode is selected
 Server is used to,
 comunicate with the gamne.
 As a example Find who is the shooter and ,giving scores are done with the help of
 the backend.
 HARDWARE
 IR emitter
 This is the heart of the this project and it is very challenging when we use IR communication for this kind of
 purpose
 To shoot further we Planned used high power IR
 IR emitter - TSUS5202
 power=
 170mW , 150mA
 Since esp32 cant give 100 mA for the transistor is used
 BD139
 and a lens is used to focus
 Diameter about
 -
 38mm (1.5″)
 IR receiver
 * SM0038 - TSOP1738 - 38KHz IR receiver
 * This Moduile has built in
 *
 signal amplifier
 *
 2.5 V to 5.5 V
 LCD screen 16x2 with I2C module
 * standard HD44780
 * 5V
 sunder
 * Buzzer Piezo Bleeper Sounder
 * Frequency 4kHz
 * power - 10mA
 vibration motor
 * 10000RPM Metal Brush
 * DC 3.7V 5V
 135mA-180mA
 And also push button and RGB LED are used.
 IR circuit
 IR lens
 IR Receiver
 SM0038 - TSOP1738 IR Receiver
 3 pin
 38KHz
 -40 to +80C
 2.5 V to 5.5 V
 binary (data)
 CONTROLLER PLATFORMS
 NodeMCU esp32 duel core microtroller
 IR Library
 * Currently it is NEC IR protocol
 * 38 KHz
 * 8
 bit is used
 ![](/e16-3yp-smart-infared-shooting-sport/images/irshot.png)
 implementation
 Design seperated into several parts for the ease of 3D printing
 Fabrication circuit
 Used small circuits that are connected to the main circuit for
 Switch buttons
 LEDs
 Vibrator motor
 IR emitter
 Buzzer
 Connections are done according to the NODE MCU esp32 datasheet
 BACKEND
 Access and authentication
 Using Email and a password players can register Xtag
 Players have to verify their
 Email before signed in
 More detail will be on testing report
 * It is ideal for our Xtag mobile app.
 * Cloud deployment -Firestore
 * database helps to store real-time and synchronize game data.
 * Firebase authentication library is used for authentication
 Storage
 Cloud Firestore
 Fast performance, high availability, and security
 Database
 Two main collections are used to store Player data and Match data.
 Player collection will store records as documents according to the Used ID.
 Match collection will store records as documents according to the Match ID.
 Players’ details of each match will be stored as a sub collection inside the relevent match document.
 Reasons behind the database
 When do a query search in a match, It will be efficient
 When player want see his paset, it will be efficient
 We can increase the efficiency of the system by deleting
 old match data.
 There Are Some data in the match,which are useless later
 Ex: isready,rescue code
 MOBILE APP.
 Main Functionalities
 How to refresh the screen when players are connected
 * Streams are used
 Syncing the game time counter
 How to set a tempid
 How to give a score to the shooter
 * Query searching is done by the killed player
 User Interface - Mobile Application
 Develop using Futter 1.17
 Home page
 User Profile
 SignIn and SignUp pages
 <width=""100""/>
 Connect gun and go to battle
 Create or join a battle
 TESTING
 Xtag application testing
 Authentication test (Integrated
 security test)
 Network compatibility testing
 Data Mapping testing
 Stored Procedures(Black box testing)
 Device compatibility testing
 software testing sumarry : https://docs.google.com/document/d/15QQ1ZPAIXyWhq7m–7BOh9n0jbu_8oZKydKhLDy0Gqo/edit?usp=sharing
 software testing report : https://docs.google.com/document/d/1bhaTQPnoYpXo6yQ9MJRpFDIS963kJpqcUUipNbIv-IE/edit?usp=sharing
 Embedded system testing
 Design level testing -
 to find best IR collecting method
 Unit tests
 Blutooth communication
 IR communication
 Physical test
 Fire range
 fire accuracy
 HArdware testing sumarry : https://docs.google.com/document/d/1yRsRNFsx3cfH2Z9USqZvHvry2KBjpZQ3aWNl-_fgJvw/edit?usp=sharing
 HArdware testing report : https://docs.google.com/document/d/1XJSeqUBuJQFIUvLsx5cCLyxi5nqX5FbRWItxmr5zCmQ/edit?usp=sharing
 BUDGET
 Demonstration
 The demonstration video :
 go to video","The purpose of this project was to design, build, and test an infrared shooting sport from the ground up to shoot farther in low cost and most importantly make it smart and updatable",E16,Cyber-Physical Systems Projects,https://projects.ce.pdn.ac.lk/3yp/e16/smart-infared-shooting-sport/,https://github.com/cepdnaclk/e16-3yp-smart-infared-shooting-sport,https://cepdnaclk.github.io/e16-3yp-smart-infared-shooting-sport,https://cepdnaclk.github.io/e16-3yp-smart-infared-shooting-sport/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/3yp/E16/smart-infared-shooting-sport/
72,smart meeting automaton,"Smart Meeting Automaton
 Unified Project
 Menu
 About
 Overview
 Solution
 Design/Progress
 Budget
 Contacts
 SMART
 MEETING
 AUTOMATON
 Find More
 Solution for a Problem
 We have seen some meeting rooms with working AC machines and projectors unnecessarily without anyone in the room, causing a wastage of current. In some cases, it is quite different that the AC machines are not working even almost all the room is filled.
 Other than that, presence of a worker to the meeting venue at the time of meeting, is essential to control different devices of AC machines and projectors. This control process is sometimes complex as it has to be used various remote controllers to control different devices.
 Our solution ""SMART MEETING AUTOMATON"" resolves almost all the things addressed here.
 System Overview
 SMART MEETING AUTOMATON consists of two major streams.
 Client Application
 Control Unit
 These two streams are connected via AWS cloud.
 Solution Architecture
 Role of Main Control Unit
 Local server on Raspberry Pi is updated automatically for every 10 minutes according to data on remote AWS server.
 In case of Wi-Fi is disconnected, the Raspberry Pi storage can manage the operations.
 Based on direction data of AC machines, projectors, server motors rotates.
 Role of Extending Unit (Optional)
 Receive signals from Main Control Unit as Wi-Fi signals.
 Give signals to AC machines, projectors as IR signals.
 Entity Relationship Diagram
 System has administrator persons
 Administrators create accounts for users(Lecturers/meeting owners)
 Administrators reserves lecture rooms/meeting rooms
 Admins figure out how to control devices in meeting room/lecture room
 Devices within meeting rooms automatically operates according to schedules
 Users can also control devices within the meeting room.(Via mobile app)
 Front-end Software
 Design Architecture
 React
 Dynamic content
 React Hooks
 AXIOS
 Communication with the backend
 Context API
 State management
 Mobile Application
 Back-end Software
 Node.js Express
 Can build scalable, fast and non-blocking I-O backend server
 MongoDB Atlas
 Automatically scale increase cluster storage with the growth of data
 Storage can be maximized with auto-scaling features
 Security
 Password hashing
 Json Web Tokens(JWT) for user authorization
 Authenticated local strategy using email and password
 Authorization using JWT strategy to protect end points
 Access tokens and refresh tokens for user identification
 Data validation before sending them to database
 Software Testing
 Summary Report
 Area of testing
 What was tested
 How was the test done ?
 Purpose
 Expected Results
 Findings
 Server
 Login to the system
 User needs to give correct username and password
 Admin and user have separate authorization methods. So some route need to admin login it will check by The API (User or Admin that are logged in)
 Token is given, if user logged successfully
 Otherwise, anyone can not access the services of the system
 Token is valid for 30 minutes. If a person knows this token within that time, he can access the system. (except for critical operations)
 Server
 Adding user to system and verify
 User need to register for the system
 Need freshly logging of admin
 User needs the verification link that comes to email
 After successfully adding, user needs to login again
 User can be added to the system with an email verifification and only admin can add user
 On success : When admin adds a user to the system, verification email is sent
 New user can only login the system via that verification link
 User is uniquely identified by the email
 User is validated whether he has provided an correct email
 Server
 Adding room
 Delete room
 Delete user
 Need admin fresh login (It expires within 5 mins)
 These operations are allowed only for admin to avoid misbehaviours of the system
 On success : Status code - 200
 Failed operations : Status code - 401
 Admin should verify his login with his password each time when he does these operations
 Unit tests to verify outputs over inputs
 Embedded Systems Testing
 What to test ?
 Why is it important ?
 How will it be tested?
 Admin can add a meeting rooms to the system.
 When a control unit is established, he can add that meeting room to the system with devices giving all configuration data.
 Checking whether configuration data is properly reached to the database and relevant hardware nodes.
 Admin and users can add meeting schedules to the system
 The devices within a meeting room work according to the given schedules.
 Checking whether the projectors or ACs working properly according to schedules.
 Devices work properly with the configured data given by system admins
 The devices within a meeting room work according to the given config data.
 Checking whether the projectors or ACs can work properly.
 Hardware Designs
 Main control unit consists of two major parts.
 Base Part
 It mainly consists of Raspberry Pi
 Other components are LCD display, relay module, sockets to connect bulbs
 Upper Cover Part
 It has few sockets to connect IR transmittors
 So that IR transmittors can be directed exactly towards the projector or AC
 Extending Unit (Optional)
 Base Part
 It mainly consists of NodeMCU unit
 It forwards the signals comming from main control unit to projectors or ACs
 So that, we can give control signals for the devices, that can not be reached be main control unit
 Circuit Designs
 Schematic Designs
 PCB Designs
 Cloud Deployment
 Budget Report
 Find more details from our github project repository
 Visit Now
 Team Members
 Visit Our github Accounts
 Chamath Amarasinghe
 Diwanga Amasith
 Wishwa Madusanka
 Email
 e16022@eng.pdn.ac.lk
 e16025@eng.pdn.ac.lk
 e16222@eng.pdn.ac.lk
 Advisors
 Dr. Isuru Nawinne
 Mr. Ziyan Marikkar
 Essential Links
 Department of Computer Engineering
 Faculty of Engineering
 University of Peradeniya
 Copyright © Smart Meeting Automaton 2020","Smart Meeting Automaton is a revolutionary idea of controlling devices automatically within a meeting room. In most of our day-to-day life, we have experienced that air conditioners, projectors are working unnecessarily even without a presence of a meeting and sometimes those devices should be controlled with a human involvement using several types of remote controllers which is really an inefficient way. Our approach to that matter is automating this process introducing a control unit per each meeting room and a client application. Client application is basically used for scheduling meetings and meeting room configurations that data related to air conditioners and projectors is set. Control unit in meeting room sends required signals to turn on/off devices according to the schedules added to the system by the client application. In addition to that, mobile application can be used to control devices by scanning a QR code.",E16,Cyber-Physical Systems Projects,https://projects.ce.pdn.ac.lk/3yp/e16/smart-meeting-automaton/,https://github.com/cepdnaclk/e16-3yp-smart-meeting-automaton,https://cepdnaclk.github.io/e16-3yp-smart-meeting-automaton,https://cepdnaclk.github.io/e16-3yp-smart-meeting-automaton/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/3yp/E16/smart-meeting-automaton/
73,smart payment system,"Smart-payment-system | e16-3yp-smart-payment-system
 e16-3yp-smart-payment-system
 Smart-payment-system
 This is the 3rd year embedded system project
 Group Members :
 Basnayake S.S. E/16/054
 Madusha shanaka E/16/351
 Nadun welikanda E/16/389
 Table of contents
 Problem
 Our solution
 Solution Architecture
 Hardware & Software Design
 Testing
 Detailed budget
 Links
 OVERVIEW
 As we all know gaming industry is growing day by day. As a result of this growth the concept of gaming centers has been popular lately. In a gaming center they normally use coins to play the games. The procedure is when a customer comes to the gaming center they have to buy coins from the cashier in order to use those as a paying method to the gaming machine. Once they put enough number of coins into the machine they are allowed to play the game.
 Problem
 The problems of this approach are the coins are simply hard to carry around the gaming center when the customer has large number of coins and also they have good chance to loose a coin or two. When we consider the gaming center’s point of view they have to collect coins every day at each machines and count it and keep the record.
 Our Solution
 As a solution for above problems we came up with a solution which completely replace coins system with a RFID and NFC technology. In our solution, when the customer arrives at the cashier and pay money they will be issued a RFID card or if they have NFC supported mobile device they can use our mobile app instead of coins. To play the game what they have to do is simply tap the RFID card or the mobile device into the reading area which is in the gaming machine. After the customer done playing he can go to the cashier and return the card. If there is any balance the cashier will return the balance.
 Solution Architecture
 Detailed Budget
 Hardware and Software designs
 3D model Design
 Cashier Node
 Gaming Node
 Hardware design
 Demonstration
 Cashier Node
 Gaming Node
 Software design
 1. Database
 2. Cashier Application
 3. Web Application
 Testing
 Under testing we checked for unit testing and intergration testing. In unit testing we checked all functions related to adding a card, refunding, return, scanning a card, issue a card, register and login.
 Under intergration testing we tested Basic route to see the server up and ruining and Route which send the 404 message
 Target Audience
 The target audience of this project is Gaming centers which is our primary target. As our next milestone, we are planning to update this system in a way that it can be used in any commercial market place such as casino, leisure world, etc.
 Links
 Department of Computer Engineering
 Faculty of Engineering
 University of Peradeniya
 Project website","In a gaming center, they normally use coins to play games. When a customer comes, they have to buy coins from the cashier to use those as a paying method to the gaming machine. So we came up with a simple but efficient solution that will allow you to play any game inside a gaming center with a single tap. It is RFID based payment system.",E16,Cyber-Physical Systems Projects,https://projects.ce.pdn.ac.lk/3yp/e16/smart-payment-system/,https://github.com/cepdnaclk/e16-3yp-smart-payment-system,https://cepdnaclk.github.io/e16-3yp-smart-payment-system,https://cepdnaclk.github.io/e16-3yp-smart-payment-system/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/3yp/E16/smart-payment-system/
74,smart pharmaceutical warehousing,"Smart Pharmaceutical Warehousing | e16-3yp-smart-pharmaceutical-warehousing
 e16-3yp-smart-pharmaceutical-warehousing
 Smart Pharmaceutical Warehousing
 Group Members :
 K.R. De Silva E/16/068
 (e16069@eng.pdn.ac.lk)
 J.M.Praveen Dhananjaya E/16/081
 (e16081@eng.pdn.ac.lk)
 F.S Marzook E/16/232
 (e16232@eng.pdn.ac.lk)
 Supervisors
 Dr. Isuru Nawinne
 Dr. Ziyan Maraikar
 Table Of Content
 Problem Overview
 Our solution
 Solution Architecture
 Hardware & Software Designs
 Testing
 Detailed budget
 Related Links
 Problem Overview
 Conventional pharmaceutical warehouses mostly use manpower to manage and handle goods inside their warehouse complexes. Some warahouses use small indoor vehicles. But all these conventional methods has some inevitable downsides. Most common suhc disadvantages are redundant activities, higher labor cost, suboptimal handling of goods and internal and external thefts. These downsides cause losses in both time and profitability and lead to inefficent warehouse management.
 Our Solution
 In this project, we are trying to address this issue by developing a fully automated warehouse management which will minimize the drawbacks while improving efficiency and profitability. We are implementing a warehouse managament system which consists two types of robots; robots arms - to handle loading/unloading of goods, automated guided vehicles(AGVs) to transport goods inside warehouse. Also an online shopping portal to make the purchases from the warehouse.
 Once a customer places an order, the order will be received, processed and will be delivered to the delivery station without any human involvement. The customer will then be informed to pickup his/her order from the warehouse.
 How It Works
 Our solution consists three components :-
 Fully automated warehouse
 Controller interface to control and override(if necessary) the autonomous operations
 Warehouse database with an online shopping portal, as an interface to retailers to make their purchases
 The warehouse has two base stations, the delivery post and receiving post, to deliver goods to the customers and receive any stocks to the warehouse respectively. In these base stations, two fixed robot arms are placed to do the loading and unloading process for goods.
 Inside the warehouse, movable robot arms are placed among shelves to load and unload the goods to AGVs.
 Once a client places an order, the database is updated and the computer fetches the relevant information about the goods and triggers up the local warehouse controller. The warehouse controller then generates instructions and are passed to the robot arm and the delivery robot to perform the task. Here, the local controller sends status messages like Insert, Takeout, Store etc. while they are processing the goods. Once the fetching of goods is done, they are delivered to the client.
 When more than one orders are received, a queueing algorithm will process them and will assign the automated guided robots (AGVs) nearby those relevant shelves in such a way that no collision will occur as well as no AGV will have an overloaded queue. This queueing algorithm will assure that all AGVs and robot arms are used efficiently. A separate algorithm will choose the shortest path for AGVs to reach their destination, minimizing the travel time.
 Hardware & Software Designs
 The current progress and the implementations of the project can be viewed from the following links:
 Hardware
 Software
 Network
 Bill of Material
 Related Links
 Department of Computer Engineering
 Faculty of Engineering
 University of Peradeniya","In this project, we are trying to address this issue by developing a fully automated warehouse management that will minimize the drawbacks while improving efficiency and profitability. We are implementing a warehouse management system which consists two types of robots; robots arms to handle loading/unloading of goods, automated guided vehicles to transport goods inside the warehouse. Also, an online shopping portal to make the purchases from the warehouse.",E16,Cyber-Physical Systems Projects,https://projects.ce.pdn.ac.lk/3yp/e16/smart-pharmaceutical-warehousing/,https://github.com/cepdnaclk/e16-3yp-smart-pharmaceutical-warehousing,https://cepdnaclk.github.io/e16-3yp-smart-pharmaceutical-warehousing,https://cepdnaclk.github.io/e16-3yp-smart-pharmaceutical-warehousing/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/3yp/E16/smart-pharmaceutical-warehousing/
75,smart pill manager,"Home - BrandSmart Pill ManagerProblem & SolutionServicesProduct DesignSolution ArchitectureHardware Design & ComponentsNetwork ArchitectureUsed TechnologyTestingBuget ReportTeam
 Smart Pill Manager
 Fun and secure way to take your medicinesGitHub Repo link
 Real life Problem and the Solution!Most of the people forget to take their medicines according to the prescription details. Due to those reasons following problems occur.Drug OverdoseDrug IntoleranceDisease get worse Can caused deathThis device will inform you when to take medicines and which amount of pills you need to take at that time More about Problem & SolutionServicesWhat We OfferWeb applicationYou can enter all the medicine and routine details using the web applicationPush notificationGet a email whether your loved ones are taking medication properlyAuthenticationEvery patient has to authenticate himself before taking the medicines Medicine TrackingGet a full medicine history
 Design
 Product Design
 Architecture
 Solution Architecture
 Hardware & Components
 Circuit Design
 Hardware Components
 Arduino Mega 2560
 RFID Receiver
 Fingerprint Scanner
 Speaker
 ESP8266 WI-FI chip
 480x320 LCD Display
 More about Hardware
 CAD DESIGNThis is our Fusion design of the smart pill manager device.INSIDE VIEWThe inside of the device has two parts. First one is circuit part. It is in the in front of the device. And second one is containers set.FRONT VIEWThe display of the device is  touch display. The inside of the device has two parts. First one is circuit part. It is in the in front of the device. And second one is containers set.CONTAINERS VIEWThis is the containers view. In our device has maximum 12 containers . There are in the backward of the device. Every container has LED Identifier.PCB Design
 Architecture
 Network Architecture
 TechnologyReact React is a lightweight front end library and it has own build packNodeJS we use the API as the node js backend. Node js is very easy to handle.MongoDB For the database, we use the mongodb database. It is no sql database. Therefore we can easily make request and responce to apiAzureWe deploy the our front end and back end in azure cloud platform.TestingMore about TestingEmbedded Testing
 Budget Report
 Team UNIVERSITY OF PERADENIYAAruna Nuwanthaco-founderBSc. Computer Engineering(undergraduate), University of Peradeniyae16261@eng.pdn.ac.lkChandula JPDMco-founderBSc. Computer Engineering(undergraduate), University of Peradeniya e16061@eng.pdn.ac.lkIsuru Lakshanco-founderBSc. Computer Engineering(undergraduate), University of Peradeniya e16203@eng.pdn.ac.lkADVISORSDr. Isuru NawinneSenior lecturerMr. Ziyan MarikkarSenior LecturerRelated LinksDepartment of Computer EngineeringFaculty of EngineeringUniversity of PeradeniyaGitHub RepositoryHomeServicesAboutTermsPrivacy PolicyCopyright  © smart pill manager 2021",This project is about a device which we called a smart pill manager to help patients to get their medicine according to the given routine. ,E16,Cyber-Physical Systems Projects,https://projects.ce.pdn.ac.lk/3yp/e16/smart-pill-manager/,https://github.com/cepdnaclk/e16-3yp-smart-pill-manager,https://cepdnaclk.github.io/e16-3yp-smart-pill-manager,https://cepdnaclk.github.io/e16-3yp-smart-pill-manager/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/3yp/E16/smart-pill-manager/
76,smart shopping cart with automatic bill system,"Smart shopping systemSearch this siteSmart shopping systemHomeProject introduction Shopping cartMobile/web appDesktop appDesignsSolution ArchitectureBackend
 and cloud deploymentTestingHardwareHardware programmingIntegrated SystemBudget TeamSmart shopping systemHomeProject introduction Shopping cartMobile/web appDesktop appDesignsSolution ArchitectureBackend
 and cloud deploymentTestingHardwareHardware programmingIntegrated SystemBudget TeamMoreHomeProject introduction Shopping cartMobile/web appDesktop appDesignsSolution ArchitectureBackend
 and cloud deploymentTestingHardwareHardware programmingIntegrated SystemBudget TeamSmart shopping cart with automatic bill system.-
 Let's
 have a better shopping
 experience
 Our MissionOur mission
 is to
 provide a more advanced
 shopping
 experience without
 congestion.Introduction definitions of the problem:Cashier takes time to billing so long queues at the counters in super markets.So that shoppers have to wait lot of time near the counter.Our the solution:smart shopping cart with automatic bill systemApp for online shoppingNowadays, shopping has become a mandatory part of everyone's life. The
 most unpleasant experience we have to face there is having to wait a long time in long queues
 to pay . Due to this , the congestion in the supermarket has also increased.
 As a solution to this, our team
 is introducing
 a smart shopping cart
 and a mobile app . The Smart shopping cart
 has the special ability of
 calculating its bill automatically .This is done by adding the value of the item to the current bill through the RFID readers in the cart when customers take it from the item rack and put it in the cart.The bill calculated in this manner has been given the opportunity to be paid in several ways . These include cash payments, credit or debit card payments and app payments . This saves customers time and gives customers a more convenient and attractive shopping experience . It also makes the supermarket a more pleasant place as it reduces congestion.
 In addition,customers can view product details ,get discount/ offers notifications ,make a shopping list ,view recent bill payments ,place online orders and do the bill payment through our mobile and web apps.
 We also provide a desktop application that allows supermarket management to add new items to the supermarket database ,manage customer online orders ,make customer bill payments and much more.Questions?Contact members with email,Ekanayake J.E.M.D.Y.
 :
 e16096@eng.pdn.ac.lk Parackrama G.T.W.
 : e16267@eng.pdn.ac.lk Prabodha U.A.K.
 : e16290@eng.pdn.ac.lk Visit our github repo
 ,
 github repo Copyright © Hi5 | All Rights ReservedPage updated Google SitesReport abuse",This project is about smart shopping system.The overall project comprises of a smart shopping cart which is capable of automatic billing.To manage the carts inside the shopping area  we introduce a desktop app for the supermarket management .We also introduced a mobile app and a web app which synchronises with the shopping cart  makes the shopping experience smarter.,E16,Cyber-Physical Systems Projects,https://projects.ce.pdn.ac.lk/3yp/e16/smart-shopping-cart-with-automatic-bill-system/,https://github.com/cepdnaclk/e16-3yp-smart-shopping-cart-with-automatic-bill-system,https://cepdnaclk.github.io/e16-3yp-smart-shopping-cart-with-automatic-bill-system,https://cepdnaclk.github.io/e16-3yp-smart-shopping-cart-with-automatic-bill-system/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/3yp/E16/smart-shopping-cart-with-automatic-bill-system/
77,smart vending machine,"Smart vending Home
 vending machine
 Store
 GithubRepo
 Sign In
 Close Menu
 Smart Vending Machine3rd year Project
 Home
 Solution
 Architecture
 User Interface
 Hardware Components
 Hardware Design
 Software Testing
 Datapath
 Designers
 Budget
 Contact
 Vending Machine Insight
 Solution.
 Problems in having a traditional vending machine are - Having to pay for the products with cash most of the time.Easy to hack Traditional vending machine.Prices and Expiry dates are not checked by the
 Traditional vending machine.
 the solution to all the problem is a Smart vending machine which has the (Gender ,Age ,generation wise) analysis , 24 hours Distribution and vending services , Transaction Database services.
 Which can be used to check the performance of a product in a specific market.Prices and expiry dates are real time because it is connected to the cloud.
 ×
 Software Architecture.
 The software architecture works like the above mentioned picture First user accesses
 the web application and he chooses the products and the interface was made by the Python Django
 and the details are updated in to the database which is made by MongoDB And it is al so in the
 server which is The Amazon EC3 instance All the communications are done through https.
 There is an API in the machine which connects the sever to get the Validation requests.Finally the Machine dispences
 the item chose by the user
 Back End Tasks Done
 User Registration (User /Admin/Companies)
 Where users are classified into who they are according to their Credential. These Roles can determine the access given to a selected user.
 Payment Handling
 Payment Handling is done through PayPal Which is a secured Payment Gateway
 Add /Modify/Delete Items
 The admin can Edit any information about the items available
 Validation
 QR code is used for more security and also Django Rest API is used for validity
 Transactions
 Every Transaction Done can be Seen by an admin .The User can also see the previous Transactions done by him.
 Server
 Local server is made using Python Django
 The database is made in PostGreSQL
 Communication to AWS is done through HTTPS
 Cloud Server
 Host in amazon EC2 Instance
 Nginix Server is used because Nginx is built to offer low memory usage and high concurrency.
 Rather than creating new processes for each web request, Nginx uses an asynchronous, event-driven
 approach where requests are handled in a single thread. With Nginx, one master process can control multiple worker processes.
 Gunicorn is used and it internally hands the calling of our flask code. This is done by having workers ready to handle the requests instead of the sequential one-at-a-time model that the default flask server provides.
 The end result is our app can handle more requests per second
 Built with Python Django
 Used database PostGreSQL
 Rest API
 To authenticate we are using JSON web token because it securely transfers
 information between software and Hardware as an JSON file.It has 2 tokens one is access token which expires within 5 minutes .
 We are using Rate limit to make sure the hardware/Web Application
 does not get overloaded by API calls to the cloud and it is only 60 API calls per hour.It ensure the safety of the systems.We can do GET,PUT,POST,DELETE in the APIs that are available in the system.
 User Interface.
 The Website gets you to this Home page which Can be used to directly buy items or you can sign in and buy things If you sign in the company can give you discounts or other options.And also from the Homepage you can go to the cart which has the items you selected and the total amount.Other than that you can also go to the tab pending orders and use the QR codes to get the paid items to dispense.
 This Page is the page you get if you want to Login.You can type in your login credentials and get into the account where you can see your previous orders and etcs. Or If you don't have an account you can go to the Signup option and Sign up for a new account.There is an option to help you reset your password as well if you forget.
 This is the SignUp page where you can Sign up for a new Account.The creditentials can be created here and there is a email checker and the email should be legitimate to sign in
 After Signing in you will have a page similar to the home page but you can see your account and you can make changes to your account when you click on your profile picture.And also you will have an Profile Button on your webpage where you can go and change your account settings.
 After you added the items your Cart the cart looks like this and you can edit the items that you are going to add here also.If you click check out it will take to a page where your can pay for the Items.You can use continue shopping to go back and add more items to the cart.
 When you go to the checkout page you can see that it will ask for a name and an email. It is just for the invoice so you can use it for refunds and other proceeds.
 After giving the Email and the name You can choose the way to pay (Paypal / Debit Card /Credit card).After choosing the payment method you will be redirected to a dialog box which is going popup and you can give your details there and pay for the items and you will receive the items.
 pop up will redirect you to Paypal
 If you Login with the admin Credentials you can see all the transactions that have happened with the vending machine over the
 time. And you can check who has bought the items and their characteristics according to their accounts.And also you can check the pending transactions.
 If you Login as an Admin You can also change the number of products in the vending machine.If you see any miscalculations. And also you can search for items according to their places and the prices and also the names.
 The other page you can check are the QR codes of the customers who are going to buy the items.
 Online Demonstration Video
 Hardware Components
 Raspberry Pi 3
 Broadcom BCM2837 64bit ARMv7 Quad Core Processor powered Single Board Computer running at 1.2GHz
 1GB RAM
 BCM43143 WiFi on board Bluetooth Low Energy (BLE) on board
 40pin extended GPIO , 4 x USB 2 ports 4 pole
 Stereo output and Composite video port Full size HDMI
 CSI camera port for connecting the Raspberry Pi camera
 Upgraded switched Micro USB power source (now supports up to 2.4 Amps) Expected to have the same form factor has the Pi 2 Model B, however the LEDs will change position
 Stepper Motor
 Motor Type: Bipolar Stepper
 Step Angle: 1.8 deg.
 Holding Torque: 40N.cm (56oz.in)
 Rated Current/phase: 1.7A
 Phase Resistance: 1.5Ohm±10%
 Insulation Resistance: 100MΩ¸ Min, 500VDC
 Insulation Strength: 500VAC for one minute
 Stepper motor driver
 stepper motor provides a constant holding torque without the need for the motor to be powered.Steppers provide precise positioning and repeatability of movement since good stepper motors have an accuracy of 3 – 5% of a step and this error is non-cumulative from one step to the next.
 Driver Model: L298N 2A
 Driver Chip: Double H Bridge L298N
 Motor Supply Voltage (Maximum): 46V
 Motor Supply Current (Maximum): 2A
 Logic Voltage: 5V
 Driver Voltage: 5-35V
 Driver Current:2A
 Logical Current:0-36mA
 Maximum Power (W): 25W
 Current Sense for each motor
 Heatsink for better performance
 Camera Module V2 for Raspberry Pi
 5 megapixel native resolution sensor-capable of 2592 x 1944 pixel static images.
 Supports 1080p30, 720p60 and 640x480p60/90 video.
 Camera is supported in the latest version of Raspbian, Raspberry Pi's preferred operating system.
 Relay Module
 High-sensitivity (250 mW) and High-capacity (16 A) versions
 Rated voltage 12 V DC
 Rated current
 20.8 mA
 Coil resistance
 576 Ω
 Must operate voltage 75% max. of the rated voltage
 Must release voltage
 10% min. of the rated voltage
 Max. voltage
 180% of rated voltage (at 23°C)
 Power consumption
 Approx. 250 mW
 Weight Sensor
 Differential input voltage: ±40mV (Full-scale differential input voltage is ± 40mV)
 Data accuracy: 24 bit (24 bit A / D converter chip.)
 Refresh frequency: 10/80 Hz.
 Operating Voltage: 2.7V to 5V DC.
 Operating current: < 10 mA.
 Size: 24x16mm.
 PIR sensor
 Input voltage: DC 4.5~20V
 Static current: 50uA
 Output signal: 0,3V (Output high when
 motion detected)
 Sentry angle: 110 degree
 Sentry distance: max 7 m
 120 degree detection angle
 Low power consumption in idle mode only 50uA
 and 65mA in fully active mode.
 This is the physical interpretation of the circuit diagram Which shows how the components are connected
 Hardware Design
 For the Convenience of the user the design was made to demonstrate which has all the attributes and more of a vending machine
 In the picture you can see a screen which is used to communicate with the user and there is a proximity sensor to make sure the vending machine only works when
 someone is present
 PCB Design
 Hardware Demontration
 The Schematic View
 For Demonstration purposes we have used proteus simulation platform here firstly the
 Vending Machine circuit is in OFF state and then if any person comes near the vending machine
 the PIR sensor detects the person and all the system comes to the on state Then the LED display shows
 the Welcome message asks to input the QR code.If the user adds the QR code (which has the details of the Transaction).
 the Vending machine sends a Validation request through an API call.If the QR code is valid then
 according to the Item list the motors according to the Item will rotate and the Items are delivered.The weight sensor makes sure
 if the items are delivered and the system goes to the power off State
 Data Path.
 The flow of data is projected above normally a user signs in or signs up with the user app
 after that user app sends the server data through HTTPs
 requests and also the response from the server also
 is through HTTPs responses similarly the API in the vending machine also sends the request as HTTPs and the response is through
 HTTPs.
 ER Diagram
 The Main entities are User,Customer , Product, Order, Order Item And QR Code.Every Entity has unique
 primary Key. And if we go in to details Customer Makes an order or multiple orders so the relationship between
 Customer and the Order is one to many.Customer can get multiple QR codes so there will be a one to many relationship.likewise one order can have multiple
 order items that is also a one to many relationship.For Order to QR Address the relationship is 1 to 1.
 For products and Order Item its 1 to many.
 Software Testing.
 URL unit Testing
 We are checking to make sure every Url gets the reverse Match.By using the dummy http request
 POST/GET
 Request unit Testing
 CRUD operations are Checked using artifical AJAX API calls for that we are using Hard coded Json data
 We are sending in dummy data to check the reliability of the authendication
 Form Validation Testing
 Authetication form is
 checked
 We are creating a new user and database and checking the form
 Post request is sent and check if it is updated
 new products are created in example to check where all of them are being updated
 Testing Results
 22 checks were done for end point reverse matches all were passed.We failed to check the QR code at the first time but after some minor changes QR code got accepted by the test
 and also 15 tests were done one of them were checking QR codes
 Designers.
 Karikaran Vettirivel
 Girishikan Selvaratnam
 Bragadeeshan Suppusamy
 Budget.
 Contact.
 Do you want us to style your home? Fill out the form and fill me in with the details :) We love meeting new people!
 Name
 Email
 Message
 Send Message",Smart Vending Machine (3rd-year unified project) This is a 3rd-year project Developed by a group of students to address the problems faced in traditional vending and help consumers have a relatively safer and easier to handle and access. This Smart Vending machine is connected to the cloud to make sure every transaction is Live A web application is devoted to helping the consumers see what they want and can order the items they want to buy The web application is very interactive where the users can sign up and make an interactive accounts where they can do changes and track their buying habits,E16,Cyber-Physical Systems Projects,https://projects.ce.pdn.ac.lk/3yp/e16/smart-vending-machine/,https://github.com/cepdnaclk/e16-3yp-smart-vending-machine,https://cepdnaclk.github.io/e16-3yp-smart-vending-machine,https://cepdnaclk.github.io/e16-3yp-smart-vending-machine/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/3yp/E16/smart-vending-machine/
78,waiterbot system,"WaiterBot System
 WaiterBot System
 Home
 Solution Architecture
 Design
 and Progress
 WaiterBot
 WaiterBot API
 Mobile Application
 Web Application
 Desktop Application
 Demonstrations
 Documentation
 Testing
 and Deployment
 Software Testing
 Hardware Testing
 Cloud Deployment
 More
 Team
 Budget
 Github Repo
 WaiterBot System
 Dine with Technology!
 Robots have seen a wide array and continuous applications in various industries since their
 inception. The efficiency and versatility that robots possess can be molded into a vast area of
 services, and robots in restaurants can step up to be a huge breakthrough in terms of customer and
 owner satisfaction and improving the overall experience in diners.
 The WaiterBot System is an automated system designed for placing and delivering orders in a
 restaurant. This system will replace the human waiters with robot waiters for an efficient delivery
 process and also give customers a new experience. Customers can place orders via the mobile
 application and once the orders are ready, the WaiterBots will deliver the orders to the customer.
 Current Practice and the Problems
 Currently in restaurants food is delivered by human waiters. Even the order placing is done through a
 waiter. You may have encountered situations where the waiter takes a long time to take the order and
 deliver it. The customers may also keep complaining that the service is not satisfactory. There may
 also be instances where the waiters might mess up the orders or not deliver the order to the correct
 customer. Due to these situations the reputation of your restaurant might be tarnished, and worse,
 if a customer faces these problems he/she will not visit your restaurant again because the service
 is not satisfactory and hence you will lose customers.
 Usually a customer placing the order will have to select the food items using a traditional menu card
 and might have questions when selecting food items like whether the selected item is good enough in
 terms of taste, quality, etc... Also, once the orders are placed, if the ordered item is not
 available due to various reasons, the waiter will have to inform the customer and the customer will
 have to order some other item, resulting in irritation of the customer in some cases as well. This
 order placing and delivery process may not be much efficient especially if the restaurant is busy
 with customers.
 Our Solution
 Our solution is to replace the human waiter with a robot waiter and also to replace the traditional
 menu cards system with a more attractive and efficient order placing system. On an event where a
 customer visits the restaurant he/she can place the order via the order placing system and once the
 ordered items are ready, the items will be delivered to the customer. Our solution will help the
 customer to select the food items more efficiently with help of the reviews from previous customers
 and also if an item is unavailable that item will not be shown in the menu. The WaiterBot system
 will help the restaurant by providing an efficient delivery mechanism. WaiterBots will not mess up
 orders and they will deliver the food items to the correct table. Also this may be a new experience
 for the customers and the WaiterBot system will attract more customers to the restaurant.
 Links
 Home
 Solution Architecture
 WaiterBot
 WaiterBot API
 Mobile Application
 Web Application
 Desktop Application
 Demonstration
 Documentation
 Software Testing
 Hardware Testing
 Cloud Deployment
 Team
 Related Links
 Department of Computer Engineering
 Faculty of Engineering
 Univerity of Peradeniya
 © Copyright 2020 WaiterBot System","The main idea of this project is to develop a system of robots and a smart ordering system for a restaurant.  Current system in restaurants, the traditional menu card ordering and waiters can be replaced with the WaiterBot system. The WaiterBot system contains main 4 parts. The WaiterBots( Main hardware component for delivery of food items), mobile application(to place orders and make payments), web application(for the management of the entire system) and the desktop application(for operation of the robots and handling the orders).",E16,Cyber-Physical Systems Projects,https://projects.ce.pdn.ac.lk/3yp/e16/waiterbot-system/,https://github.com/cepdnaclk/e16-3yp-waiterbot-system,https://cepdnaclk.github.io/e16-3yp-waiterbot-system,https://cepdnaclk.github.io/e16-3yp-waiterbot-system/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/3yp/E16/waiterbot-system/
79,water quality monitoring and usage monitoring system,"Project site
 Toggle navigation
 ×
 Github Repo
 Water Quality and Usage Monitoring Device
 Check your tank with care
 Intro
 How It Works
 Team Members
 Components and Technology
 Advisors and Mentors
 Work Done So Far
 Intro
 Problem
 Water mismanagement and consumption of poluted water is a major challenge, the world is facing currently.
 Current market is yet to address this issue with an effective solution. The solutions that are currently
 present on the market are either too expensive for domestic usage or does not meet the user expectations.
 Solution
 To overcome these issues, we are propossing a device which can be used to measure the water quality of the
 inlet of a water tank and the water level inside the tank. We expect to target both domestic user requirments
 and industrial user requirements by a reasonable price (click here for budget) User can access to the data using a mobile application.
 Water level Monitoring
 By using an ultrasonic sensor, water level will be monitored and when the tank is nearing to finish filling, user
 will be notified. We use ultrasonic sensor because of it's high accuaracy and sensor is not easily effected by
 water. Also this will generate the usage reports of the water and allow user to observe the current water level
 in the tank if needed. This measurement also allows to rationing of water and user will be warned in case of overusage.
 Water Quality Monitoring
 TDS count – by electric conductivity of water
 The presence of dissolved solids in water may affect its taste.The palatability of drinking-
 water has been rated by panels of tasters in relation to its TDS level as follows:
 excellent, less than 300 mg/litre;
 good, between 300 and 600 mg/litre;
 fair, between 600 and 900 mg/litre;
 poor, between 900 and 1200 mg/litre;
 unacceptable, greater than 1200 mg/litre
 Water with extremely low concentrations of TDS may also be unacceptable because of its flat,
 insipid taste. Click here for see WHO guidelines for drinking water quality
 Turbidity – by measuring scattering of light in water
 Turbidity, which is caused by suspended chemical and biological particles, can have both water safety and aesthetic implications
 for drinking-water supplies. Turbidity itself does not always represent a direct risk to public health; however, it can indicate the
 presence of pathogenic microorganisms and be an effective indicator of hazardous events throughout the water supply system,
 from catchment to point of use. For example, high turbidity in source waters can harbour microbial pathogens, which can be
 attached to particles and impair disinfection; high turbidity in filtered water can indicate poor removal of pathogens; and an
 increase in turbidity in distribution systems can indicate sloughing of biofilms and oxide scales or ingress of contaminants through
 faults such as mains breaks.WHO says that for dinkable water turbidity must be below 5NTU and should be low as possible like 0.1NTU
 Click here to download the review done on the turbidity by WHO.
 For these 2 measurments, TDS sensors and Turbidity sensors will be used and their readings will be analyzed with ideal water
 quality levels and determine the water quality of the incoming water to the tank during a filling and notify the user if there
 is any contamination in the water. We use these 2 sensors because by using these 2 readings we can determine most of the
 characteristics of water.
 How It Works
 This diagram shows the overall implementation of the system.There are sensors to get the data, server to handle the clients.The system works as below steps.
 Sensors reads the data
 Sends the data to the server
 Server sends the data to the clients
 According to above steps, firstly sensors read the data. The ultra sonic sensor is situated inside the tank and the turbidity sensor is situated inside the water pipe which uses to fill
 the tank. The readings from these sensor are sent to the atmega chip and by using a wifi module those readings are sent to the server. From server, relavant data sends to clients accordingly.
 Client may be a mobile user or pc user. When consider the mobile users, by using the mobile app, clients can check the level of water filled to the tank and also can see the water purity percentage.
 When consider the pc users, they can log into the website and get the above details. There is an alarm module to function without any internet connection which is helpful when the client has no internet access.
 And it can be implemented according to the clients' desire. Clients can also get daily usage details via the website.
 Team Members
 Harshana Bandara
 E/16/049
 S.D.M.V.G.H.N.Bandara
 Department of Computer Engineering
 Faculty of Engineering
 University of Peradeniya
 Yasitha Herath
 E/16/134
 H.M.Y.B.Herath
 Department of Computer Engineering
 Faculty of Engineering
 University of Peradeniya
 Thushara Weerasundara
 E/16/388
 W.M.T.M.P.B.Weerasundara
 Department of Computer Engineering
 Faculty of Engineering
 University of Peradeniya
 click on images to visit github pages of the members
 See more..
 Components and Technology
 HTML , CSS and JS
 HTML is a simple and dominant language to formatting web pages.It supports every browser .HTML Combined with CSS and JS can be used to give client an elegant and yet fast experience
 Flutter
 Flutter is free and open source UI framefork which is able to cross platform developments.The language used in
 flutter is DART which is optimized for UI development.
 MQTT
 Protocol which allows efficient data distribution with lightweight overhead and with high sclabability
 Reduce bandwidth consumption so the client can have maximized available bandwidth.
 MongoDB
 High scalability and better for cloud services
 NodeMCU
 Provides the connectivity between the device and server.low cost which reduce the prie of the product and low energy consumption
 which is essential to a long battery life of the devise.
 Ultasonic Sensor
 Reliable hardware for measuring short distances with a sufficient accuracy
 Mentors
 Dr.Isuru Navinna
 Senior Lecturer
 Dr.Zian Marikkar
 Senior Lecturer
 Work Done So Far
 GUI prototype
 GUI was designed with the aid of figma.Click herefor experience the GUI
 Circuit Designs
 Click this link to see more about our Circuit Design
 Hardware Testing and Simulation
 Hardware Components are simulated in Tinkercad. click here to see more
 Our Front End Implementation
 Web Site
 Our web site UI is designed to give a user friendly experience to the customer in a very simple manner.
 It uses dynamic web pages to interact with users therefore fast responses can be obtained with less data
 usage.
 Role Based Access
 This offers different user expirience to the user depending on the role of a user.
 3 roles in our system are,
 Super Admin - Give access to Admin Users
 Admin - Supervise Users (Customers)
 User - Customers
 Implementation
 Uppon registration to the service, each user should confirm their account using their email account by clicking on a generated url
 User actions are based on their roles and authorization middleware monitor and control those ations accordingly.
 authorization middleware make sure,
 Verify / Idenify users and roles
 Users don't access unauthorized locations and perform unauthorized actions
 Redirect users on unauthorized locations and unauthorized actions
 Customers can login and monitor their devices. They can monitor,
 Daily Usage of water
 Monthly Water Usage reports
 Water Quality of their tanks
 Admins can monitor the system and manage customers, also they can provide customer support.
 Their tasks include,
 Send notices to users
 Edit and remove customers
 Monitor devices with issues
 Super Admins can allow access to the admin users to the system and remove admins.
 Web Site Views
 1. Customer UI
 Log In
 Daily Usage
 Monthly Usage
 Sign Up
 Getting User Details
 Demonstration
 2. Admin UI
 Admin View
 Delete a User
 Update a User
 Demonstration
 3. Super Admin UI
 Admin View
 Create Administrative User
 Delete an Admin
 Demonstration
 Mobile App
 App with limited access for customers
 App uses validate methods when a user does a login attempt to avoid un-necassary requests to the server
 Login Screen
 Home Screen
 Tank Details
 Invalid email
 Invalid Password
 Tank info
 Tests Conducted
 1. Penetration Testing
 Several Penetration Tests were carried out in order to check vulnerabilities in our backend
 system. Tools used on those tests are
 Wire Shark
 BURP Suite
 Dirb
 Nmap
 Sqlmap
 Nilto
 Results
 Using wire shark we conducted packet sniffing tests. As a result of that we observed information exposed in requests.
 Using Dirb we tested using a brute force attack to check vulnerabilities in every directory/object of our website and server.
 We managed to identify exposed endpoints by the test.
 Using BURP Suite and Sql map we tried to conduct a sql injection attack but attack was failed due to security measures in our
 server.
 Using nmap and nikto we tried to obtain information about the web server and it's components. We managed to get server type, OS,
 open ports, vulnerabilities in our content policies, server capabilities. These information could be used for DOS, DDOS, sql injections
 Packet Sniffing
 End Point Responses
 Security Headers
 OS information
 Security Headers
 2.Unit and integration Testing
 Unit test are carried out to check the functionality of the seperated functions and integration tests are used to test user creation and roles, to test sensor
 readings storing and validity.Below tools are used to test these.
 Mocha
 Chai
 Tests
 Results
 Conclusions
 Only the users with unique and valid credentials can sign up
 Only the admin can get the signup data, redirection to the adminSignup view
 Only the valid sensor data is stored , guarantee the quality of the sensors
 Any user with different roles can log by giving correct credentials
 We implemented our security measures using these results.
 Security Aspects
 https server
 Using self signed certificates using open ssl for devolepment purposes. This protects sensitive
 information by encrypting, thus protects from outsiders.
 Using Credentials to Authenticate Users
 Users are authenticated by using passwords. These passwords are hashed therefore even the database is
 compromised, passwords are safe therefore attacker cant use it.
 Using cookies for Track Sessions
 For successfull authentication of a user, user will be issued a jwt tocken. Using that token user will be
 authenticated. Since tocken is sent via https it is secured. Also secure tag and httponly tags are set therefore
 no one can manipulate the tockens from browsers and use undetected.
 Role Based Authorization
 Server uses middleware to authorise users in server. Therefore each user will be limited to actions pre defined
 for his corresponding role.
 Setting Rate Limits Per User
 By setting x rate headers, number of requests from a user are monitored and limited. If rate is overflowed 429 status
 is returned. Thus protecting from dos attacks.
 Request burst tracking
 Server tracks number of requests received in a given time. If the number of requests go pass that number, server will
 take predefined necessary actions. This gives protection from ddos attacks.
 Hiding Server Information
 By hiding server type, OS, database information and capabilities server has protection from attacks sepecially designed
 for our server. Also using content policies we can have protection from sql injection attacks.
 Embeded Hardware Designs
 Components List
 Ultra Sonic Sensor - U1
 Water Level Monitoring.
 ATMEGA328P chip - U2
 Microcontroller chip.
 DC-DC step down Buck converter - U3
 Voltage Controller.
 Li Battery Charger - U4
 Battery Charging module.
 Turbidity Sensor - U5
 Turbidity Measurement of Water.
 TDS Sensor - U6
 TDS Measurement of Water.
 Flow Meter - P1
 Water Usage Monitor.
 NODEMCU - MK1
 Internet Connection Module.
 Speaker - SP1
 Warning System.
 Reset Button - SW1
 Power Button - SW2
 Designs made for fabrication
 Embeded System
 Water Quality Monitoring Module
 Water will enter this module and Quality Measurements will be recorded. Readings will be taken as analog inputs.
 Water Usage Monitoring Module
 Water Usage will be recorded using this module. Readings will be taken by external inturrupts in microcontroller chip.
 User Expirience Optimization
 Using Solar Power
 Ability to Perfrom during Blackouts
 Protection from Environmental Conditions
 User Freindly UIs
 Faster Response time due to dynamic web pages
 Scalability
 Using Nginx to run multiple servers to load balancing.
 Using Proactive scalling features in MongoDB to scale database to support high traffic events.
 Each user can monitor multiple devices using a single app.
 Reliability
 In case of a server failure, another server will responds to requests.
 Thoroughly calibrated and tested sensors.
 Cross platform application.
 Plan For The Embeded Systems Testing
 There are two parts of our embedded system testing plan.Those parts are showed below.
 Hardware Testing
 Node Software Testing
 Aim is to ensure the reliability of our product by doing these tests.
 Hardware Testing
 In this we do tests when soldering and to test the functionality of the sensor devices.
 When soldering we do below steps,
 Soldering and testing the power sections one by one
 Solder the relavant wires from the sensors with microcontroller and test the basic functionality
 check the connectivity
 To check the connectivity we use oscilloscope or logic analyzers.
 To do the sensor testings for turbidity sensor and TDS count sensor we use glasses with water which desolved with different kind of substances as below.
 By dipping the sensors inside the water glasses we test the functionality of the sensors and how the readings change according to the difference of water. By those readings we can calibrate the sensor readings accordingly. And also we can detect if the sensors are deffected by checking the sensor readings.
 To test Ultrasonic sensor we can use above glasses with different water levels and do the same tests.
 Node Software Testing
 In this testing we do test to check variable overflows, compiler optimizations and typing mistakes.
 We use below techniques to do the debugging.
 Debug breakpoints
 Disassembly window
 Call stack window
 And also we are planning to check the real time sensor readings are showing on our web site and the app. By this we can make sure that the interaction between node software and backend works correctly.
 Embeded Software Explanation
 This device utilizes the services of Arduino UNO R3 board which has ATMEGA328P U chip and NodeMcu ESP-12E board for WiFi connection.
 Let's consider the working procedure of the device step by step.
 Device Initialization
 When the device is powered up, NodeMCU will creates a WiFi server on port 80 under the url of www.aquawatcher.com using ""DNS"" and ""WebServer"" modules. We can connect to that network and set up user details and tank details. Then we can enter our WiFi network details as well. Those details will be stored in EEPROM memory of the device until they are updated. Therefore user won't have to set those values again on reboot.
 We can reset the above details by pressing reset button and going through the same procedure as before.
 Thereafter, server will be closed down and normal procedure of the device will occur.
 Working Procedure
 Water Level will be measured using Ultra Sonic sensor using an analog input pin. This will be used to get tank size as well.
 Water Usage will be measured using a Flow Meter using external inturrupts.
 Water Quality will be measured using TDS and Turbidity Sensors using an analog input pins.
 All the above modules will use their own custom libraries.
 Readings will be taken periodically.
 When the water is contacted with the sensors, device will identify water is filling.
 Then the device will start sending new data to the server.
 When an issue occurs regarding water level or quality, an alarm will be set off using Tone function.
 Water quality standards are preseted values.
 Communication between 2 boards will be done using serial communication. Extra pins in NodeMCU board was set as serial pins using Softwareserial library.
 Solar panels are intergrated as power source.
 Device Installation
 Water Level Monitor
 Water Quality Monitor
 Water Level Monitor
 1st empty the tank
 Then Install the ""Water Level Monitor"" inside of the closing lid of the tank. Make sure it is levelled
 Water Quality Monitor
 1st remove the inlet of the tank.
 Then Install the ""Water Quality Monitor"" to the inlet and then insert inlet pipe to the back of the device.
 Make sure to expose the device to direct sun light if you choose to use our solar panel intergrated version..
 Getting Started
 There are two parts of setting up the device.
 Device Initialization
 Connect to WiFi network
 Device Initialization
 1st we need to store user email and tank number in the device. 1st connect the device to the power, then connect with ""AquaWatcher device Setup!"" wifi network. Then go to the address www.aquawatcher.com and provide information.
 It will be convinient to you if you installed our ""AquaWatcher"" app on your android/iOS device as well.
 Connecting to WiFi
 Then wifi network ""Aqua Watcher"" will be available and connect to it. Then go to manage wifi network and enter your wifi network information.
 Start Operation
 You can now start to fill the tank.
 You don't have to keep monitor the device. It will send notifications when,
 Tank is almost full
 Water pollution is detected
 Tank is nearly empty.
 Water cuts and relevant details regarding to water suplyer.
 You can monitor your devices via our website www.aquawatcher.ml as well.
 Our services will include direct interactions with National Water Supply and Drainage Board for your convinience as well.
 Cloud Deployment
 A Node.js based server is deployed in an aws ec2 instance.
 URL of the website is www.aquawatcher.ml .
 2 webservers are handling requests and they are configured using nginx. Round robin method is utilized.
 Admin access is restricted to certain ip addresses and ports to prevent unauthorized requests to admin endpoints.
 Database is a MongoDB implementation.
 Database access is restricted to unknown ip addresses.
 Database takes snapshots of it self to keep backups.
 Future Plans
 Water leakage detection functionalities.
 Extend the compatibility to other chemical types to install with chemical tanks.
 Set automatic valves.
 Improve app and server architecture.","In this project, we are developing a water tank monitoring system for providing clean water and reduce the water wastage. Also users will get notifications about related news like water cuts. Mainly our project focused to monitor those factors in very user friendly way. By using a simple mobile app or using a web site it can be done. This will initiate a different way of water monitoring instead of using traditional methods. ",E16,Cyber-Physical Systems Projects,https://projects.ce.pdn.ac.lk/3yp/e16/water-quality-monitoring-and-usage-monitoring-system/,https://github.com/cepdnaclk/e16-3yp-water-quality-monitoring-and-usage-monitoring-system,https://cepdnaclk.github.io/e16-3yp-water-quality-monitoring-and-usage-monitoring-system,https://cepdnaclk.github.io/e16-3yp-water-quality-monitoring-and-usage-monitoring-system/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/3yp/E16/water-quality-monitoring-and-usage-monitoring-system/
80,Accelerating Adaptive Banded Event Alignment Algorithm with OpenCL on FPGA,"Projects | Department of Computer Engineering
 Projects - Department of Computer Engineering
 University of Peradeniya, Sri Lanka
 Accelerating-Adaptive-Banded-Event-Alignment-Algorithm-with-OpenCL-on-FPGA
 Team
 E/15/123, Wishma Herath, email
 E/15/280, Pubudu Premathilaka, email
 E/15/316, Suenth Samarasinghe, email
 Supervisors
 Prof Roshan Ragel, email
 Dr Hasindu Gamaarachchi , email
 Table of content
 Abstract
 Related works
 Methodology
 Experiment Setup and Implementation
 Results and Analysis
 Conclusion
 Publications
 Links
 Abstract
 Nanopore sequencing is a third-generation sequencing technology that can analyze long DNA, RNA fragments in real-time. It measures the change in electrical current as nucleic acids pass through a protein nanopore. The Nanopolish software package utilizes the aforementioned signal level changes to obtain useful results in oxford nanopore DNA sequencing. Adaptive Banded Event Alignment (ABEA) is a dynamic programming algorithm used in nanopolish software packages to polish sequencing data and identify nano-strand nucleotides such as measuring DNA methylation. Prior investigations show that ABEA consumes 70% of total CPU time in nanopolish. Thus, optimizing the ABEA algorithm is vital for nanopore sequencing applications. Previous work has deployed accelerated version of ABEA on GPUs using CUDA and has gained improvements on execution time but with high power requirement. With the advancements of HLS (High-Level Synthesis) tools, FPGAs (Field Programmable Gate Arrays) are emerging as accelerators in the field of high performance computing that gives reasonable runtime performance while consuming less power.In this work, we induce a modified version of ABEA for FPGAs using OpenCL. We experimentally identify and adapt optimization techniques to achieve better performance on DE5-net FPGA. We show that our implementation is able to archive
 energy consumption of 43% of the previous implementation of ABEA on GPU (f5c). Further, we present performance comparison of our implementations with other different implementations on different platforms in terms of execution time and energy consumption.
 Introduction
 DNA can be defined as molecules that encodes the genetic instruction of humans and almost all other organisms. DNA sequencing is the process of identifying the order of the four chemical building blocks called ‘bases’ that make up the DNA molecule. These four bases are adenine (A), thymine (T), cytosine (C) and guanine (G). A rapid DNA sequencing technology is beneficial in different applications including the ability to act preemptively before disease development and commence treatment.
 The latest sequencing technologies generate data in the order of terabytes. In particular, the MinION sequencer manufactured by Oxford nanopore technologies has the potential to generate around TB of raw signal data during a typical sequencing run. This higher data throughput of sequencers has become a crucial challenge since it requires high computational power to process data.
 Nanopore sequencing is a third-generation DNA sequencing technology that does not need sample amplification and it offers several advantages over other sequencing technologies. Some of the significant improvements are the ability of long-read sequencing, de novo sequencing and real-time analysis.
 Nanopore sequencing measures ionic current variation as the DNA molecule passes through the nanoscale pore. This ionic current variation is used to identify each base as it passes through the pore. The ‘base-calling’ is the process of converting the raw signal into character representations of DNA bases (e.g. A, C, G, T). To overcome the base-calling errors, the raw signal is aligned to a biological reference sequence and this process is called ‘polishing’. One of the pivotal algorithms used for polishing in nanopolish is Adaptive Banded Event Alignment (ABEA) which uses a dynamic programming strategy to align raw signal to a biological reference sequence.
 The GPU-based HPC systems are favorable architectures for data parallelism with higher memory bandwidth. In comparison to GPUs, modern FPGAs provide reasonable processing speed while consuming a fraction of GPUs’ operating power. Compared to multi-core CPUs, choosing an FPGA accelerator is favorable because of its broad performance improvement from one generation to another. Therefore, higher performance and superior power efficiency result in increased performance-to-power-efficiency of FPGAs compared to GPUs and CPUs.
 DNA sequencing is an emerging field. Therefore, it is crucial to reduce the total implementation time due to several factors like technological improvements. Traditional Hardware Description Languages (HDLs) such as Verilog hardware description language have been a design bottleneck for FPGA accelerators over the past years. Using an HLS tool like OpenCL massively reduces the designing and programming time.
 OpenCL platform model shown in below is an abstract hardware model for devices. One platform has a host and one or more devices connected to the host. Each device may have multiple Compute Units (CUs) with multiple Processing Elements (PEs).
 OpenCL Platform Model
 The OpenCL memory model can be divided into two major memories host and device. Host memory is accessible by only the host, and the device memory accessible by kernels executing on OpenCL devices. The device memory can be further divided into global memory (shared by all the work-groups), constant (read-only memory for the device) memory, local memory (shared by all the work-items in a work-group), and private memory (specific to each work-item).
 OpenCL Memory Model
 OpenCL supports two types of kernels, namely NDRange kernels and Single-Work-Item (SWI) kernels. In the NDRange kernel, OpenCL generates a deep pipeline as a computing unit. All the work-items from all the work-groups execute on that pipeline. The compiler automatically performs work-group pipelining. NDRange kernel has a similar thread hierarchy to CUDA. Each thread is called a work-item, and multiple work-items are grouped to form a work-group. In the SWI kernels, the entire kernel is run by a single work-item, and loop iterations are pipelined to achieve high performance. Initiation Interval (II) is the number of hardware clock cycles a pipeline must wait for before launching the successive loop iterations.
 Related works
 Previous research, which is done under the objective of accelerating ABEA, deployed an accelerated version of the algorithm on GPUs using CUDA. Their implementation is referred to as f5c-gpu. They have achieved 3-5x performance improvement on the CPU-GPU system compared to the original CPU version of the nanopolish software package. Rucci et al. has presented Smith-Waterman (SW) implementation, which is capable of aligning DNA sequences of unrestricted size. In this work, the kernel is implemented using the task parallel programming model. Rucci et al. SW kernel, has exploited inter-task parallelism. They have utilized the SIMD (Single Instruction Multiple Data) vector capability available in the FPGA.
 Methodology and Implementation
 The host program executes on the CPU and loads the dataset into the main memory. Then, it programs the FPGA, allocates buffers, sets kernel arguments and finally launches the kernels. When FPGA finishes the execution, the host program reads the results from FPGA and stores them in the main memory. The CPU and FPGA use direct memory access (DMA) to transfer data through PCI-e bus.
 OpenCL supports two types of kernels, namely NDRange kernels and Single-Work-Item (SWI) kernels. In the NDRange kernel, OpenCL generates a deep pipeline as a computing unit. All the work-items from all the work-groups execute on that pipeline. The compiler automatically performs work-group pipelining. NDRange kernel has a similar thread hierarchy to CUDA. Each thread is called a work-item, and multiple work-items are grouped to form a work-group. In the SWI kernels, the entire kernel is run by a single work-item, and loop iterations are pipelined to achieve high performance. Initiation Interval (II) is the number of hardware clock cycles a pipeline must wait for before launching the successive loop iterations.
 A. NDRange Kernel Implementation
 For our NDRange kernel implementation, we followed the GPU approach taken in f5c-gpu and re-engineered it to evaluate the performance of OpenCL implementation on FPGA. We broke the main kernel into three sub kernels, namely pre, core, and post. We tried to achieve the maximum benefit of hardware resources and optimal work-group configuration by splitting the kernel.
 B. Single-Work-Item (SWI) Kernel Implementation
 The implementation of SWI kernels is very similar to a typical C program written for CPU and they are best suited for implementing deeply pipelined algorithms. They contain loops. Each loop-iteration is used as the unit of execution of a kernel. Therefore, multiple loop-iterations are computed in different pipeline stage in parallel.The ABEA algorithm can be divided into three main steps. Initialization of first two bands, filling the cells with score value for the rest of the bands, and finally traceback step which finds the best event-space alignment. Out of these three, the second step is highly compute-intensive.
 The first step initializes bands and trace arrays, initializes the first two bands and fills an array called ‘kmer_ranks’. This array is required in later computations. Rank for each kmer in the sequence is determined by assigning a weight for each base and shifting according to the place of the base within a kmer. This for-loop can be pipelined with an initiation interval of 1 since there are no data or memory dependency between two iterations.
 The second step calculates the rest of the bands (b2, b3,..) while moving the adaptive band according to the Suzuki Kasahara rule. Calculation of the current band depends on the previous two bands results. Therefore, the loop has to be serially executed. An inner loop always goes through the band and fills the cells within a band. This loop can be pipelined with a minimum initiation interval of 1 due to the absence of data or memory dependency between loop iterations. The final traceback step consists of a loop with high data dependency between two loop iterations. This behavior results in pipelines with an initiation interval of almost the latency of the pipeline stage. Therefore, it is equivalent to serial execution, which is more suitable for running on a CPU than a SWI kernel on FPGA. According to the above observations, we merged the first step and second step to build a deeply pipelined SWI kernel. Then CPU performs the traceback step. Following figure shows a pipeline diagram including only the main for-loops in the kernel. Computations related to a new read starts its execution in every clock cycle, set of bands in a read executes in a serial manner due to unavoidable data dependencies, and a new cell inside a band starts its execution in every clock cycle.
 Pipeline Diagram
 Pseudo Code for SWI Implementation
 Experiments and Results
 Experiment Setup
 Table below shows specifications of hardware accelerators and the host PC used to obtain results.
 Dataset
 The experimental data set is a subset of publicly available reads aligned to a 2kb region in the E. coli draft assembly and publicly available NA12878 (human genome) ‘‘Nanopore WGS Consortium’’ sequencing data.
 The datasets used for the experiments, their statistics (number of reads, total bases, mean read length and maximum read length) are listed below.
 Performance Results
 Detailed analysis of all the loops in SWI kernel is shown below. Apart from the three of the main for-loops mentioned above, other loops are fully unrolled when the lower and upper bounds are constant for each iteration of its outer-loop. Rest of the loops are made to execute in a pipeline manner with an initiation interval of 1.
 Table below shows the estimated resources used by SWI kernel in the design, all channels, global interconnect, constant cache, and board interface compiled for DE5-net FPGA.
 Figures below show the execution time of each implementation and the power consumption of each implementation.
 The observations can be analyzed and justified as follows.
 Eventhough NDRange kernels on FPGA have a lesser power consumption than GPU implementations, they reported a higher execution time. Therefore, they are ranked at 7 and 8 in terms of the energy consumption.
 Usually, f5c-gpu allocate a set of very long reads selected according to a heuristic to be computed on the CPU and the rest of the reads on the GPU. It results in around 50 seconds of execution time on Tesla K40. But, here we force the f5c-gpu implementation to compute all the reads only on the GPU (cuda-k40). We observe that cuda-k40 and ocl-k40 perform almost at the same level.
 Unlike in FPGAs, in NVIDIA GPUs, Our NDrange OpenCL implementation executes in a similar programming model to CUDA, and it works as a SIMD. When considering CUDA and OpenCL, there are minor differences. The reason for slight execution time degradation in the ocl-k40 could be the kernel compilation during the execution time.
 Due to the lesser execution time of cuda-k40, it outperforms the energy advantage of cpu and gets ranks 4 and ocl-k40 gets rank 6.
 As mentioned, since the CPU’s power requirement is lesser than that of the GPU, based on the energy consumption, the cpu implementation gets rank 5.
 Among SWI implementations, kernels with suitable FPGA specific optimization techniques shows an improved the performance in execution time and power consumption which lead to less energy consumption. Hence, swi-opt-2 implementation is in rank 1 and others get rank 2 and 3.
 Among NDRange implementations on FPGA, decomposition of kernels into too many kernels results in poor execution time eventhough the power consumption (estimated for DE5-net) is the same.
 Among FPGA implementations, all SWI kernels (swi-) perform significantly better than NDRange kernels on FPGA (nd-) in terms of both execution time and power consumption. The best SWI kernel is 2x faster and consumes only 34% of the energy compared to the best NDRange kernel.
 As shown in Figure, In terms of execution time, GPU implementations (both cuda-k40 and ocl-k40) perform better and 4x faster than swi-opt-2 on DE5-net.
 However, In terms of the energy required to perform ABEA on the same dataset, SWI kernel implementations on FPGA are in lead. swi-opt-2 on DE5-net needs only 43% of the energy consumption of the GPU implementation on Tesla K40.
 Conclusion
 The Adaptive Banded Event Alignment algorithm is an improved version of DNA sequencing, which is extensively used in nanopore DNA sequencing. In the previous work, this algorithm has been parallelized and run efficiently on GPUs.
 In our work, we introduce several implementations of the ABEA algorithm using OpenCL to run on FPGA. We evaluate the performance of the implementations in terms of runtime and energy consumption.
 Among FPGA related implementations, SWI kernel with suitable FPGA specific optimization techniques performs better than other FPGA implementations including NDRange kernel.
 In terms of runtime, GPU implementations (both CUDA and OpenCL NDRange kernel) on Tesla K40 perform better and 4x faster than FPGA implementations on DE5-net.
 However, in terms of the energy consumption to perform ABEA on the same dataset FPGA implementations are in lead. FPGA implementation on DE5-net needs only 43% of the energy consumption of the GPU implementation on Tesla K40.
 Through out our work, we identified the potential and ease of using HLS over traditional methods for hardware programming. We used DE5-net FPGA with OpenCL 18.0 for experimenting and evaluation of results. It is a mid-range hardware compared to the state-of-the-art.
 The maximum predicted frequency we got for the kernels was around 250 Hz and it is even lesser at the execution. The kernel operating frequencies of FPGAs are significantly low compared to CPUs and GPUs. The absence of power sensors in the DE5-net board we had to estimate based on the circuit elements using Intel Quartus Early Power Estimator which they state gives a medium accuracy of the estimate. The true power consumption of kernels may differ due to many other reasons such as the environmental conditions.
 Therefore, we believe that with the advancement of FPGA hardware and HLS tools with better optimizations methods can provide better results.
 Publications
 Semester 7 report
 Semester 7 slides
 Semester 8 report
 Links
 Project Repository
 Project Page
 Department of Computer Engineering
 University of Peradeniya
 Back to top",Effective utilization of OpenCL to map the Adaptive Banded Event Alignment(ABEA) algorithm to run eﬃciently on an FPGA.,E15,Undergraduate Research Projects,https://projects.ce.pdn.ac.lk/4yp/e15/Accelerating-Adaptive-Banded-Event-Alignment-Algorithm-with-OpenCL-on-FPGA/,https://github.com/cepdnaclk/e15-4yp-Accelerating-Adaptive-Banded-Event-Alignment-Algorithm-with-OpenCL-on-FPGA,https://cepdnaclk.github.io/e15-4yp-Accelerating-Adaptive-Banded-Event-Alignment-Algorithm-with-OpenCL-on-FPGA,https://cepdnaclk.github.io/e15-4yp-Accelerating-Adaptive-Banded-Event-Alignment-Algorithm-with-OpenCL-on-FPGA/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/4yp/E15/Accelerating-Adaptive-Banded-Event-Alignment-Algorithm-with-OpenCL-on-FPGA/
81,Brain Computer Interface for controlling virtual objects,"Brain Computer Interface for controlling virtual objects using self-paced mind intent
 Projects - Department of Computer Engineering
 University of Peradeniya, Sri Lanka
 Brain Computer Interface for controlling virtual objects using self-paced mind intent
 Team
 E/15/023, Avishka Athapattu, email
 E/15/059, Prageeth Dassanayake, email
 E/15/238, Sewwandie Nanayakkara, email
 Supervisors
 Dr. Isuru Nawinne, email
 Prof. Roshan Ragel, email
 Theekshana Dissanayake, email
 Table of content
 Abstract
 Related works
 Methodology
 Experiment Setup and Implementation
 Results and Analysis
 Conclusion
 Publications
 Links
 Abstract
 Non-invasive EEG based Brain Computer Interface (BCI) systems have been an interesting research area for many fields. However most of the research done on this subject is synchronous, therefore the state of mind of the user is not similar to its natural behaviour. Considering to provide possible experience in practical applications, self-paced BCI systems started gaining popularity in recent years. However, there are certain challenges yet to be addressed when following this method. Out of the research done on self-paced BCI systems most of them are focused on motor-imagery control whereas research on nonmotor imagery mental tasks is limited. In this research, we analyse the possibility of using the techniques used in the motorimagery method for non-motor imagery mental tasks to be fed into virtual object controlling applications.
 Related works
 Both non-motor imagery EEG signals related to virtual object manipulation and motor imagery EEG signals are sensorimotor rhythms(SMR). These are specific brain waves over the sensorimotor cortex that are generated after MI or ME. In research by Faradji et.al research paper, they explored the idea of rotation of a virtual object in 3D space in a more natural way. They used auto scalar auto-regressive methods for feature extraction and the classification was done with quadratic discriminant analysis. They obtained a true positive rate (TPR) value of 54.6% TPR and 0.01% FPR. Although there are numerous researches on using motor imagery to control virtual objects that give us higher accuracy [2], research done by Faradji et al. explores the possibility of controlling objects in a more natural way. It was stated that although the TPR is relatively low compared to MI related research, this method is more preferable in real-time applications since this method requires less computational power.
 Methodology
 The procedure of self-paced BCI module for virtual object controlling consists of 8 steps
 The subject should know what are the activities that need to be done since it is important to induce brain waves related to those activities. Most of the research subjects practice to perform a minimum number of activities, for example in virtual object controlling, moving an object up and down, left and right.
 Subjects should train without feedback provided to acquire the required data as well as to analyze signal patterns Fig 1.
 Preprocessing the data by artifact reduction(Electrooculogram(EOG), Electromyogram(EMG)) and signal filterings methods such as low-pass/high pass or bandpass filter
 Feature extraction to find a suitable representation of the electrophysiology data that simplify the subsequent classification or detection of specific brain patterns.
 With the extracted features classifier being trained, the accuracy should be 70% or higher if not we have to recollect data and extract features and train a classifier model all over again.
 Training in real-time with the help of visual feedback Fig.2.
 Update the classifier if the frequency band or EEG pattern changes. (Post-processing)
 Feed the classification output into an application interface with virtual objects.
 Figure 1
 Figure 2
 Experiment Setup and Implementation
 First we trained the subject to train three mind intents which are left, right, and None without any visual aid. Afterwards,
 we trained the subject with GUI aid. We used an OpenBCI Cyton board to capture EEG data in the experimental setup and signals were processed using Python. EEG signals were fed for processing and denoising. We used the OpenBCI GUI to send EEG signals
 through LSL (Lab Streaming Layer) into a Python application where we extracted the features. Our subject was a male volunteer, of age 24. Initially the subject performed a mental task while watching a virtual object on a screen. This training was done in a limited time trial like 0 -10 seconds, because the performance of the mental task degrades over time.
 A. Cyton Board (Hardware platform)
 Cyton board is an Arduino compatible wireless device which is able to capture EEG signals. It consists of 8 biopotential input channels. It must be powered up with 3-6V DC battery only. It has the ability to send samples at 250Hz frequency. Each packet contains a header followed by a sample counter, followed by 8 ADS channel data, followed by the three axes values of the accelerometer, followed by a footer. The USB dongle is connected to the laptop where the cyton board communicates with it using Bluetooth to transfer data.
 Figure 3
 B. OpenBCI GUI and LSL
 OpenBCI GUI (fig 4)here is a powerful software that is used to visualize, record and stream data from OpenBCI boards. This GUI helps to visualize data coming from eight channels of Cyton board to understand if there are any faults in connections. If there are external disturbances that interfere with the visualization of EEG signals it can be recognized as well. It also visualizes the real-time representations of FFT, power spectral distribution and time series.
 Figure 4
 Lab Streaming Layer is a system developed for synchronising streaming data for real-time analysis and recording. This is used to send the raw EEG data as time series into a python application for signal processing. PyLSL library is used to input the data to the python application. We are taking in time series EEG data. Data is transferred at 250Hz. Each sample contains data of each channel as floats.
 C. Electrodes and electrode placement
 We used eight Golden cup electrodes to sample EEG data. We placed those on the subject according to the 10-20 method. The 10–20 system or International 10–20 system is an internationally recognized method to describe and apply the location of scalp electrodes in the context of an EEG exam. EEGs were placed in 10% and 20% spaces on the scalp as follows. The brain waves related to controlling virtual objects are induced in the motor cortex so electrode placement positions are chosen so as to extract the maximum amount of information. In our experiment, we placed electrodes as shown in Fig. 5.
 Figure 5
 D. Virtual Environment
 Virtual objects that were meant for controlling are created with Unity. The subject is trained on a virtual environment where the display is 15.6 inch, monitor resolution of 1920 x 1080 p and 60Hz. Data of mind intent will be recorded where the subject will focus on moving the objects along axes. Shown in Fig. 6 is the virtual environment we created.
 Figure 6
 Results and Analysis
 Frequency bin components extracted by FFT and Detailed coefficients extracted by wavelet transform were used as features for the classification purpose. All the classifications have the ability to perform in real time. We used Random Forest, QDA, KNN, Catboost and SVM for classifying. In Table II we have compared the accuracies between different classification models. Table III gives the TPR of each class with respect to the model. The confusion matrix of the KNN model is shown in Fig.7.Best hyper
 parameters combination for each model is determined by a grid search using 10 fold cross validation as evaluation method. KNN model with features obtained with FFT showed the
 highest accuracy. Overall accuracies obtained when using FFT is higher than when using WT. Since we have data collected over 5 days we used a 5-fold cross validation to get an estimation of the consistency of accuracies. This is shown in figure 8
 figure 7
 figure 8
 Demo
 Conclusion
 Filters that were used in EEG signal processing causes a phase shift that makes the usage of wavelet features impossible. Therefore we have used the FFT feature extraction method to provide frequency bins as features for our classification methods. But by substituting those filters (Butterworth filter) with others (zero phase filters) the effect of the phase shift can be removed. We can explore the possibility of using a combination of features provided by WT and FFT to train a more accurate classification model. With all the classification models that were trained KNN algorithm with FFT algorithm would be the ideal choice of features and classification combination. We were able to obtain around 55% TPR value. By implementing statistical analysis we can rectify the anatomical localization effects on EEG data would further increase accuracy of these models. Deep learning methods proved to have a lot of potential when it comes to MI based research in recent history. Possibility of using deep learning approaches in non motor imagery
 intent with self phased brain computer interfaces is something that can be explored as well.
 Publications
 Semester 7 slides
 Semester 8 report
 Semester 8 slides
 Athapattu A.D., Dassanayake P.S.B., and Nanayakkara G.S.C., “Self Paced Brain Computer Interface On Sensoriomotor Rhythms For Virtual Objects Controlling” (2021). PDF.
 Links
 Project Repository
 Project Page
 Department of Computer Engineering
 University of Peradeniya
 Back to top",Develop a Brain-Computer Interface for Controlling virtual objects using the self-paced mind intent of a person. Here we have developed a Real-time pipeline for recognizing thought patterns using machine learning techniques and feed into a virtual environment(unity engine) for controlling virtual objects.,E15,Undergraduate Research Projects,https://projects.ce.pdn.ac.lk/4yp/e15/Brain-Computer-Interface-for-controlling-virtual-objects/,https://github.com/cepdnaclk/e15-4yp-Brain-Computer-Interface-for-controlling-virtual-objects,https://cepdnaclk.github.io/e15-4yp-Brain-Computer-Interface-for-controlling-virtual-objects,https://cepdnaclk.github.io/e15-4yp-Brain-Computer-Interface-for-controlling-virtual-objects/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/4yp/E15/Brain-Computer-Interface-for-controlling-virtual-objects/
82,Doppelganger Cartoon,"Doppelganger Cartoon
 Projects - Department of Computer Engineering
 University of Peradeniya, Sri Lanka
 Project Title
 Team
 E/15/065, De Silva K.G.P.M., e15065@eng.pdn.ac.lk
 E/15/076, Dileka J.H.S., e15076@eng.pdn.ac.lk
 E/15/220, Maliththa K.H.H., e15220@eng.pdn.ac.lk
 Supervisors
 Dr. Asitha Bandaranayake, asithab@pdn.ac.lk
 Mr. Sampath Deegalla, dsdeegalla@pdn.ac.lk
 Mr. Ishan Gammampila
 Table of content
 Abstract
 Related works
 Methodology
 Experiment Setup and Implementation
 Results and Analysis
 Conclusion
 Publications
 Links
 Abstract
 Human face recognition and feature extraction have been the most interesting technologies to study for many researchers. It allows a huge number of face images to be recognized in just a short amount of time and extract the face features very easily, rather than recognizing each image and it’s features individually through a normal human’s eyes.Using these technologies researches are being carried out to find the look-alike characters within humans. Using methods for real people, cartoon character faces can hardly be detected and recognized because the face features of cartoon characters differ greatly from those of real people in terms of size and shape. This research was conduct to find the techniques to face detection,feature extraction of a cartoon characters and recognize look-alike cartoon character for a given human image. We have created Disney cartoon repository including 800 images from 77 characters, 5 images from each character with mirror images. Also include face features marked by hand as 35 labeled coordinates. For cartoon face detection and feature extraction, landmark based model trained using feature marked dataset. Used distances and the areas between the landmarks as features. Total 92 features(50 areas and 42 distances) are stored as csv files along with the cartoon images. To compare features of a real image with all the cartoon image features euclidean distance was considered. To increase the accuracy we used landmark based model with hair extraction model and also include gender prediction model. This combined model improves the performance compared to basic landmark based model. Alternatively, we implemented a classification model to find the best matching cartoon character. It shows 84\% accuracy on training data and 80\% accuracy on validation after 100 epochs. Finally we were able to find the best matching Doppelganger Cartoon character with good accuracy. So we hope this research and the dataset created by us will be more useful to other researchers.
 Related works
 There is not any directly related project happening to find a machine learning algorithm that can find the cartoon character that best looks like you. But some websites published manually founded cartoon images with their matching human image
 [10][11][13][14]. Also, some websites provide the best matching celebrity image for your
 uploaded image.[12]Chase, Davis, and Amanda Jacquez did the research and reported
 it called Finding Your Celebrity Look Alike.[15] They found vector representation of
 faces and then used OpenCV HaarCascade classifier in order to detect faces. Images
 inputted to the system and images in IMDB-WIKI data set were represented as 2622
 dimensional vectors. Then IMDB-WIKI data set to compare with the inputted image
 and find the similarities between them. According to that, they find the best matching
 celebrity image/images using the Euclidean distance.
 Similarity Learning
 A similarity measure is defined as the distance between various data points. Measuring
 the similarity between two images is mostly used in image retrieval and computer vision
 Fields.
 SimNet[16] is a methodology proposed by Srikar Appalaraju and Vineet Chaoji,
 This deep siamese network is trained on pairs of positive and negative images using a
 novel online pair mining strategy inspired by Curriculum learning. Wang, J., Song, Y.,
 Leung, T., Rosenberg, C., Wang, J., Philbin, J., Chen, B. and Wu, Y. proposed a deep
 ranking model that learns the similarity metrics directly from images[17]. By comparing
 the models which are based on handcrafted features, this approach has higher learning
 capability. Their goal was to learn similarity models. Euclidean distance and nearest
 neighbor search problem concepts were used to rank similar images. For ranking the loss function, a triplet based network was proposed and image triplets were taken as
 the inputs. Because of the recent success of the ConvNet for image classification[18],
 they started a convolution network that contains convolution layers, max-pooling layers,
 local normalization layers, and fully-connected layers for each individual network. An
 asynchronized stochastic gradient algorithm[19] with a momentum algorithm[20] was
 used because training a deep neural network needs a large amount of data. The ImageNet
 ILSVRC-2012 data set was used as training data which contains roughly 1000 images in
 each of 1000 categories. A relevance training data which was generated in a bootstrap
 fashion was also used as a training data set.
 Face Detection & Recognition
 The computer technology that finds and identifies the human faces in digital images
 is called face detection. Feature -based approach and image-based approach are the
 two main approaches of detecting faces of real people[8]. Imager:: Anime Face is an
 image-based approach which is used to detect faces of cartoon images[21]. This method
 finds that the input images are faces or non-faces. As face detection, face recognition
 also can be classified into two categories called model-based approach and image-based
 approach. Kohei Takayama, Henry Johan and Tomoyuki Nishita proposed the first
 relevant work concerning face detection and face recognition of cartoon characters
 extracting the features[22]. In face detection the skin and the edges of the input image
 were extracted firstly. Edges were extracted using Canny Method and they considered
 that the skin color of the cartoon image has to be near to real people. Jaw contour and
 Face symmetry are used to face detection. For comparison, OpenCV Face Detection
 and Imager:: AnimeFace[21] are used as candidates and 493 various cartoon characters
 are given as inputs. By comparing the results, the proposed method is more accurate
 than the previous method. Feature extraction of the detected face and determination of
 the individuality of the face and Character search are the main two purposes of their
 face recognition system. Skin color, Hair color and the Hair quantity are the three
 features that they extracted to build the feature vector. Face similarity is calculated
 by measuring the distance between the features of two feature vectors of input image
 and images in the database. 71% of output images contain the same characters as input
 images(success) and 29% of the search are failure. Saurav Jha, Nikhil Agarwal and
 Suneeta Agrawal presented a methodology to improve the Cartoon Face Detection and
 Recognition systems. MTCNN architecture which offers a deep cascaded multi-task
 framework is used to face detection. This architecture has three sequential deep CNNs and they are Proposal Net, Residual Net and the Output Net. For securing the baseline
 results, Haar Features and HOGfeature are employed. Face recognition is experimented
 on two different techniques, inductive transfer using inception v3 + SVM/GB and their
 proposed method. The proposed method has two phases. The first phase consists of
 preprocessing ( converting the cartoon image to gray scale and normalized), landmark
 extraction(15 facial landmarks of 750 images of 50 characters) and landmark detection(5
 layer LetNet architecture). In phase 2, leverages the images using a hybrid CNN (HCNN)
 model. Benchmark IIIT-CFW database which contains 8,928 annotated cartoon images,
 is used as the dataset.
 Feature Extraction
 Feature extraction is important when finding a similar face to another face such as face
 recognition, face detection, and expression detection. Eyes, mouth, and nose are the most
 important features for face recognition[23]. Hua Gu, Guangda Su, and Cheng Du from
 Tsinghua University proposed a method to extract feature points from faces[3]. This
 approach is based on human visual characteristics. The features of the face are extracted
 with the properties by using the geometry and the symmetry of faces. Normalizing the
 image size before processing is not needed in this method. Integrating the local edge
 information is not easy when we extract the face features. In this method, the Smallest
 Univalue Segment Assimilating Nucleus(SUSAN) operator was chosen to extract the
 edge and corner points of the feature area. Feature points were located by using face
 similarity and geometry.
 Lilipta Kumar Bhatta and Debaraj Rana proposed a technique for extracting facial
 features from a color image through skin region extraction. Extracting the characteristics
 of human face color and face region using Sobel operator[24], Converting the image
 into YCbCr components and extracting skin region using morphological operation, and
 extracting the regions of the human eye, mouth, and nose by means of gray level intensity
 value were the three steps of their proposed technique. FEI face database[25] was used
 for their experiment. They normalized the image size to 640*480. Using this technique,
 they experimented and showed that the locating of the feature points is exact and fast,
 this technique increases the accuracy of face recognition.
 Face Landmark Detection
 Facial landmarks detection is used in many computer vision applications like face alignment, drowsiness detection, face recognition, facial expression analysis, facial animation,
 3D face reconstruction as well as facial beautification, etc.[26] The aim of face landmark
 detection is to detect the predefined key points like eyes, eyebrows, mouth, nose, etc. Yue
 Wu·Qiang Ji classified these detection algorithms into three methods like holistic methods, Constrained Local Model (CLM) methods, and regression-based methods depending
 on how they model the facial appearance and facial shape patterns. The holistic methods
 models represent the global facial appearance and shape information. The Constrained
 Local Model leverages the global shape model but builds the local appearance models.
 And the regression-based methods capture facial shape and appearance information. [27]
 Yongzhe Yan1,Xavier Naturel,Thierry Chateau, Stefan Duffner, Christophe Garcia,
 Christophe Blanc divided facial landmark detection algorithms mainly into two types,
 generative or discriminative. The generative types algorithms, which include the partbased generative models like ASM and holistic generative models like AAM, model the
 facial shape and facial appearance as probabilistic distributions. They have provided
 a comparison of different face alignment methods as well as different deep compression
 models. To this comparison, they included traditional cascaded regression methods and
 deep learning-based face alignment methods.[26]
 Zixuan Xu1, Banghuai Li2, Miao Geng3, Ye Yuan identified that face landmarks
 detection becomes a challenging task when dealing with faces in unconstrained scenarios,
 especially with large pose variations. They targeted the problem of facial landmark
 localization across large poses and give a solution based on a split-and-aggregate strategy.
 When splitting the search space, they proposed a set of anchor templates as references for
 regression, which well solved the problem which had with large variations of face poses.
 Then depending on the prediction of each anchor template, they proposed to aggregate
 the results, which reduce the landmark uncertainty due to the large poses.[28]
 Hair Segmentation
 Since the appearance of hair can vary between different people based on their gender, age,
 ethnicity, and the surrounding environment, automatic hair segmentation is challenging
 in general.
 Recently, there has been much success with deep neural networks (DNNs) and in many
 tasks, including semantic segmentation, DNN-based hair segmentation methods havebeen introduced. The work of Liuet al. [29] introduced a multi-objective learning method
 for deep convolutional networks that jointly models pixel-wise likelihoods and label
 dependencies. A nonparametric prior was used for additional regularization, resulting in
 better performance. Guo and Aarabi [30] presented a method for binary classification
 using neural networks that perform training and classification on the same data using the
 help of a pre-training heuristic classifier. They used a heuristic method to mine positive
 and negative hair patches from each image with high confidence and trained a separate
 DNN for each image, which was then used to classify the remaining pixels.
 Methodology
 Proposed Methodology
 A machine learning algorithm to find the doppelganger cartoon for a given image is the
 final outcome of this research. After reviewing previous works on face detection, feature
 extraction and feature comparison, the proposed methodology is under the following
 conditions.
 • Cartoon images are limited to only Disney characters.
 • Full body of cartoon images and real human images are not compared.
 • Real human images should be given as the input.
 Conceptual design
 There are more approaches done to detect faces, extract features and measure similarity of
 images of real images and cartoon images separately. But there are very few applications
 that compare cartoons and real humans using these concepts. After an extensive
 study of the work done by various approaches and experiments, we came up with a
 methodology. This application provides a number of analysis steps including preprocessing,
 face detection, feature extraction, measuring similarity and displaying the results with a
 user friendly web application. The web application is designed for users who want to
 find the doppelganger of him/her. The real image is obtained and the result is displayed
 through the web application. Image preprocessing, face detection, feature extraction and
 measuring similarity steps are done in the backend.
 Web Application
 Frontend of the web application is designed using React and the backend is developed
 using python Django. React is an efficient, flexible javascript library which is developed
 by Facebook for building interactive web applications. It lets us compose complex user
 interfaces from small pieces of codes.The web application basically provides two features.
 Users can upload a photograph of a person and see the resulting cartoon image. And
 also they can share it to social media sites like facebook, instagram etc. Django REST
 framework is a powerful and flexible toolkit for building Web APIs. Since we are building
 our machine learning model using python tensorflow and keras libraries having a python
 based backend is easy.
 Methodological approach
 Data Collection
 For image classification tasks there are some popular data sets that are used across research
 and industry applications. The most popular ones are Imagenet, CIFAR, MINST. But for
 tasks like cartoon-human image similarity checking there is no well-known dataset that
 can directly be used. There are some freely accessible comic and animated cartoon image
 repositories which differ more from human faces out there. Cartoon image repository in
 this research is only contained with Disney cartoon images which are more similar to
 human faces to simpler this approach as this is the beginning. Disney cartoon repository,
 created by our own and ibug 300-w Human repositories are used to train the landmarks
 detection model. For the classification model, the dataset contains 58 Disney cartoon characters with 406 images, 348 images for training, and 58 images for validation. To
 test each algorithm of predicting the doppelganger cartoon image, we used a test set
 contains 20 already known doppelgangers.
 Data Preprocessing
 Normalizing the images before feeding them into models is caused to give good results
 and specific sizes are required from most models. Image data normalization ensures that
 each pixel has a similar data distribution. This causes us to speed up the converging
 process. Data normalization can be done by subtracting the mean from each pixel value
 and dividing them by the standard deviation. So the normalized data is in the range of
 [0,1] or [0,255].
 As we were only considering 35 special landmarks on the face, we extracted that 35
 landmarks from the given 68 landmarks in the human dataset ibug 300-W dataset. The
 special 35 landmarks, considered in this research, is shown in the Figure 3.4.
 Face detection and Feature extraction
 After preprocessing images the next step is to extract features. This is the most important
 part of this project because the accuracy of the algorithm directly depends on the extracted
 features. Face detection and feature extraction can be done by various approaches. These
 approaches are discussed in the section Face detection and Feature extraction
 Store extracted features
 When comparing images going through all the images in the repository, extracting
 features and checking similarities will take a lot of time and need considerably huge
 performance. So to reduce the effect of above problems we stored the extracted features
 of cartoon images in a csv file with the image paths. So it is easy to go through the csv
 file and check similarities with features extracted from human images.
 Similarity
 Distance metric or matching criteria is the main tool for finding the similar images. Two
 vectors, a vector with extracted features of the real human image and a feature vector of a cartoon should be compared to find the similarity of the two images. The L1 metric
 (Manhattan Distance), the L2 metric (Euclidean Distance) which are main two distance
 metrics, have been proposed in the literature for measuring similarity between feature
 vectors.
 Euclidean Distance : If there is two points a and b have n dimensions such as
 a=(x1 ,x2,…,xn) and b=(y1,y2,…,yn) , the Euclidean distance between two points can be
 generalized as in Equation 3.1
 The calculated Euclidean distances of each cartoon feature vector with the real image
 feature vector are compared and the cartoon image with the least distance is selected as
 the best matching cartoon image for the real image.
 Experiment Setup and Implementation
 Research Tools
 In the purpose of implementing our project we have used several libraries and frameworks.
 • Numpy : This is a library for python programming which supports multidimensional arrays and matrices, with a large collection of high level mathematical
 functions.
 • Keras : This is an API designed to follow best practices for reducing cognitive
 loads and it offers consistent and simple APIs which minimizes the number of user
 actions for common use cases.
 • Tensorflow : Tensorflow is a open source library for machine learning. It can be
 used across a range of tasks which involve deep neural networks.
 • Cv2 : OpenCV is a library which is designed for solving computer vision problems.
 • MobileNetV1: A family of general purpose computer vision neural networks
 designed with mobile devices in mind to support classification, detection and more.
 This is pre-trained on the ImageNet dataset, a large dataset consisting of 1.4M
 images and 1000 classes.
 • Matplotlib : Matplotlib is a comprehensive library for creating static, animated,
 and interactive visualizations in Python. In our case it is very useful in displaying
 images.
 • os: This module provides a portable way of using operating system dependent
 functionality
 • Csv: This library helps to manipulate csv files writing and reading.
 • PIL: Pillow library adds fairly powerful image processing capabilities and provides
 extensive file format support, and efficient internal representation.
 • React: This is an open source front end development javascript library for building
 interactive user interfaces.
 • Django REST framework: This is a powerful and flexible toolkit for building
 restful web APIs.
 To run and test codes which are written in python, we used Google Colaboratory.
 It is a jupyter notebook which runs in the cloud and is integrated with google drive,
 making it easy to set up ,access and share. So the image repository is located in the
 shared google drive. By default this notebook runs on the CPU. But it supports GPU
 and TPU hardware acceleration for achieving higher performance.
 Data manipulation and Testing
 Our cartoon data sets is a repository containing Disney cartoon images. Though the
 pretrained models are not perfectly detecting the cartoon faces, this dataset is containing
 only frontal faces of cartoon images. Data manipulation for cartoon landmarks detection
 model is done by annotating the landmarks on the cartoon faces using a tool. The iBUG
 300-W dataset which has already annotated landmarks is used as the human dataset to
 train the model for landmarks detection.
 Pitfalls and workarounds
 During this project, one of the main challenges encountered was to get the background
 knowledge of the data set, feature extraction and face detection of images. As a remedy
 for that issue, we had to do lots of background research. Finding a strong data set with
 cartoon images is required for our task. At first, we could not properly understand the
 already existing data sets. After gaining knowledge about previous works, we understood
 that there are a number of data sets which contain various cartoon images but existing
 data sets are still lacking similarities compared to humans. As a solution, we decided to
 collect images to build a Disney cartoon image repository on our own as Disney cartoons
 are more similar to human images. But still the lack of data for training purposes has
 remained. A number of researches are done on face detection and feature extraction.
 But one issue with that was lack of documentation about feature extraction and face
 detection of cartoon images. After reviewing the research papers, we had to spend a
 considerable amount of time figuring out how the pretrained models work on cartoon
 images and human images to select a best model for our case. By analyzing the results
 given by the models, we figured out that the pretrained models are not performed well
 in cartoon face detection. But actually training a model from scratch is very expensive
 and requires huge data sets to achieve good generalization performance.
 The method landmarks detection, is required to detect the face first and detecting
 face of cartoon images is not similar to human face detection. We handled this by using
 the frontal face of cartoons and using the dlib frontal face detector as the detector.
 Also, when running the code on google colabs, due to the high throughput of the data
 set, we faced an issue of insufficient memory and low speed. We handled this problem by
 switching the run time mode to GPU from none in the Google Colabs environment.
 Results and Analysis
 Overall analysis
 All the algorithms we tried on this research are tested on the same test set which contains
 already known doppelgangers. To compare the models we ranked the resulted images
 according to the ascending order of euclidean distance. Table 5.2 Shows the summary of ranks for each cartoon. For some images, the models do not give an output because of
 some failures in the hair extraction model or gender prediction model for some cases. As
 an example, some female images can have short hair, then the gender prediction model
 wrongly predicted the gender and then our expected result is not within the resulted
 images.
 Figure 5.32 shows the variation of the ranks of each cartoon character for different
 weight values. By analyzing the graph, we can conclude that the rank varies for different
 weights. Some cartoons get better rank in w = 0.5 and someones get better at w=0.2.
 The graph of w=1 (landmarks-based model only) always gives higher ranks (far away
 from expected result) for all cartoons with respect to other models. According to the
 graph, w=0.2 and w=0.5 (combined model) gives some good ranks for all cartoons.
 But according to the classification results in the Table 5.2, the classification model
 gives the best results for all cartoons as all the expected outputs are within the top 5
 ranks.
 Conclusion
 During recent years, many researches have been carried out in various ways of feature
 extraction of human images, finding looks-alike twins, and so on. We entered the research
 using the pre-trained model based approach and after analyzing the results, we concluded
 that we should simplify this by considering only the Disney cartoons which are more
 similar to humans as this is the beginning and train our models for building the algorithm.
 In this paper, we mainly researched an approach that finds the best matching Cartoon
 character for human image based on landmarks model. Because lack of existing cartoon
 datasets, we have created a dataset with landmarks on the faces of cartoon characters
 for training a model and it will be more useful for future researchers. Combination of
 landmark based model with hair extraction model and gender prediction model has
 improved the performance. But the best cartoon image is resulted for different weights
 on the models for various images. Alternatively implemented classification model shows
 84% accuracy on training data and 80% accuracy on validation after 100 epochs. As
 features on cartoon faces such as eyes, nose are more differ from human features, the
 combined model is also not accurate like classification. So, the classification algorithm
 with a strong dataset will be the best model for this finding doppelganger task. Today’s
 society is interesting to compare their appearance with cartoons because it brings mental
 relaxation and fun for their minds. So we hope this research will be helpful for them.
 Publications
 Semester 7 report
 Semester 7 slides
 Semester 8 report
 Semester 8 slides
 Links
 Project Repository
 Project Page
 Department of Computer Engineering
 University of Peradeniya
 Back to top","The objective of Doppelganger Cartoon is to create a machine learning model to find the best matching cartoon character for a human image . This was implemented using three approaches: pre-trained model based approach, combined model (face + hair + gender) and classification model.",E15,Undergraduate Research Projects,https://projects.ce.pdn.ac.lk/4yp/e15/Doppelganger-Cartoon/,https://github.com/cepdnaclk/e15-4yp-Doppelganger-Cartoon,https://cepdnaclk.github.io/e15-4yp-Doppelganger-Cartoon,https://cepdnaclk.github.io/e15-4yp-Doppelganger-Cartoon/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/4yp/E15/Doppelganger-Cartoon/
83,Explainable Machine Learning for Real World Resource Constrained Problems,"Projects | Department of Computer Engineering
 Projects - Department of Computer Engineering
 University of Peradeniya, Sri Lanka
 Project Title
 Team
 E/15/092, Imesh Ekanayake, email
 E/15/187, Devin Kulanjith, email
 Supervisors
 Dr. Dhamayanthi Herath, email
 Dr. Upul Jayasinghe, email
 Dr. Kasun Amarasinghe, email
 Table of content
 Abstract
 Related works
 Methodology
 Experiment Setup and Implementation
 Results and Analysis
 Conclusion
 Links
 Abstract
 Machine Learning (ML) has contributed to many advances in science and technology.
 Recently a trend of applications in high-stake decision-making has been initiated. The development of ML made the decision-making process unclear with complex black-box models, especially the state-of-the-art models which have maximized the performance are more complex, inexplicable, and hard to explain. On the contrary, high-stakes settings as healthcare, finance, and criminal justice, have strict ethical concerns that made a mandatory requirement to explain each decision or the model as a whole. Besides, the acts and regulations like General Data Protection Regulation (GDPR)
 make it obligatory to explain the decisions made by computer systems and became a social right to explanation.One of the most pressing problems in this field is explainability and interpretability of the decisions which are made by the several algorithms Moreover, it is necessary to ensure the fairness and transparency of a decision to obtain the stakeholders’ trust. The theoretical knowledge of explainable machine learning is not well-tested on real-world problems with direct social impact. In this paper, we have identified a quandary that reflects the characteristics of a high-stakes machine learning problem in the public sector. An early warning system to predict and help the projects that could be unfunded in an educational crowdfunding platform in a resource-constrained environment has been presented.
 Related works
 Methodology
 Experiment Setup and Implementation
 Results and Analysis
 Conclusion
 Even though the machine learning became one of the hot topics in current days in almost all the fields, trust, transparency and fairness of the predictive models has not been properly considered. In high stake settings, where it makes a direct impact on people’s lives or future of a business it is important to obtain the trust of the domain experts to use the model predictions in decision making process. In the above workflow, we have trained 8 machine learning algorithms in periodic manner using cohort concept and designed
 a grid search for the time series data to optimize without having a data leakage. Next, we
 obtained the overall performance of the predictions to select the best performing models. Once those models were selected the top-K (here we have taken K as 100) recall has been measured to select the best predictive model for the task and selected CatBoost model for further analysis.
 Next we have used explainable AI models (SHAP and LIME) analyse individual analysis of the predictions, there we obtained the importance of each attribute separately for the prediction of each instance. Once the top-k,
 middle-K and bottom-k importance of attributes are selected, then values are normalized for each instance (addition of importance become 1 in each instance).
 Finally the correlation, of the normalized importance with the actual values have been considered and identified the distribution of actual values along with the importance variation.
 In conclusion, the final machine learning model has adhere to the thinking pattern of users and domain experts can use the explanation to improve the project quality using the insights of the model.
 Links
 Project Repository
 Project Page
 Department of Computer Engineering
 University of Peradeniya
 Back to top","Machine Learning (ML) has contributed to many advances in science and technology, recently the applications in high-stake decision-making has been increased. The ability to build statistically complex relationships between data and outcomes, without explicitly programming the rules makes it popular among the computerized decision-making community. The state-of-the-art models which performer well, are more complex and inexplicable. On the contrary, in high-stakes settings as healthcare, finance, and criminal justice, have strict ethical concerns which lead to explain each decision or the model in whole. Moreover, there it is necessary to ensure the fairness of the decisions and the transparency to avoid wrongful rationality. Furthermore the acts like General Data Protection Regulation (GDPR) and Personal Data Protection Act (PDPA) makes it mandatory to explain the decisions made based on statistical relationships and gave a right for users to demand an explanation. ",E15,Undergraduate Research Projects,https://projects.ce.pdn.ac.lk/4yp/e15/Explainable-Machine-Learning-for-Real-World-Resource-Constrained-Problems/,https://github.com/cepdnaclk/e15-4yp-Explainable-Machine-Learning-for-Real-World-Resource-Constrained-Problems,https://cepdnaclk.github.io/e15-4yp-Explainable-Machine-Learning-for-Real-World-Resource-Constrained-Problems,https://cepdnaclk.github.io/e15-4yp-Explainable-Machine-Learning-for-Real-World-Resource-Constrained-Problems/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/4yp/E15/Explainable-Machine-Learning-for-Real-World-Resource-Constrained-Problems/
84,Hand Gesture Recognition using sEMG,"Projects | Department of Computer Engineering
 Projects - Department of Computer Engineering
 University of Peradeniya, Sri Lanka
 Visit Department Site
 Improved Gesture Recognition for sEMG based Human Machine Interface
 Team
 E/15/043, Yasiru Bhagya T.P., yasirubhagya@eng.pdn.ac.lk
 E/15/131, Hisni Mohammed M.H., hisnimohammed@eng.pdn.ac.lk
 E/15/348, Suhail S., suhailsajahan@eng.pdn.ac.lk
 Supervisors
 Dr. Isuru Nawinne, isurunawinne@eng.pdn.ac.lk
 Prof. Roshan Ragel, roshanr@eng.pdn.ac.lk
 Mr. Theekshana Dissanayake, theekshanadis@eng.pdn.ac.lk
 Table of content
 Abstract
 Related works
 Methodology
 Experiment Setup and Implementation
 Results and Analysis
 Conclusion
 Links
 1. Abstract
 Identifying hand gestures using surface electromyography (sEMG) signals is vital in the development of next-generation human-machine interfaces (HMI). sEMG based HMIs provide users with a more natural and convenient way to communicate with computing systems. sEMG signals recorded from muscle tissues give information about the intended muscle movements triggered by the brain waves. Identifying these movements allows developing interfaces that can control computing devices. In this research, an attempt was made to improve a hand gesture recognition model that could be used as a human-machine interface using an online open dataset of sEMG signals. First sEMG signals were preprocessed using a bandpass filter and notch filter to remove noises in the signal. Then various time, frequency, and time-frequency domain features extracted and they were fed into machine learning algorithms such as random forest, support vector machines (SVM), K-nearest neighbors (K-NN), and recurrent neural networks. All the results were validated using 10-fold cross-validation. Maximum testing accuracy of 90.03% was obtained using an SVM classifier with root mean square, mean frequency, and median frequency of the signal as features for 24 channel data. Later an attempt was also made to use this result to control a simple game developed in Unity using sEMG signals collected from an 8-channel signal acquisition device.
 2. Related works
 Various approaches have been proposed in the area of EMG-based applications. As sEMG signals from different muscle groups exhibit different characteristics, different techniques have been employed to characterize muscle movement. But in general, it is all down to feature extraction to represent the signal as a vector of features, followed by feature selection to reduce the vector’s dimensionality, and finally, classification to determine each vector as belong to one of a fixed set of classes [5].
 Paleari et al. [6] have used the root mean square (RMS) feature with a neural network model to classify hand movements using 192 channel high-density sEMG (HD-sEMG) signals from the forearm. Stango et al. [7] have used the SVM classifier with variogram, which is a measure of the degree of spatial correlation to build a model to control upper limb prostheses using HD-sEMG signals. Liu et al. [8] proposed an invariant feature extraction (IFE) framework based on kernel fisher discriminant analysis to enhance the robustness of myoelectric pattern recognition. Tsai et al. [9] proposed multi-channel EMG-based motion pattern recognition. They have used the SVM classifier with STFT-ranking features based on short-time Fourier transform and principal component analysis to feature selection. Amma et al. [10] used HD-sEMG signals recorded by 192 electrodes to classify finger gestures using the RMS feature and a naive Bayes classifier. Most of these researches based on HD-sEMG signals and complex feature extraction with complex deep learning models that give higher accuracy are not suitable to implement in a real-time environment as they have higher latencies, expensive in terms of electrode configuration, and inconvenient for the user.
 3. Methodology
 3.1. Data Set
 An online open dataset “putEMG” [4] published by Kaczmarek et al. was used for our research. This dataset is a database of sEMG signals collected through an experiment conducted on a group of 45 subjects. This group consisted of 37 males and 8 females aged between 19 and 37 years old. To record sEMG signals, a signal acquisition device with 24 electrodes placed in 3 elastic bands such that 8 electrodes per elastic band was used. Signals were recorded from right forearm muscles. The signals were sampled at the rate of 5120 Hz, with a 12-bit analog to digital converter.
 The sEMG signals were recorded when subjects were not moving the hand-keeping the muscles relaxed (idle gesture), when hand fist, while flexion of the hand, while the extension of the hand, and while pinching the fingers (pinching with thumb and index finger, pinching with thumb and middle finger, pinching with thumb and ring finger, and pinching with thumb and small finger). Therefore, this dataset includes the idle gesture and 7 active gestures. Figure 1 shows all the gestures that are included in the dataset and Figure 2 shows the electrode placement configurations used to acquire signals.
 It is very important to record sEMG signals when subjects perform gestures repetitively as when doing the same gesture again and again subjects tend to perform gestures in a similar pattern [4]. putEMG dataset was created using an experiment that was conducted such that each data instance consists of 20 repetitions of each gesture. Therefore, this dataset allows us to develop and evaluate more robust algorithms for gesture recognition systems.
 3.2. Data Preprocessing
 putEMG data contains raw sEMG signals directly collected from the muscles therefore these signals contain noises. Due to the amplifier’s direct current offsets, the signals will have low-frequency noises and due to electronic devices (computers, radio broadcasts, etc.), the signal will have high-frequency noises [16]. Typically, sEMG signals are within the 10-700 Hz frequency range and signals beyond this range are considered not useful. Furthermore, sEMG signals may contain interference noises generated by the main power line and other equipment used during the acquisition of data. Therefore, a 5th order bandpass filter of range 20 and 700 Hz was used to filter out low, high-frequency noises, and an adaptive notch filter (ANF) was used to reduce interferences. Attenuating frequencies used for ANF were 30, 50, 90, 60, and 150 Hz. These filter parameters were determined using the suggestions made by the true authors of the dataset [4].
 Figure 3 shows the sEMG signals before and after using a bandpass and notch filter. Furthermore, each active gesture in the dataset is of 1 second or 3 seconds, separated by a 3-second idle gesture. Therefore, the dataset contains more ‘idle’ gestures than any other active gestures hence the dataset is unbalanced. Therefore, extra idle gestures were removed to balance the dataset.
 3.3. Feature Extraction
 For the classification using classic machine learning models, ten features from time, frequency, time-frequency domains were extracted from each channel. They were integral absolute value, mean absolute value, mean frequency, median frequency, root mean square, slope sign change, variance, waveform length, Willison amplitude, zero-crossing, and Mel-frequency cepstral coefficients. In previous works, it is found that these features give higher performance, high insensitivity to window size, and low computational complexity [17].
 Root mean square (RMS) which is a time-domain feature gives insights into the amplitude of the signals. The amplitude of the sEMG signal is related to the contraction level of muscles and muscle force involved during the movements. this feature was calculated as,
 sEMG signal frequencies vary with different muscle movements.
 Therefore, frequency domain features such as mean frequency (MNF) and median frequency (MDF) of the signal are also used in the feature vector that is fed into gesture recognition classifiers. These features were calculated as,
 Other time domain features mean absolute value (MAV), wave-length (WL), variance (VAR), slope sign change (SSC), and Willison amplitude (WAMP) were calculated as follows,
 To find Mel-frequency cepstral coefficients (MFCC), the mfcc function from the librosa python library is used. While calculating these features sliding windows size of 2048 and hop length size of 1024 is used. These extracted features were grouped into four separate sets and fed into classifiers separately. The first feature set consists of root mean square, mean frequency, and median frequency. The feature sets II and III were based on previous studies. The second feature set is based on the suggestion made by Hudgins et al. [18] which consists of features mean absolute value, wave-length, zero-crossing, slope sign change. The third feature set consists of integral absolute value, variance, wave-length, zero-crossing, slope sign change, and Willison amplitude was proposed by Du et al. [19]. Finally, feature set IV is made up of MFCC data. Moreover, both preprocessed data and MFCC data were used to train the neural network models.
 3.4. Classification
 Classic machine learning classifier models and deep learning were used to identify eight gestures: idle, fist, flexion, extension, pinching with thumb and index finger, pinching with thumb and middle finger, pinching with thumb and ring finger, and pinching with thumb and small finger. Classic machine learning models used are k-nearest neighbors (k-NN), random forest (RF), support vector machine (SVM), and linear discriminant analysis (LDA). Two sets of data, one with features extracted from all 24 channels data and another with features extracted from 8 channel data were fed into these classifiers.
 The grid search algorithm [20] is used for optimizing parameters for classifier models. To evaluate the classifier models, 10-Fold cross-validation was used. All of the classic machine learning algorithms, the grid search algorithm, and cross-validation used were taken from Python scikit-learn API [20].
 Furthermore, we experimented with two types of neural networks, the long-short-term memory (LSTM) model and the LSTM with convolutional neural network (CNN) model. Extracted MFCC data was fed to the neural network. Neural networks were implemented using TensorFlow with Keras in python language. The first model is a basic model with two LSTM layers and one dense layer to output each class. Here class labels were one-hot encoded and categorical cross-entropy was used as the error function. Figure 3.4 shows more details about the LSTM model. Figure 3.5 shows LSTM-CNN model parameters.
 4. Experiment Setup and Implementation
 4.1. Signal Acquisition Device
 Our device is inspired by the Backyard Brains’ Muscle SpikerShield device. Muscle SpikerShield has 6 channels of sEMG signal acquisition capability. To use it we have to connect it to an Arduino board. On the other hand, our device has 8 channels, a dedicated 8 channel ADC and a powerful STM32F103 microcontroller. Figure 3.6 shows the high-level view of our device and Figure 3.7 shows the circuit diagram for a single channel. Figure 3.8 and Figure 3.9 Illustrates the printed circuit board (PCB) layout of our device. Figure 3.10 shows our final signal acquisition device and Figure 3.11 shows the the 8 channel electrode band.
 4.2. The Game
 A game similar to the space invader game has been created to demonstrate the project. The device will recognize the gesture, then the gesture will be classified through the machine learning algorithms and then the movement of the spaceship can be changed according to the assigned gesture for each movement. The game has been created using the Pygame library, which is a python library mostly used to build games. To make the game interesting, we have created the game with an environment that is similar to the current pandemic situation. The covid19 viruses come towards the earth and the player has to protect the earth from the virus by shooting it from the spaceship. The spaceship can be moved in all 8 directions using the arrow keys and it can fire using the space key. If the covid19 virus reaches the earth or comes near to the spaceship then the game will be ended. Figure 3.12 shows the interface of our game.
 4.3. Real-Time Controlling
 An attempt was made to control the game using the 8 channel signal acquisition device
 developed by us. The first signal acquired by the device was filtered using 5th order
 bandpass filter of range 20 and 700 Hz. Then root mean square, mean frequency, and
 median frequency of the signal were extracted and fed to support vector machine classifiers
 as features to identify the gestures. Figure 3.13 shows our signal acquisition device while
 testing.
 5. Results and Analysis
 Feature Set I: root mean square, mean frequency, median frequency
 Feature Set II: mean absolute value, wave-length, zero-crossing, slope sign change.
 Feature Set III: integral absolute value, variance, wave-length, zero-crossing, slope sign change, and Willison amplitude
 Feature Set IV: Mel-frequency cepstral coefficients
 Classifier models: linear discriminant analysis, k-nearest neighbor, support vector machine, random forest
 Table 1 and Figure 8 illustrate the validation results obtained for different classifier models with different feature sets using 24 channel data. According to the results feature set I which consists of root mean square (RMS), mean frequency, and median frequency, gives the best result for all classifier models. Furthermore, the support vector machine (SVM) classifier gives the highest accuracy of 90.3% for 24 channel data. The linear discriminant analysis (LDA) model has the second-best accuracy of 89.92%. Feature set IV has the lowest accuracy for all the classifier models.
 Table 2 and Figure 9 illustrate the precision score for each gesture when different classifiers are used with feature set I and 24 channel data. Similarly, Table 3 and Figure 10 illustrate the recall value for each gesture when different classifiers are used with feature set I and 24 channel data. Results from Table 2 and Table 3 for we can see that idle, fist, flexion, and extension gestures have higher precision and recall scores, that is all the classifier models tend to predict more accurately these sets of gestures than pinching gestures. Idle gestures are correctly classified by the random forest classifier model with the highest recall score of 0.96 and fist gestures are correctly identified by the SVM classifier with the highest recall of 0.89. LDA has the highest recall for flexion and extension of hand with scores of 0.91 and 0.94 respectively. Pinching thumb with index finger have the lowest recall value for all the classifiers with the best recall score being 0.64 with the LDA classifiers. Other pinching gestures are also correctly identified by the LDA model with the highest recall values of 0.85, 0.86, and 0.88 for pinch thumb-middle, pinch thumb-ring, and pinch thumb-small gestures.
 Table 4 and Figure 11 illustrate the classifier results for 8 channel data. For classifying 8 gestures, the highest accuracy of 86.02% was achieved using the SVM classifier with feature set I. LDA model performed better with the feature set III achieving an accuracy of 85.47% and the random forest model also performed better with the feature set III with an accuracy of 85.17%.
 Table 5 and Table 6 illustrate the precision score and recall value respectively for each gesture when different classifiers are used with feature set I and 8 channel data. The same result is graphically illustrated in Figures 12 and 13. Similar to 24 channel data results pinching finger gestures had low precision and recall values compared to the other 4 gestures. Predicting idle, fist, and flexion of hand have the highest recall value when SVM classifier is used while k-nearest neighbor classifier predicts extension of hand more accurately with the recall value of 0.94. For pinching fingers, SVM predicted more accurately with the highest recall values of 0.52, 0.65, and 0.72 for pinch thumb-index, pinch thumb-middle, and pinch thumb-small gestures.
 Few researchers have worked on the putEMG dataset for different sEMG applications, and Table 6 illustrates the results obtained. Tsinganos et al. [21] have worked on data augmentation methods and with the use of these techniques, they have achieved a maximum testing accuracy of 96.97%. Our results do not match with their results but their research was mainly based on data augmentation techniques that are to create new data from existing data. The performance of machine learning models improves with the amount of data available. This could be the reason that they have achieved higher accuracy. Nacpil et al. [22] have worked on creating a model to control steering wheel for drivers with disabilities and they have achieved a precision score of 96% and 94% for extension and flexion of the hand. We have achieved a precision of 95% for both of these gestures using the SVM classifier with feature set I and also we are classifying eight gestures.
 6. Conclusion
 The objective of this research was to find a hand gestures recognition model that could be useful to interact with machine interfaces using sEMG signals. If we consider results obtained for 24 channel data, the difference in classification accuracies achieved by LDA and SVM are insignificant for feature set I. From precision and recall results also suggest that both LDA and SVM classifiers have similar results. If we consider results obtained for 8 channel data, the SVM classifier with the feature set I performed better. Precision and recall results also suggest that the SVM classifier gives a better result. In both cases, pinching gestures were not accurately classified compared to other gestures: idle, fist, flexion, and extension gestures. Therefore, a system that utilizes a support vector machine classifier with root mean square, mean frequency, and median frequency as features could be adopted to implement an end-user human-machine interface that utilizes limited gestures. This system might not be very useful for classifying pinching gestures as they have lower precision and recall values, and on average they are below 70%.
 sEMG signals characteristic from different muscle groups and characteristics of a single muscle group of different locations have variations. These results obtained depend on the muscle location where signals are acquired and the signal acquisition device used in the experiment. Therefore, when implementing a real-world end-user system these factors also need to be considered.
 References
 [1]
 T. M. M. V. M.A. Cavalcanti Garcia, “Surface electromyography: Why, when and how to use it,” Acta Médica Colomb., vol. 43, no. 2S, p. 176, 2019, doi: 10.36104/amc.2018.1400.
 [2]
 M. B. I. Reaz, M. S. Hussain, and F. Mohd-Yasin, “Techniques of EMG signal analysis: Detection, processing, classification and applications,” Biol. Proced. Online, vol. 8, no. 1, pp. 11–35, 2006, doi: 10.1251/bpo115.
 [3]
 W. Wei, Y. Wong, Y. Du, Y. Hu, M. Kankanhalli, and W. Geng, “A multi-stream convolutional neural network for sEMG-based gesture recognition in muscle-computer interface,” Pattern Recognit. Lett., vol. 119, pp. 131–138, 2019, doi: 10.1016/j.patrec.2017.12.005.
 [4]
 P. Kaczmarek, T. Mánkowski, and J. Tomczýnski, “PutEMG—A surface electromyography hand gesture recognition dataset,” Sensors (Switzerland), vol. 19, no. 16, 2019, doi: 10.3390/s19163548.
 [5]
 A. Jaramillo-Yánez, M. E. Benalcázar, and E. Mena-Maldonado, “Real-time hand gesture recognition using surface electromyography and machine learning: A systematic literature review,” Sensors (Switzerland), vol. 20, no. 9, pp. 1–36, 2020, doi: 10.3390/s20092467.
 [6]
 M. Atzori et al., “Characterization of a benchmark database for myoelectric movement classification,” IEEE Trans. Neural Syst. Rehabil. Eng., vol. 23, no. 1, pp. 73–83, 2015, doi: 10.1109/TNSRE.2014.2328495.
 [7]
 V. H. Cene, M. Tosin, J. Machado, and A. Balbinot, “Open database for accurate upper-limb intent detection using electromyography and reliable extreme learning machines,” Sensors (Switzerland), vol. 19, no. 8, 2019, doi: 10.3390/s19081864.
 [8]
 F. Giordaniello et al., “Megane Pro: Myo-electricity, visual and gaze tracking data acquisitions to improve hand prosthetics,” IEEE Int. Conf. Rehabil. Robot., pp. 1148–1153, 2017, doi: 10.1109/ICORR.2017.8009404.
 [9]
 Y. Du, W. Jin, W. Wei, Y. Hu, and W. Geng, “Surface EMG-based inter-session gesture recognition enhanced by deep domain adaptation,” Sensors (Switzerland), vol. 17, no. 3, pp. 6–9, 2017, doi: 10.3390/s17030458.
 [10]	C. Amma, T. Krings, J. Böer, and T. Schultz, “Advancing muscle-computer interfaces with high-density electromyography,” Conf. Hum. Factors Comput. Syst. - Proc., vol. 2015-April, pp. 929–938, 2015, doi: 10.1145/2702123.2702501.
 [11]	M. Paleari, M. Di Girolamo, N. Celadon, A. Favetto, and P. Ariano, “On optimal electrode configuration to estimate hand movements from forearm surface electromyography,” Proc. Annu. Int. Conf. IEEE Eng. Med. Biol. Soc. EMBS, vol. 2015-Novem, pp. 6086–6089, 2015, doi: 10.1109/EMBC.2015.7319780.
 [12]	A. Stango, F. Negro, and D. Farina, “Spatial Correlation of High Density EMG Signals Provides Features Robust to Electrode Number and Shift in Pattern Recognition for Myocontrol,” IEEE Trans. Neural Syst. Rehabil. Eng., vol. 23, no. 2, pp. 189–198, Mar. 2015, doi: 10.1109/TNSRE.2014.2366752.
 [13]	J. Liu, D. Zhang, X. Sheng, and X. Zhu, “Enhanced robustness of myoelectric pattern recognition to across-day variation through invariant feature extraction,” Proc. Annu. Int. Conf. IEEE Eng. Med. Biol. Soc. EMBS, vol. 2015-Novem, no. 1, pp. 7262–7265, 2015, doi: 10.1109/EMBC.2015.7320068.
 [14]	A. C. Tsai, J. J. Luh, and T. Te Lin, “A novel STFT-ranking feature of multi-channel EMG for motion pattern recognition,” Expert Syst. Appl., vol. 42, no. 7, pp. 3327–3341, 2015, doi: 10.1016/j.eswa.2014.11.044.
 [15]	P. Kaczmarek, T. Mánkowski, and J. Tomczýnski, “putEMG: sEMG Gesture and Force Recognition Datasets – Biomedical Engineering and Biocybernetics Team.” https://biolab.put.poznan.pl/putemg-dataset/ (accessed Feb. 16, 2021).
 [16]	H. A. Yousif et al., “Assessment of Muscles Fatigue Based on Surface EMG Signals Using Machine Learning and Statistical Approaches: A Review,” IOP Conf. Ser. Mater. Sci. Eng., vol. 705, p. 012010, Dec. 2019, doi: 10.1088/1757-899X/705/1/012010.
 [17]	M. A. Oskoei and H. Hu, “Support vector machine-based classification scheme for myoelectric control applied to upper limb,” IEEE Trans. Biomed. Eng., vol. 55, no. 8, pp. 1956–1965, 2008, doi: 10.1109/TBME.2008.919734.
 [18]	B. Hudgins, P. Parker, and R. N. Scott, “A New Strategy for Multifunction Myoelectric Control,” IEEE Trans. Biomed. Eng., vol. 40, no. 1, pp. 82–94, 1993, doi: 10.1109/10.204774.
 [19]	Y. C. Du, C. H. Lin, L. Y. Shyu, and T. Chen, “Portable hand motion classifier for multi-channel surface electromyography recognition using grey relational analysis,” Expert Syst. Appl., vol. 37, no. 6, pp. 4283–4291, 2010, doi: 10.1016/j.eswa.2009.11.072.
 [20]	F. Pedregosa et al., “Scikit-learn: Machine learning in Python,” J. Mach. Learn. Res., vol. 12, no. May 2014, pp. 2825–2830, 2011.
 [21]	P. Tsinganos, B. Cornelis, J. Cornelis, B. Jansen, and A. Skodras, “Data augmentation of surface electromyography for hand gesture recognition,” Sensors (Switzerland), vol. 20, no. 17, pp. 1–23, 2020, doi: 10.3390/s20174892.
 [22]	E. J. Nacpil and K. Nakano, “Driving Simulator Validation of Machine Learning Classification for a Surface Electromyography-Based Steering Assistance Interface,” Adv. Intell. Syst. Comput., vol. 1206 AISC, pp. 143–149, 2021, doi: 10.1007/978-3-030-51064-0_19.
 Links
 Project Repository
 Project Page
 Department of Computer Engineering
 University of Peradeniya
 Back to top",Recognizing hand gestures that could be useful to interact with machine interfaces using sEMG signals generated by muscle activations from forearm contraction. And try to build a model that could be used in a real-time environment,E15,Undergraduate Research Projects,https://projects.ce.pdn.ac.lk/4yp/e15/Hand-Gesture-Recognition-using-sEMG/,https://github.com/cepdnaclk/e15-4yp-Hand-Gesture-Recognition-using-sEMG,https://cepdnaclk.github.io/e15-4yp-Hand-Gesture-Recognition-using-sEMG,https://cepdnaclk.github.io/e15-4yp-Hand-Gesture-Recognition-using-sEMG/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/4yp/E15/Hand-Gesture-Recognition-using-sEMG/
85,Identifying keywords in legal articles using ML techniques,"Identifying Keywords in Legal Articles using Machine Learning Techniques
 Identifying Keywords in Legal Articles using Machine Learning Techniques
 Abstract
 This paper presents a survey of strategies and approaches for keyword extraction task. The paper provides an in depth review of existing analysis, additionally to the organisation of strategies. Related work on keyword extraction is concerned for supervised and unsupervised learning strategies.
 Nowadays there are plenty of legal documents offered in electronic format. Therefore, legal scholars and professionals are in need of systems able to search and quantify connotative details of those documents. Legitimate information and customary laws are generally offered in raw form and onerous to know, since they are not in organized form. All legitimate information is nowadays processed since the legal information gets generated often in a large volume due to the rise of law courts. The objective of this analysis is to explore an associate economical way to implement an algorithm to identify keywords by predicting the connectedness of legal documents from an enormous classification system which is difficult to do manually.
 The system to analyze this legal knowledge will serve effectively for lawyers and law students, which might address a lawyer’s role and may even become powerful to unleash such a task in future. Designers
 of such systems face
 a key challenge
 that the bulk of those documents are
 in natural language streams which are
 lacking formal structure or different specific linguistics information. During this analysis, we tend to describe associate unsupervised learning approach for automatically distinguishing necessary details in each legal document.
 The machine learning and deep learning algorithms based mostly analysis systems apply these strategies in the main for document classification. Legal document classification, translation, account, data obtention are part of the goals obtained from this research. During this study, we tend to review the various strategies of deep learning employed in legal tasks like Legal knowledge search, Legal document analytics, and Legal perspective interface. Through this review, we tend to instituted that machine learning models are giving advanced performance.
 Everything from README.md
 The information that was added to README.md should be added here as well.
 Team members: Names, Email, Student ID, Links to profiles
 Project Supervisor/s: Names, Email, Links to profiles
 Links
 Publications
 Introduction
 Keywords are considered as important parameters in a given context. For example people, places, words, or ideas that provides the idea of the relevant context. In this research,
 legal documents are used to obtain keywords, and thus the keyword is expected to convey a considerable idea of the
 legal document. Then, the key is the quality measuring parameter, which represents the importance of the given context.
 Keywords can be single word or multi-word keywords, which is known as key phrases. Phrases can be made by combining words together , and they usually generate a new meaning which is not related to the meaning given by single keywords. Therefore, if we take only single words as keywords, then , it would sometimes miss the significant things in the document.
 There are two factors considered in the process of identifying keywords. First, if
 a word is more frequently occurs in the document, then it can take as a keyword. And second, if
 a word is more frequently
 occurs in a speech,it has a less chance to take as a keyword of any document. According to the second factor the words that very frequently use in a speech, such as prepositions, conjunctions or
 common nouns, cannot be considered as keywords.
 What is keyword extraction?
 Keyword extraction is one of the text analysis methods, extract the most important words in the document and express the idea of it. It helps to get a summarizing of
 the content of the document. A keyword is a important unique word that convey whole idea of a document,or a word which is used to find information when studying legal cases. They can express approximately the overall idea of the document. Keywords are also called as ‘Search Queries’, since they are the words or phrases that people use when they are searching. Keywords are important since they provide the connection between what people search for and what system they have.When there are thousands of documents, keyword extraction helps to find the best matching document for our purpose. Keywords may be considered as a summary for a document which lead to have information extraction, or
 to categorize a document collection. However, in our case , there are relatively few documents keywords are assigned. Therefore finding methods to automate the assignment is important thing in legal context.
 Reading legal documents is a very difficult task and sometimes it needs some domain knowledge related to that document. And also it is hard to read the full legal document without missing the key important sentences and it is a very time consuming task. With an increasing number of legal documents it would be convenient to get the essential information from the document without having to go through the whole document. Hence manual extraction of keywords is slow, expensive and prone to mistakes.
 Finding database and e-Resource that provide legal and legislative information is vital need for lawyers in Sri Lanka.There are current implementations but those systems does not come up with efficient solution.There also manual work is costly.We need to reduce man work from the beginning of the portal.To implement user friendly and a system which learn itself to categorize documents the keyword extraction is essential. Also part of this research important information is mined. Therefore, many algorithms and systems for automatic keyword extraction have been proposed in the recent past. Those experiments are the basic background for this
 project.
 Methodology
 Website will contain judgements, statutes and various other content. We need an intelligent system, which can identify those.
 For that, when go through the documents, We found that there are set of special words and
 phrases surrounding the previous judgment.
 By referring set of documents, we found that there are some patterns in each document. Those can be a single word, phrase or a preposition like here,
 the case of, vide, held, the judgement of, in
 that have been used to introduce previous judgements.
 Those patterns will precede and follow with the name of the statutes also.
 We will be able to develop a comprehensive list of words that precede and follow the names of judgements.
 When we are doing this project, We have used
 Black’s laws online legal dictionary to identify the key concepts in the judgments, because we should understand how judgments are published and what are the key concepts
 Developing a comprehensive list of word patterns precede and follow the wanted informations because in machine learning, the algorithm should be able to identify and extract the details.
 And also there are words, which are unique for the particular document. Those words do not refer any previous judgment, legal document or lawyer name.Those are called keywords.To extract keywords we used TF-IDF method and Text Rank method
 Experiment Setup and Implementation
 To extract keywords,we used two different algorithms as TF-IDF method and Text Rank method.Bsically we used python language and libraries related to algorithm. For the experiment we took two diffrent formats such as NLR and suprime court doucuments. For the project we used 50 documents from NLR and 25 documents from suprime court documets. Also by using TF-IDF method we extracted five keywords from each document and Text Rank method we extracted 10 keywords from each documet.
 Results and Analysis
 We could identify the above mentioned key information for a given document. When considering keyword extraction results, TF-IDF Method table shows that TF-IDF methods shows 0.4347 accuracy for NLR documents and 0.5666 accuracy for supreme court documents. Text-Rank Method table results shows the number of correctly and wrongly identified keywords accordingly. With respect to that the NLR data-set has achieved 0. 3742 accuracy and supreme court data-set has achieved 0. 3960 accuracy.
 To evaluate the results we had to use manual method because its not like assigning keywords to other documents, legal documents have different context and assigning keywords need prior knowledge for legal documents. When it comes to automate the keyword extraction, therefore we had to do evaluation by manually.
 Conclusion
 The goal of this research is to discover answers on the questions of keyword identifying process of legal domain, especially, legal documents vary
 from others. It is considered about what are the things that make a legal document unique, which features important most in each document, if the formation is important in the applicable prediction, and what mechanisms work best for applicable prediction in the legal domain.
 During the experiments on data, some ideas on enhancement of the relevance prediction were proposed. Our plan was to implement a method searching documents using a single keyword or keyword phrase. There are still chances for additional improvements , which might rapidly accelerate and simplify lawyer’s work.Expectantly, this research will help everyone who are involved in the legal domain
 and software developers in coming-up decisions to obtain these improvements.\\Besides the analysis, a concurring result of the research was additionally the process of making a system that uses the discussed methods and is integrated with Lawciter which is an E-discovery system. The system conferred all documents of law cases. If the user testing , confirms
 quality and usability
 of the add-ons, it will come
 the finishing deliverance.","In Sri Lanka, it is vital need to implement a system to extract key information from legal document. This research focused on how to mine basic information from legal documents and how to apply machine learning techniques to extract keywords from documents.",E15,Undergraduate Research Projects,https://projects.ce.pdn.ac.lk/4yp/e15/Identifying-keywords-in-legal-articles-using-ML-techniques/,https://github.com/cepdnaclk/e15-4yp-Identifying-keywords-in-legal-articles-using-ML-techniques,https://cepdnaclk.github.io/e15-4yp-Identifying-keywords-in-legal-articles-using-ML-techniques,https://cepdnaclk.github.io/e15-4yp-Identifying-keywords-in-legal-articles-using-ML-techniques/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/4yp/E15/Identifying-keywords-in-legal-articles-using-ML-techniques/
86,Microservice Based Edge Computing Architecture,"Microservice Based Edge Computing Architecture
 Projects - Department of Computer Engineering
 University of Peradeniya, Sri Lanka
 Project Title
 Team
 E/15/048, Gayal Laksara, email
 E/15/243, Sewwandi Nisansala, email
 E/15/271, Sonali Prasadika, email
 Supervisors
 Dr.Upul Jayasinghe, email
 Dr. Isuru Nawinne, email
 Table of content
 Abstract
 Related works
 Methodology
 Experiment Setup and Implementation
 Results and Analysis
 Conclusion
 Publications
 Links
 Abstract
 With technological advancement, the adoption of advanced Internet of Things (IoT) technologies has improved impressively in the past few years. These services place such services at the extreme edge of the network. With such improvements, speciﬁc Quality of Service (QoS) trade-offs are needed to be considered. Some of such trade-offs are, particularly in situations when workloads vary over time or when IoT devices are dynamically changing their geographic position or when the data is needed to be processed in real-time and so on. Recent research has given much emphasis on realizing AI computing at the edge in contrast to cloud computing approaches to support the delay-sensitive IoT applications, autonomic decision making, and smart service creation at the edge in comparison to traditional IoT solutions. However, existing solutions have limitations concerning distributed and simultaneous resource management for AI computation and data processing at the edge; concurrent and real-time application execution; and platform-independent deployment. In our research, we focus on developing a novel platform and relevant modules with integrated AI processing and edge computer paradigms considering issues related to scalability, heterogeneity, security, and interoperability of IoT services. Further, each component is designed to handle the control signals, data flows, microservice orchestration, and resource composition to match with the IoT application requirements.
 Related works
 Distributed Computing
 Distributed Edge and Fog address some challenging scenarios of traditional cloud computing architecture. Chao Gong et al. presented ICE computing architecture that combines AI techniques and edge computing. They have achieved lower latency and a higher caching hate ratio at the edge to achieve a smart IoT [10]. Muhammad Alam et al. proposed distributed architecture with cloud, fog and edge devices, which makes sure that the data gets collected and analyzed at the most
 efficient and logical layer [11]. Edge Computing is inbuilt with predictive algorithms that may make decisions autonomously without looking forward to the cloud [7]. Fog Computing extends device-centric approaches to IoT development by introducing support for edge processing, network processing, and integration with Cloud Computing. Consistent with the research paper [12], fog devices will be
 classified as edge, IO (Input/Output), and compute nodes. In [13], the authors have presented the results of the efficient utilization of resources in the network infrastructure by using
 fog, cloud architecture. Here Fog computing results in solving the problem of latency in time-critical IoT applications.
 Microservices Architecture
 Microservices is an architectural style that structures an application as a collection of services that are highly maintainable and testable, loosely coupled, and independently deployable. These services are purposely built to perform a cohesive business function and are an evolution of the standard service-oriented architecture style [7], [14], [15]. In a microservice architecture, interdependent software components are individually configured as a microservice, where each
 service is liable for its small purpose. During a monolithic architecture, all functional logics for handling demands operate within the same process [16]–[18] [19]. There are some advantages in microservice architecture which are independent deployment, and fault isolation, meaning we are able to fix the fault only within the corresponding microservice otherwise the complete monolith to be re-developed, mixed technology stack which means we will use different technologies in several
 microservices.
 Methodology
 Design the Algorithm with Neural Network
 The main purpose of the AI algorithm of the project is to create a neural network from scratch in Python, which is capable of solving multi-class classification problems and can be distributed over the three-level architecture ROOF, Fog and Cloud. Some parameters of the algorithm should be transferred between each layer, then a certain model is not suitable for this case since the model can not pass through APIs from one layer to another layer. Therefore weight matrices are considered as parameters that are transferred between each layer. Softmax and cross-entropy functions are used as activation function and loss functions for creating the neural networks for multi-class classification. The cross-entropy cost function is used for optimizing the cost with softmax activation function at the output layer. There are two algorithms in our project one for predicting the vehicle’s speed and another for predicting air condition state in the vehicle.
 Proposed Platform
 To facilitate real-time processing and distributed communication at the edge, we propose a three-layer architecture, namely ROOF, fog, and cloud. Edge consists of fog devices and ROOF devices to process data in real-time but with less computation power and memory size. Fog acts as an intermediate level between ROOF and Cloud. Fog consists of more computation power and memory power than ROOF but not as much as Cloud. And finally, we have Cloud level to do higher computations to achieve the desired goals. This proposed architecture is network independent since this is three-layered architecture. The ROOF is the closest layer to the IoT devices and it does the AI computing on the sensor data from IoT devices. Here the horizontal distribution is also used to delegate the computational power on several nodes at the same time on ROOF and Fog layers. Therefore we have reduced memory and computational issues at the ROOF and Fog layers. Apart from this, there are policies that implemented to get high accuracy on the AI model which are discussed in section III.
 Even though this hierarchical architecture provides a solution for the real-time data processing issue, we needed a system that can have components that we can reuse. With that intend we move to the microservice-based architecture rather than going with a monolithic architecture.
 Experiment Setup and Implementation
 From the theoretical view, the proposed hierarchy and reason for going such a hierarchy is explained in Section III. The system is designed for the use case, an autonomous car. To validate and run the system we took a testbed approach.
 Implementation of the Prototype
 Since the system is designed only on a software basis we need a method to generate data in a way that happens in a real vehicle. In real vehicles, we have a microcontroller to collect data from different sensors such as Lidars, GPS, speedometers, etc. The microcontroller sent these data to the desired processing units to process and get the desired output. This is where the testbed is coming from. In our system, the testbed acts as our microcontroller and it sends data in a manner which the microcontroller sent. The dataset is found from Kaggle, provided freely by Victor R. F. (Car trips data log). As the ROOF layer, easily obtainable hardware which is a total of three Raspberry Pi 3s (RPis) is used as ROOF nodes. RPis 3 are single-board computers (SBCs) with 1.2 GHz CPU and 1 GB RAM, 16 GB storage disk while also having integrated WiFi. Due to the hardware limitations of a single Raspberry Pi, the processing is delegated through the three ROOF nodes. Due to the less processing power of Raspberry pi and the focus is to improve the processing power by delegating between the three of them. The three ROOF nodes interact using WIFI. Two laptops were used as the Fog layer. One with Intel® Core™ i3-3227U CPU @ 1.90GHz × 4 and Ubuntu 18.04.4 LTS as the operating system and the other laptop with Intel(R) Core(TM) i7-4600 CPU @ 2.10 GHz and Windows 7 operating system.
 We have used the Google Cloud Platform to provide cloud computing services at the cloud level. For that a machine type of e2-medium (2 vCPUs, 4 GB memory) with Ubuntu 18.04.5 LTS as the operating system. To communicate with these three layers, the Restful API method is used.
 Dynamic offloading
 Dynamic offloading improves the performance of ROOF architecture since it has lower computational power. In [20], the authors proposed task-centric and data-centric algorithms to analyze the threshold when the dynamic offload is happening. In our case, since the data is sent to the upper levels (FOG and Cloud), the data are not stored at the ROOF. Therefore the data-centric method is not suitable for this case. Here the task-centric algorithm is considered to do the offloading. Since the Raspberry Pis have less computation the overload can happen and it gets too much time to process data
 even the processing is delegated horizontally on several nodes. The tasks which get larger processing times in the nodes are offloaded to the least overloading nodes. The utility function for calculating offloading algorithm is as follows.
 Meaning
 Symbol
 Max device factor
 αa
 Min device factor
 αb
 Number of connected edge nodes
 En
 Threshold
 T
 Time per offload service
 βi
 Total time
 βt
 αa = 1/(En)
 αa = 1/(En + 1)
 T = (βi / βt)
 Offload occurs when,
 T > αa
 ### Microservices Implementation
 For developing this microservice-based edge computing architecture, we propose to use a three-level hierarchical system, Namely as ROOF, fog, and cloud. On each level to some extend the processing is happening and each level has AIbased microservices for doing specific tasks. Microservices we mainly used processing microservice, AC model training microservice, speed model training microservice, confusion matrix microservice, classification report microservice and accuracy microservice
 In our hierarchy, except AC Model Training Microservice and Speed Model Training Microservice, all the other microservices act as a shared resource to achieve their goals. The goal of the processing microservice is to get data from the testbed (for ROOF) and for other levels, lower-level processing
 microservice sent data to upper-level processing microservice. Further, the functionalities in the processing are splitting data to testing and training, and assigning separate APIs to respective results (e,g-: AC model x training data, Speed model x training data, etc). Speed model train microservice and AC model training microservice both are responsible for training the model for both the Speed and AC services. But all the accuracy, confusion matrix, and classification report microservices are responsible for providing accuracy, a classification report, and also a confusion matrix, and those results are used to validate the results. Here we have implemented a policy in a model train, which is it requests the accuracies from all the upper layers and if an upper layer has greater accuracy compared to its current accuracy, then the weight matrices of that corresponding upper layer are requested and replaced in the model. As a result of this, since the lower level has less computation and storage powers compared to its upper layer, there is a possibility that
 the upper layer may have achieved greater accuracy. So we can achieve the same accuracy level for lower layers by sharing the model in this way. The ROOF model can be seen in figure 1
 The whole hierarchy can be seen in Figure 2. All the functionalities happening in ROOF happens in fog and cloud. Additionally, we have a separate microservice called Global Accuracy at both fog and cloud levels. Global accuracy microservice is the one that responds for keeping the track of accuracy and weight matrices of near vehicles. It requests the accuracy from all the nearby vehicles and if a vehicle has higher accuracy, we update the global accuracy microservice with that vehicle’s accuracy and weight matrix data. A policy in this microservice is, at the start, we have seen with the
 lesser number of datasets we get about 100% accuracy. But this accuracy is not valid because it can not predict the correct outputs with the changing natures. The policy is, to update the global accuracy, the corresponding vehicle must have generated more than the size of 1000 data sets.
 As seen in figure 3, cloud level has some additional functionalities compared to its lower levels. Since the cloud is the topmost layer, all the input data coming from the testbed is saved in the cloud firestore for archiving reasons. The further initial plan is to use the cloud database service to act as a global accuracy saver, but since firestore does not allow us to save 2-D matrices we fall back to the strategy we used in the fog here. Further, we have developed a mobile app that is interconnected with a special service provided with the use of cloud functionalities. The service is to give a fuel consumption assumption for the user by combining speed and ac control data. The mobile app is for the user and the user can give the current location and destination with the available amount of fuel. Those data sent to a service running in the cloud which calculates the rough assumption of fuel consumption with the speed and ac data at cloud level, and the result is sent back
 to the mobile app
 lementation of the NN Algorithm
 As mentioned above, in the methodology section, the algorithms are divided into two sub-phases as the feed-forward phase and the backpropagation phase.
 Results and Analysis
 Conclusion
 With the staunch objective of providing real-time processing at the edge, we have developed a microservice-based AI computational hierarchy. The processing happens in both vertical hierarchical manner(ROOF, Fog, Cloud) and also horizontal hierarchical manner(In the ROOF level). Each level has its own policies (Accuracy checking from upper levels…etc) to control the flow of data and how the process should be distributed.
 Publications
 Semester 7 report
 Semester 7 slides
 Semester 8 report
 Semester 8 slides
 Author 1, Author 2 and Author 3 “Research paper title” (2021). PDF.
 Links
 Project Repository
 Project Page
 Department of Computer Engineering
 University of Peradeniya
 Back to top",,E15,Undergraduate Research Projects,https://projects.ce.pdn.ac.lk/4yp/e15/Microservice-Based-Edge-Computing-Architecture/,https://github.com/cepdnaclk/e15-4yp-Microservice-Based-Edge-Computing-Architecture,https://cepdnaclk.github.io/e15-4yp-Microservice-Based-Edge-Computing-Architecture,https://cepdnaclk.github.io/e15-4yp-Microservice-Based-Edge-Computing-Architecture/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/4yp/E15/Microservice-Based-Edge-Computing-Architecture/
87,Mixed Reality based Simulation Platform for Swarm Robotics,"Mixed reality based simulation platform for Swarm Robotics
 Projects - Department of Computer Engineering
 University of Peradeniya, Sri Lanka
 Mixed reality based simulation platform for Swarm Robotics
 Team
 E/15/140, Jaliyagoda A.J.N.M., nuwanjaliyagoda@eng.pdn.ac.lk
 E/15/142, Jayalath A.H.G.D., ganindudananja@gmail.com
 E/15/173, Karunarathna S.D.D.D., dinelkadilshani95@gmail.com
 Supervisors
 Dr. Isuru Nawinne, isurunawinne@eng.pdn.ac.lk
 Prof. Roshan Ragel, roshanr@eng.pdn.ac.lk
 Table of content
 Abstract
 Introduction
 Methodological Approach
 Implementation
 Experiments and Analysis
 Conclusion
 Links and References
 Abstract
 The term “Swarm Intelligence” is the collective behavior of a combination of many simple individuals, where they operate autonomously. “Swarm Robotics” is the application of swarm intelligence used in collective robotics. This has been a new approach to the coordination of mass of robots that are capable of local communication, decentralized controlling, autonomous and also operations based on biological inspiration senses. In order to achieve the highest effectiveness of swarm robotics applications, virtual reality has been used.
 Mixed Reality (MR) was originally derived from Virtual Reality (VR). When designing MR systems, users are provided with the illusion that digital objects are in the same space as physical ones. Mixed Reality is typically correlated with Virtual Reality by the solutions that have been made to address the problems related to robotic applications. However, the use of MR was clearly identified as very useful from just only VR implementations by its flexibility, scalability and availability with respect to each implementation.
 Testing and experimentation of robotic applications could be made far easier than VR with a significant increase of control over various environmental constraints and limitations. Hence, we combined both the virtual and physical robots and created a swarm robotics platform in mixed reality. Furthermore, we conducted some experiments to test the functionality of the system in the mixed reality environment and represent the behavior in the web-based simulator we developed.
 Introduction
 Swarm Robotics
 Over the past few decades technology has been evolving rapidly in so many aspects and fields to reach out to the human population in a more user-friendly manner. With these technological developments, one such industry that took a huge leap with an advanced improvement is the field of robotics. Even in robotics, swarm robotics has been a major breakthrough.
 The term “Swarm Intelligence” refers to sophisticated collective behavior that is being emerged from the combination of many simple individuals, each operating autonomously. It consists of a biologically inspired emphasis based on the emergence of global behavior, also on decentralized local control and local communication. “Swarm Robotics” is the application of swarm intelligence used in collective robotics.
 This has been a new approach to the coordination of mass of robots that are capable of local communication, not controlled centrally, autonomous and also operations based on biological inspiration senses.
 When it comes to swarm robots, this concept has been derived from the behavioral patterns of creatures like ants, wasps, locusts, termites, bees, fishes, turtles and birds (Figure 1.1). Swarming is seen as a behavioral pattern because they move together in search of food and shelter to survive from predator attacks. That is since the discrete individuals have a higher chance of surviving in the group than being alone. Swarming results in responding to the speed of their peers to avoid collisions within the swarm by communicating with each other while maintaining a decentralized network and exhibiting self-organized behavior. The goal of these creatures in swarms is basically to ensure the process of solving problems more efficiently through cooperation and division of labor which is being modified and infused into swarm robotic technology.
 Figure 1.1 Examples of diverse creatures in swarms
 One of the main advantages of swarm robots is to outperform individual robots as they accomplish tasks concurrently and faster than individual robots. Even if these tasks are too difficult for a single robot to accomplish, it would be very much convenient for the swarm robots.
 The main characteristics of a swarm robotics systems are as follows,
 Robustness: The system’s motionlessness or in other words not showing any biasing or emotion towards randomly occurring changes within its surrounding environment where it enables the system to perform its designated tasks despite any disturbances.
 Autonomous: It is the system being independent of each other while interacting with each other and its surrounding environment.
 Local Communication: Swarm robots do not have a comprehensive understanding of their environment. Due to that, the interaction between individuals is based on the concept of local communication which is termed as a stigmergy mode of communication.
 Cooperation: Swarm robots are not capable of completing an allocated task individually, it is needed for them to work together. Therefore, cooperation can be considered as a compulsory characteristic found in swarm robots because the tasks are too difficult to be carried out by a single robot.
 Flexibility: This is the ability of the system to perform different tasks at one time
 Aggregation: This is the process of grouping the individuals of the swarm into a cluster without using any external impact which is very crucial in swarm robots as it plays an important role in all three co-operation, communication and interaction aspects.
 Virtual Reality and Mixed Reality
 Virtual reality is another major part of the technical field which is experienced through sensory stimuli such as sights and sounds provided by a computer and partially determines what happens in the 3D computer-generated environment.
 Virtual reality describes the simulation of a synthetic environment similar to the actual environment. The term “virtual reality” was introduced by computer scientist, Jaron Lanier for the first time and he founded the first company (“VLP Research”) to develop VR products like VR goggles, joystick, data gloves, and “Bird 3D” electromagnetic tracker.
 Virtual reality has been used in flight simulation training for pilots, procedural training for surgeons, phobia treatments, disorders, gaming consoles, etc. When creating a virtual environment for these applications, the user should provide visual feedback by a head-mounted device, projection system, or a flat-screen to gain the virtual effect.
 The Virtual Fixture System which was invented by Armstrong Laboratory located in Arlington, Texas, USA presented a Mixed Reality (MR) incorporating features of the sight, sound, and touch. Later Milgram and Kishino introduced the “virtuality continuum” concept that linked the real and the virtual world (Figure 1.2). Milgram’s scale represents the real environment and the virtual environment at the ends while introducing a new concept known as Mixed Reality (MR) in the middle. According to Milgram’s diagram, there are two broad areas, Augmented Reality (AR) and Augmented Virtuality (AV) which belong to the MR. However, the medium that represented a combination of real and virtual environments mostly known as AR rather than MR.
 Figure 1.2 Milgram’s Reality–Virtuality continuum
 When it comes to Mixed Reality (MR), it has been originally derived from Virtual Reality (VR) not only conceptually but historically as well. Here, both the physical environment around them and digital elements presented are distinguished by users, for example, the use of semitransparent displays. When designing MR systems, users are provided with the illusion that digital objects are in the same space as physical ones. By sensing those physical interactions, it provides interactive feedback to the users.
 Figure 1.3 Milgram’s continuum with examples from Media Convergence Laboratory projects.
 Left to right: Physical reality, Augmented reality, Augmented virtuality, Virtual reality
 Challenges in Swarm Robots
 There are many possible reasons for the absence of robot swarms in the real world, for instance, the hardware limitations of the available robots. Here we have discussed some open problems.
 1. Scalability
 Swarm intelligence needs a considerable number of robots with the corresponding features to simulate the algorithms. So, we identified scalability as a major fallback of swarm intelligence-related research. Unless there is a large number of robots to test these algorithms it is difficult to do experiments in the real world. Building a group of robots takes a lot of time to build hardware components of particular robots. Not only that, as the required parts to build a single robot are very much expensive, when considering a bulk of robots to be built it will cost even much more than the affordable values.
 As a solution to this problem, implementing robots with hardware capabilities to run basic swarm intelligence-related algorithms will allow robots to be multifunctional. However, buying a set of pre-built robots does not solve the whole problem since it is too expensive yet. Another solution is to use computer-based simulation software. Nevertheless, the problem with that approach is that the simulators do not guarantee how robots act in an actual environment, how they react to complex physics, noisy sensor data, control loop delays, etc.
 2. Physical Execution
 Some applications like search and rescue, explorations in extreme regions and bomb disarming are too dangerous for human beings to be carried out. To avoid difficulties during these missions, swarm robots can be deployed.
 Even if swarm robots are used to complete these tasks, yet there is a high risk of swarm robots being destroyed during these extreme conditions. Therefore, building new robots each time will be even more of a misspending of money.
 In some swarm intelligence-related algorithms, the result mostly depends on the number of robots. Therefore the test result by using few physical robots can not be used to prove the correctness or scalability of the algorithm.
 In the development and testing life cycle, the actual operations in MR environments can be identified as much simpler with the ability to separate system components with ease and the transition of system components between each environment. Hence, the MR platform implementations are very useful for the applications of Multi-Robot or Swarm Robot architecture research topics and other fields as well for an instance; Embedded systems and IoT (Internet of Things) projects.
 Proposed Solution
 Virtual Swarm Robots
 Virtual swarm robots are often very useful to perform simulation prior to the investigation of real robots. Simulations are easier to set up, less expensive, normally faster and more convenient to use than physical swarms. Virtual swarm robots need a simulator to interpret the changes and the movements of each robot and also to test the swarm intelligence-based algorithms. The two main requirements in virtual swarm robot simulators are the flexibility to add new features and efficiency. There are many simulation platforms like Player, Stage, Gazebo etc.
 Mixed reality in Swarm Robots
 MR is a relatively refreshing and new area of technology although many implementations and experiments have been carried out over the past years. It can be categorized under an expanding field of expertise due to its promising potential for a large number of applications in various fields and purposes comprising testing state-of-the-art computer architectural systems, process optimizations, training and testing of hardware components for Machine Learning applications, etc. The immense interests in many researchers for MR are almost related to robotics, especially Multi-Robotic systems and Swarm Robotic systems. In particular, MR based robotic development will require additional attention to detail for tasks including collaboration between robots, additive manufacturing and other related manufacturing tasks to achieve interfacing, programming related outcomes in software level and functionalities.
 It is known for a fact that developing a physical robot that specializes in some certain production level functionality will have a higher bill of material to some extent because of certain advanced hardware components and devices. Hence, there exist certain limitations for development and testing physical robots. However, with the recent interests and advances in virtual sensing and related technologies, using MR is a promising solution for reducing the experimental and development costs. especially for scaling up swarm robots. Human-robot interaction due to the ability to separate between the physical and virtual robots with MR is considered safe. Hence, MR creates safer and low-risk environments for extensive testing of Swarm Robotic behaviors.
 MR implementations tend to merge virtual and physical realities where enabling a robot to sense both physical and virtual environments via augmented means, allows the ability to interact with both physical and virtual environments and experiment on robot behaviors on simulated environments with simplified addition and removal of obstacles to an extent. Their collaborative design patterns and individual functionalities could be accessible and monitored remotely for debugging and development. This remote accessibility is a more flexible feature than any of the functionalities that can be seen in other control system architectures. The so-called “spatial flexibility” allows the collaboration between the researchers, developers and the test subjects to be not limited by some parameters including geographical constraints. The work done by Freund and Rossman describes a mixed reality robotic representation in which a physical robot executes control commands propagated and translated from the virtual environment that is allowed to be operated by a human. This enables the use of an MR approach called “Tele-Immersive Environments”.
 MR environment allows adding or changing virtual features to robots that may be too costly, time-consuming, or impossible in reality. As an example, for the practical implementations, adding 8 or more bearing sensors for all the required directions is costly and not practical in small robots. However, with the aid of virtual sensing, it is possible to add that sensor as a virtual sensor by using the technology of mixed reality.
 All over the world, in the field of robotics, many applications with different approaches have been executed which were inspired by this exposition. MR based swarm robots have especially been one such advanced application implemented so far in the industry. The very same concept was used in this project of ours. First of all, a few physical swarm robots with basic functionalities were built and they were interpreted in a graphical user interface with other virtual robots.
 Deliverables and Milestones
 Mixed Reality Simulator
 There are many factors associated with the design of Mixed Reality-based Swarm Robotic applications that are considered as unique to that particular instance of implementation and use case. Many factors can be classified as predictable characteristics and unpredictable characteristics. Dealing with these characteristics in a systematic manner is a crucial part of such implementations.
 Regardless of the complexity of characteristics, a common challenge among the representations in swarm robotics is the interaction with the environment. These interactions can be visualized as characteristics which are measurements and sensor readings (thermal, sonar, IR, etc.) and behavioral events. Visualization of such interactions between software and hardware represented with a simulation using AR.
 AR provides a means with an extent and more accurate visualization of robots in the real world while many non-AR visualization or simulation provides a simpler visualization from ad hoc means. Proper monitoring (or potential controlling) is needed to work with these visualizations because working with predictable or unpredictable characteristics will eventually lead to inconsistencies and bugs in the system. With the accuracy provided by the AR, the system is also able to provide a real-time simulator environment and it was implemented web basely.
 Localization of physical robots
 A precise mapping between physical and virtual reality is required for mixed reality, so the system must know the position of relevant physical objects relative to the display system. To maintain this coexistence, the exact coordinates of the physical robots needed to be identified.
 Dead Reckoning is a common technique used for Differential Drive mobile robots to measure the offsets from the start coordinate. It uses a rotary encoder attached to its wheels to measure the angle of rotation of each wheel and calculate the offsets using simple trangiometry. However, this method has cumulative errors, due to erroneous sensor readings or precision of the sensors.
 Therefore, it is better to use a combination of localization methods, which can be able to eliminate the cumulative error. Followings are few approaches for ranging measurements, can be used for localization.
 GPS: A good solution for geographically spread swarms, but cannot be used indoors because precision is less due to interference by obstacles.
 RSSI: There are two problems with using RSSI (Received Signal Strength Indicator) as a ranging technique used with RF devices such as Bluetooth or ZigBee which are not stable since some protocol takes measures to ensure the higher quality link and it is dependent on the device orientation. RSSI for ranging might be applicable in a static environment with fixed device orientation but it is not adequate in a moving device system.
 Infrared: An array of Infrared distance sensors for each robot can measure relative distances between each robot/the environment and calculate the relative position using MEDUSA localization system. The problem with this approach is the interference from sunlight and measurable distance limitation.
 Laser Ranging sensor: This method uses a rotating laser distance sensor and measures the Acoustic Signal. This method is relatively low cost and the range is limited to 5m.
 Ultrasonic with RF pulse: This implementation uses an Ultrasonic pulse with a radio frequency pulse. The distance is measured by the receiver based on the delay between Radio pulse and Ultrasonic pulse. This approach is used in Cricket Localization systems.
 Image processing with an overhead camera: Can use an overhead camera and marker with a unique ID for each robot. Image processing based on marker identification is used as the tracking algorithm. This method is limited to small areas, based on the camera angle and the resolution.
 So, after considering all the advantages and disadvantages of the above-mentioned solutions, an overhead camera with markers was selected to identify the coordinates of the physical robots. It uses image processing with OpenCV and ARMarker support libraries.
 Communication
 The communication between each robotic instance and with the simulation platform in a mixed reality representation is crucial because frequent communication is a key part in representing the outcome of the simulation platform in the simulation server itself and the visualization system. The communication system is to be handled using MQTT because of its lightweight and event-driven functionality. A distributed system with repository data architecture is the approach of implementing the communication module for the simulation.
 Methodology
 As described in the previous sections, developing a large number of physical swarm robots is not practical within constraints such as budget and time. Also, simulating the swarm behavior in a purely virtual environment does not give a guarantee about the real-world execution. Hence, the solution we suggest is a hybrid method, by combining both the characteristics of physical and virtual robots with the aid of mixed reality technologies.
 Our proposed system consists of several parts as listed below;
 Physical Robots, which are integrated with real sensors and actuators.
 Virtual Robots, virtual representation of the physical robots, which can be added to the simulation arena by software level definition.
 Localization system, a system that recognizes the movements of the physical robots and maps them into virtual reality.
 Mixed Reality Simulator, a software application that can interface with both physical and virtual robots and model the functionality in a mixed reality environment.
 Visualization platform, a virtual environment, which can visually update the behaviors of both realities.
 In a nutshell, the overall Mixed Reality Simulation platform (Figure 2.1) is a collection of several decentralized and distributed components which are connected with each other by various aspects including reality, communication mode (synchronous or asynchronous), visualization, etc.
 Figure 2.1 Overall Abstract System Architecture
 A simple block diagram of the whole mixed reality simulator system is shown in Figure 2.2. Physical robots and Virtual robots communicate with the simulator back and forth via MQTT protocol. Also, both Physical and Virtual robots send their current states to the Visualizer to represent them in an MR environment. The visualizer renders the 3D view of the robots and obstacles of both realities. Visualizer is only a representative body and it can not take decisions and control the behaviors.
 Figure 2.2 Interactions between subsystems
 Conceptual design
 Physical and Virtual Robots
 The basic requirements of the physical robots are to be general-purpose swarm robots, which can be used for swarm intelligence behavior related experiments. Those robots should be able to move in the simulation platform and interact with other physical robots as well as with virtual robots.
 Virtual robots are designed to behave as the same as physical robots, but virtually. They can communicate with the virtual robots and also with physical robots with the aid of the simulator. Since virtual robots are generated as instances, it can be scalable to any number and the same algorithms which are running on the physical robots can run on the virtual robots as well.
 The architecture of virtual robots we have built is similar to the firmware architecture of physical robots to maintain consistency. Physical robots have real physical sensors and the virtual robots were provided augmented sensors, which reads the mixed reality environment measurements through the simulator using specific communication channels.
 Physical robots have physical motors and wheels. There are functions in robot firmware, which can control the speeds of the motors using PWM signals. Since there are no motors in the virtual robots, the movements with the given motor speeds are calculated using a mathematical model known as Dead Reckoning for differential drive robots, as shown in the Figure 2.3.
 dx = (R/2)(VR + VL)cos(Φ)
 dy = (R/2)(VR + VL)sin(Φ)
 dΦ = (R/2)*(VR - VL)
 Figure 2.3 X and Y coordinate change with given speeds for left and right motors
 Robot Localization
 We recognized that there must be an accurate and effective way to track the movements of the physical robots and map them into the mixed reality environment and represent them on a mixed reality visualizer. After considering a lot of possible options and as a result of the literature survey that we conducted, we came to the conclusion, that having an image processing-based localization system would be the best solution to this project. An abstract localization module was identified as a requirement for the simulation that is to be used in the simulation server, to keep the localization information of the robots of both realities, and also having the capability to customize and scale up in the future for any other mixed reality swarm robotic representations.
 Physical robots have an AR marker on top of them. An overhead camera was set up on top of the physical simulation environment as shown in the Figure 2.4. The localization data (x coordinate, y coordinate and the heading direction) are calculated based on the video feed and mapped into the MR simulator environment with the aid of the AR library of the OpenCV. Then the coordinate data will be sent to the simulator via a predefined MQTT topic, as an event-triggered update for each and every individual physical robot. This updated event will be triggered only if the robot moved or rotated than a given threshold value.
 Figure 2.4 Localization system on the physical arena
 Mixed Reality Simulator
 Mixed Reality Simulator is the most important part of the whole system. It helps the sub-components to interact with each other. As an example, the simulator can provide the possible distance sensor readings for a virtual robot, based on its own location. Here, the simulator will consider both physical obstacles in the physical simulation arena as well as the virtual robots and virtual obstacles when calculating the distance sensor reading for the virtual robot.
 The status of the robot entities is reflected in the simulator and vice versa using controllers and emulators that we have implemented [Figure 3.5]. The term “emulator” refers to a functional service that is responsible for providing virtual actuator/sensor support. For example, Virtual Robots do not have actual distance sensing capabilities, hence a distance emulator is required to mimic the functionality of distance sensors for the connected virtual robot instances. The emulators differ from controllers as they do not directly change or more generally, “manage” the characteristics of certain entities in the simulator as with controllers (eg: Obstacle Controller).
 Methodological Approach
 Obstacles in Mixed Reality
 When doing experiments with swarm behaviors, we need to have specific environment setups. For example, we want to have walls, specifically shaped obstacles, etc., based on the experiment.
 When simulating the robot behaviors in a Mixed Reality environment, we may have those obstacles in both physical and virtual realities, and the physical robots should ‘sense’ virtual obstacles as well as virtual robots should ‘sense’ both virtual and real obstacles.
 As previously mentioned, our one goal is to make the simulator modular, and flexible. So we came up with a special interface to represent the obstacles in a mixed reality environment via the support of the simulator.
 Different types of obstacles from primitive shapes such as boxes, cylinders, spheres can be designed by following the interfaces and users can implement the functions defined in the interface.
 For the defined obstacles, it is possible to implement the above-mentioned methods by modeling
 the behaviors using geometry.
 Figure 2.5 Obstacles in MR visualizer
 For example, to create a wall obstacle we need the start coordinate, orientation relating to that point and the length of the wall. When calculating the distance to the obstacle through the heading direction, first it is needed to calculate the heading angles relative to the two endpoints, P1 and P2 of the wall obstacle shown in the Figure 2.5.
 angle1 = Θ − α
 angle2 = Θ − β
 If one of them is positive and the other angle is negative we identified that the obstacle is in the front of the robot. The below logic will give true or false if this condition is satisfied.
 (|angle1|≤90 or|angle2|≤90) and (angle1 * angle2 ≤ 0)
 To calculate the distance, we need the line equation of the wall obstacle and also the line equation through the heading direction. It can be obtained by using this equation.
 sin(angle)x − cos(angle)y − x0  sin(angle)+y0  cos(angle)=0
 Then we can find the intersection point of two lines and can calculate the distance from the coordinate of the robot to the intersection point.
 x= (b1c2 - b2c1)/(a1b2-a2b1)
 y= (a2c1-a1c2)/(a1b2-a2b1)
 distance = √(xDiff2 + yDiff2)
 After modeling the obstacles using this method, it is possible to build the environment with the support of these obstacles. We can make virtual entities for physical existence obstacles and mark them as real obstacles, and add some virtual obstacles and mark them as virtual obstacles. Detailed explanation on how to use these obstacles with Robots will be explained in the Reality Integration Section.
 Apart from the user-defined obstacles, the server considered virtual and physical robots also as moving obstacles.
 Augmented sensing and Reality Integration
 Physical robots can not sense the virtual robots and virtual obstacles from their inbuilt hardware sensors. Similarly, the virtual robots do not have any sensor to sense physical robots, obstacles and also other virtual robots and virtual obstacles. So to give the effect of mixed reality, the simulator acts as a broker or interface between the entities. It keeps track of the robot coordinates, given by the localization system for physical robots, and coordinates of the virtual robots reported by individual robots themself.
 The simulator contains the data on both realities and feeds the required details to the robots. Physical robots get the physical sensor readings of physical obstacles by their inbuilt sensors and virtual sensor readings of virtual obstacles through the simulator. Then it takes the minimum of those readings and detects the closest obstacle as in the Figure 2.6. Besides, virtual robots will request the sensor reading of both realities and the simulator reacts accordingly.
 Figure 2.6 Augmented sensing in simulation
 Limitations and considerations
 When designing the methodological approach the simulator and the visualizer need to be updated in real-time in order to give the mixed reality effect effectively. However, there is a considerable amount of delay during the transmission and we neglected it as it was inconsiderable and cannot handle by ourselves.
 As we discussed earlier, we designed MQTT protocols for communication. Through that design, it ensured confidentiality, integrity and availability which are the main components of security, up to some level. However, apart from the QoS supported by MQTT, we did not consider the successful delivery of the MQTT packets. This is because it makes unnecessary complexity and generates blocking calls.
 Furthermore, we are concerned about the research aspect of the mixed reality environment and not about security. Therefore the communication protocols do not include special authentication. Since MQTT brokers are connected with robots through authentication and it shares the username and the password between every robot. There is no control after the connection since no implementation was provided from the MQTT. However, this can be continued as future work.
 Implementation
 Hardware Implementations
 We identified several types of sensors that help to identify the properties of the environment and the behaviors of the robot itself. After considering a lot of possible designs we selected the following design (Figure 3.1) for the swarm robots. The round shape helps robots to tolerate the collisions (no any entanglements possible) and the special flattened edge of the back helps to identify the orientation of the robot from any view.
 Figure 3.1 Overview of the Physical Robot
 The top cover contains a 6x6 pixel AR marker, which helps to track the robot’s coordinate and orientation from the overhead camera.
 There is a distance sensor and a color sensor in front of the robot, and those can be used to explore the distance to obstacles in front of the robot and the color of the obstacles if there are any nearby. The distance sensor can measure a point distance maximum of 200cm.
 The robot contains a compass and accelerometer module, which can be used to measure and calculate the orientation of the robot.
 Each robot contains 4 IR transmitters (one outgoing channel) and 4 IR receivers (4 separate incoming channels), which can transmit and receive 32bit binary values (can be extended until 64 bit) and possible to use those as a communication method between robots.
 Also, a robot has a ring of 20 RGB LEDs, which can be used to give a visual indication of the robot’s status. It can be used as a robot to a robot communication method, with the aid of the color sensor,
 The physical robots have two geared motors with optical encoders which can measure the amount of rotation or the distance traveled by the robot with a 3mm step.
 The microcontroller of the robot has inbuilt WiFi and Bluetooth communication facilities and WiFi is currently used for communication with the swarm simulator.
 Robots are powered by 2 Li-Ion batteries, and the power distribution circuit has an inbuilt battery protection circuit, which protects batteries from overcharging and over-discharging.
 In the front bottom of the robot, there is a DIP switch with 2 toggle switches. It can be used to switch between 4 different behavior algorithms defined by the firmware.
 Implementation of virtual robots
 Virtual robots were implemented as virtual swarm nodes using java language. The robot class constructor has two attributes, an Id and the reality to differentiate the two types by the simulator. Also, there are some methods as stated in the diagram below (Figure 3.2). We created virtual robots by extending those features and included other features such as sensor interrupts and communication interrupts which needed to be built only for virtual robots as an abstract interface in software level since physical robots have implemented them in the hardware level. Then those methods can be overridden and can be implemented with the desired functionality according to the requirements of different swarm algorithms.
 Figure 3.2 Class diagram of the virtual robot
 As in the Figure 3.2, there are two sensors which are color sensor and distance sensor. Since they are built virtually those sensors get updates from the simulator with the respective readings.
 We build a setup method and a loop method to imitate the hardware functionality of the physical robots. The loop acts as an event loop, which manages multiple events and behaviors such as interrupt checking, MQTT communication, etc., as similar to a physical robot. Inside the setup method, the necessary objects of sensors, indicators and communication were created.
 Furthermore, these virtual robots were implemented as a finite state machine with three states which are wait, run and begin. The transferring methods between those states were defined in the IRobotState interface, as start(), stop(), reset().
 In addition to testing the functionality of these robots, we implemented a few swarm algorithms like color ripple formation, discovering obstacles and obstacle avoidance algorithms in virtual robots. We will further discuss these in the Experiment Section.
 For each virtual robot instance, we created a separate thread and performed their functions in it, so that each instance will run parallelly.
 Apart from the Java implementation described above, we have considered a JavaScript implementation of a Virtual Robot instance realization in our early stage of the development to address the asynchronous feature in a given swarm experiment. It is more event-driven than traditional approaches and it also followed the same base approach with the Java realization as it is one of the major expected outcomes of real-life swarm robotic experiments.
 The JavaScript implementation consumes the modules and classes from the “pera-swarm” library and some are customized for specific control capabilities. However, the virtual sensor implementations required some sequential procedure calls that needed synchronous function calls instead of callbacks. Also, it required an additional overhead of customization for specific robot models. Therefore, we did not further develop the asynchronous JavaScript robot instance realization, whereas this experiment consisted of synchronous robot instances.
 Swarm Simulator Architecture
 As previously described in the above sections, it is required to implement an interaction server to handle the reality integration between physical and virtual swarm agents as well as other objects.
 Swarm Simulator contains two parts, a Mixed Reality Simulator and a Mixed Reality Visualizer. The simulator is a helper server to simulate things in a Mixed Reality environment and the Visualizer is used to visualize both realities in a single environment using a web-based virtual environment.
 Simulator Server
 Considering the possibility of easy modification, it was developed by following a modular approach. First, we implemented a general interface specifying the structure and some abstract methods. Then we created classes by implementing those interfaces since it was easy for us to define a protocol of behaviors that could be implemented anywhere in the class hierarchy and also to implement new features like virtual sensor emulators and helpers for swarm behavioral experiments.
 The simulator application uses the modules from the JavaScript libraries “pera-swarm” and “@pera-swarm/mqtt-router” (Library_Implementation Section) to address the swarm logic and other functional level requirements. The libraries were implemented for the general swarm robotic use case and are open for improvements to address specific swarm behavioral requirements for researchers.
 Figure 3.3 Swarm Simulator - UML Overview
 The Figure 3.3 describes the high level UML representation of the swarm simulator application. The Swarm class is associated with four modules from the “pera-swarm” library and they are further customized according to the use case of the experiments. The Robots module is attached with the Swarm class in a composition relation and it contains five modules namely, Color Sensor Emulator, Distance Sensor Emulator, Simple Communication, Directed Communication and NeoPixel Agent. These modules are again coming from the said library and they address robot-related functionality.
 The Following describes the high-level modules associated with the Swarm class.
 Scheduler Service: A service to manage the session timeout for robot pruning within the simulator so that inactive robot instances are filtered and removed from the simulator cache and a callback function to communicate with the other applications after the scheduler event.
 Localization Controller: A broker module which handles localization-related communication messages to propagate throughout the simulator and other applications.
 Environment Controller: A controller module which handles and manages the arena configurations and obstacles in a given instance throughout the experiment. This will consume a JSON configuration file that describes the environment configurations according to the experiment.
 MQTT Router: A customized MQTT Router module from the “@pera-swarm/mqtt-router” library to handle MQTT connection and routes each and every communication message to the related handler functions.
 The emulator modules that are composed in the Robot class are basically functional services responsible for providing virtual actuator/sensor support as described in the above sections. They do not directly take decisions and only provide virtual sensing capabilities. The Color Sensor Emulator and Distance Sensor Emulator implement a basic virtual sensing functionality based on the following concepts.
 Virtual Distance Sensing: A simple broker implementation of an actuator realization for distance sensing capabilities for virtual robot instances using the following MQTT communication topics. The module will calculate distances between the given robot instance and a selected obstacle or another robot instance for the following MQTT topics. - /sensor/distance/[robotID]/? - This will request distance sensor readings from a robot by the Simulator. - /sensor/distance/[robotID] - Simulator will inform Mixed Reality Environment readings to the robot, as a reply to the topic /sensor/distance. - /sensor/distance - Robots can request mixed-reality sensor reading from the simulator through this topic. There is an optional parameter, ‘reality’ is used to request the reading only on a specified reality. Reply from the simulator will be received through the topic /sensor/distance/{robotID}.
 Virtual Obstacle Sensing: The simulator contains emulators for various sensors and those sensors should feel the obstacles. Hence, we included several methods in these interfaces to support those emulated sensors. The Following are some of them.
 \begin{itemize} - isInRange(heading, x, y): This will return a boolean value, true or false about the existence of the obstacle within the heading direction, from the given x,y coordinates. - getDistance(heading, x, y): This will return the distance to the obstacle, from x,y coordinates along heading direction.
 Mixed Reality Visualizer
 The simulator platform needed to be represented in a seamless way that the changes of each robot instance with their movements as well as the obstacles in the environment should be clearly identified for the users. Not only that, users should have the capability to filter these entities in the visualizer by the reality of choice for a given experiment. So, we chose the framework three.js (https://threejs.org/) to develop the Mixed Reality Visualizer with these functionalities:
 The movements of the robot instances are represented according to their reality in nearly real-time.
 The obstacles in the experiment environment are represented according to the configuration described in the Simulator with few considered limitations.
 The robot units and obstacles are given unique labels to distinguish each and every one of them.
 A control box to filter labels and entities according to their realities as well as choose whether or not to display robot snapshot information.
 A small statistical tool to display the performance metrics (e.g.: FPS counter, visual latency, etc.) of the visualizer application.
 Figure 3.4 A Screenshot of the Mixed Reality Visualizer
 The visualizer (Figure 3.4) represents the mixed reality information according to the simulator configuration and robot instance information. The application consumes an MQTT connection that follows the communication protocols that are described in the Communication Protocols Section}.
 Library implementation
 The realization of the simulator platform followed both a generalized architecture towards robot units and their behavior towards the environment while developing a specific simulation environment for the experiments. To address a general use case and encourage future work towards the mixed reality realization method, we wanted to develop a collection of open source libraries. The Swarm Server was implemented using Node.js (https://nodejs.org/) and the Visualizer was developed using a native stack (HTML, CSS, JavaScript), making Node.js our choice of platform for the libraries.
 The library “pera-swarm” (https://github.com/Pera-Swarm/pera-swarm) is the center of the library as it contains the modules that we have developed to address the general use case in Swarm Robotic Simulators and Robot instances. Each of these modules was identified and modeled according to real-life aspects and experiment considerations.
 The architecture of the “pera-swarm” library which was mentioned before is shown in the Figure 3.5. We developed this library using the design patterns described below.
 Facade Pattern: To manage the library architecture and attach each subsystem
 and module
 Abstract Factory Pattern: To create instances from each module
 Singleton Pattern: To make sure only one instance is available during the run time for certain modules (e.g.: ObstacleController instance)
 State Pattern: To provide the functionality of finite state machines in certain modules (e.g.: Robots)
 The library implementation followed the module abstraction to maintain coherence in each high-level module while providing the functionality to extend for further implementations in some of them. For example, one of them is the Robot module and it can be either import and use as it is or the underlying methods of the Robot module can be overridden by the developers or researchers easily. The documentation for each library module is available on the Pera-Swarm documentation website, listed in (https://pera-swarm.ce.pdn.ac.lk/docs/). Following is a brief description of a few important interfaces and abstract classes as shown in the Figure 3.5.
 Abstract Controller Class: An abstract class consists of the functions “publish()” for publishing certain messages, “defaultSubscriptions()” for handling MQTT Routes according to route definitions.
 Abstract Emulator Class: The Abstract Emulator class also contains the functions described in the above Abstract Controller class. Besides, it associates further, the Robots class in agent emulators whereas it does not include the said class in other child classes. This approach was chosen because we wanted to realize a real-life swarm behavioral model in the simulator.
 Abstract Environment Class: This class contains an association relation of the “EnvironmentConfig” and the “AbstractObstacleController” classes with “updated”: timestamp as well as their corresponding abstract getter functions. In addition to that, three more abstract functions namely; “readConfig()”, “updateConfig()” and “createObstacles()” to realize more control over the ease of arena configuration and management.
 Abstract Obstacle Builder and Controller Interfaces: The Abstract Obstacle Builder interface contains only the functions including “createWall()”, “createBox()” and “createCylinder()” for instantiating those obstacles as well as an additional “changeMaterial()” method to change their materials. On the other hand, Obstacle Controller Interface contains a list of Obstacles and following methods:
 createObstaclesJSON(): generates the JSON config data for the arena configuration.
 setMaterialById(): sets material by obstacle id.
 setColorById(): sets color by obstacle id.
 findObstacleById(): finds an obstacle by given id.
 findObstaclesByType(): finds obstacles by the obstacle type.
 removeObstacleById(): removes an obstacle by given id.
 visualizeObstacles(): returns a list of obstacles that are in the three.js supported format for visualizing.
 Abstract Obstacle Class: An abstract class for representing the required attributes for the general obstacle entities as well as their getter and setter functions. This class includes the properties: id, type, position, color, geometryType, materialType, debug, created, updated, reality as we identified these are general attributes for all obstacles in the simulated environment.
 In the inherited obstacle entities (Wall, Box, etc) there is a method named “geometric()”, which returns the geometric properties of the obstacle and a method named “visualize()” which returns how the obstacle should be rendered in the mixed reality visualizer, based on a predefined schema. Schema allows users to define a list of primitive objects with their own geometric properties, material properties and positioning options.
 Figure 3.5 “pera-swarm” - Library Architecture
 The above described abstract classes and interfaces were identified according to their real-world usability and flexibility for supporting wider use cases. In this way, a clear separation of these creation and manipulation methods is obtained that is required for future implementations for the developers and researchers to customize these individual interfaces according to their application purposes.
 In addition to the “pera-swarm” library, we have also developed an MQTT router implementation named “@pera-swarm/mqtt-router” (https://github.com/Pera-Swarm/mqtt-router). The communication between the swarm server and each virtual robot instance are to be handled via this library as it consists of a message queue implementation and flexible routing functionality to handle each relevant message endpoint in the protocol stack.
 Figure 3.6 “@pera-swarm/mqtt-router” Block Diagram
 As shown in the Figure 3.6, we have implemented three high-level modules namely, MQTT Router, Message Queue and Route with Wrapper to function an efficient MQTT router for handling communication within the dependent applications. The wrapper will add certain higher-level attributes to each route depending on the specific functionality in order to complete the relevant subscriber event for the selected MQTT topics. The Message Queue simply implements an efficient queue processing using the npm library “queue” (https://github.com/jessetane/queue) with the dispatcher function as the route subscriber event handler method as specified by the routes list. In the MQTT Router module, there are two services namely MQTT Client Service and Discovery Service and they provide low-level communication handling and a route discovery realization with a simple locking mechanism for a specific MQTT channel/route.
 Both of these libraries were developed using Typescript language and compiled into “ES5” standard ( https://en.wikipedia.org/wiki/ECMAScript) “CommonJS” module ecosystem for JavaScript and were published into npm directory with the library names. The final experiments were carried out on the following versions.
 “pera-swarm”: 1.2.3
 “@pera-swarm/mqtt-router”: 1.2.1
 We developed the above-described libraries for the Open Source Community with the conclusion that researchers and enthusiasts can quickly get started on developing a customizable mixed-reality swarm environment platform according to their specific requirements without the cost of overhead for implementing from the ground up to most general use cases.
 Communication
 Communication between Simulator Components
 Communication is an important part of the swarm simulation. Since we followed a distributed architecture, communication between each sub-component is very important and it should be in real-time. Also, the communication delay should be minimized. The distributed system contains various components with various resource allocation, including web servers, local servers as well as micro-controllers. Hence, the communication method should be able to run on all these sub-component.
 We chose MQTT (Message Queuing Telemetry Transport) as the primary way of communication. It is an OASIS standard messaging protocol for the internet of things, based on lightweight publish/subscribe messaging transport with support of the quality of service.
 The Figure 3.7 contains a few communication channels we implemented.
 Figure 3.7 MQTT Protocols on robot localization
 Communication between Swarm Agents
 Swarm communication is an important area of swarm behavioral research, considering inter-agent communication. Since there are both physical and virtual robots in our approach, communication between them should be modeled with the support of the swarm simulator server.
 Physical robots can have hardware support for the communication while virtual robots can have emulators for this purpose. However, with an aid of the simulator, we can easily define entire virtual communication methods, without depending on the expensive hardware modules, but more similar to the real hardware functionalities. Therefore, we implemented two entire virtual communication modules; simple communication and directed communication.
 Figure 3.8 Robot to robot simple communication
 In “simple communication”, robots can broadcast messages to the robots nearby within a defined radius. In the “directed communication”, the robots can communicate only with the robots in front of them, also until defined distance range.
 When a robot transmits a message in “simple communication”, it will send that message to the ‘communication-out’ channel of the simulator, and the server is listening to this channel (Figure 3.8). Once the simulator receives a message into this ‘communication-out’ channel, it will consider the robot’s coordinates and determine the robots who are eligible to receive this message. The radius or the distance it considers is determined by the robot that originates the message or the communication protocol implementation. Then the simulator will send this message into the ‘communication-in’ channels of the selected robots and the robots will receive this as a communication interrupt message.
 Testing Toolkit - Sandbox
 The components which have been described above are distributed and decentralized. Also, we needed to validate and test these components individually and collectively at different stages. So, we implemented a “SandBox” application to monitor the performance and validate their functionalities.
 The Pera-Swarm SandBox (v2.0 - the current stable version) is a cross-platform progressive web application (Figure 3.9) developed over a time frame to overcome these problems. The basic functionalities of the application are testing each communication protocol and validating responses, creating and managing virtual robot units within the application, building up the virtual environment with virtual obstacles, managing authentication for Visualizer so that the Visualizer will only be accessible via the generated URL.
 Figure 3.9 SandBox Application
 The SandBox application has the following views and routes for the described configurations and monitoring to provide the high-level functionality to administrate swarm robotic experiments.
 Settings route: to configure the basic settings (host, MQTT channel, path) for the swarm environment after authenticating to the system.
 Robot Route: to send and receive messages relevant to robot configuration protocols.
 Communication Route: to test communication protocols namely, simple communication and directed communication as described in inter-robot communication Section.
 Distance Sensor Route: to send and receive messages relevant to robot and server distance sensor protocols.
 Color Sensor Route: to send and receive messages relevant to robot and server color sensor protocols.
 NeoPixel Route: to monitor and control NeoPixel/RGB LED strip of robots.
 Environment Route: to configure and preview environment configurations and create, update and export a config file.
 Log View: contains all the intercepted communication through the MQTT channel.
 After authenticating, the users can conduct their own experiments and configure obstacles and set up environments through the SandBox application. Also, the intercepted communication history can be viewed and cleared in a given time. The application is a responsive, progressive web application implemented using Framework7 (https://www.framework7.io/) which is a cross-platform framework for developing web applications with the additional support of native features. In the Floating Action Button, the authorized URL for tokenized Visualizer application for the configured experiment can be found along with the pera-swarm documentation and supported communication protocols documentation.
 Experiments and Analysis
 Experiments
 The main objective of the designed experiments was to validate the functionality of the simulator and the possibility of running swarm behavior algorithms on a mixed reality environment. For that two behavioral experiments were designed. The first experiment was to test the communication and the interactions between robots and the second experiment was to validate the mixed reality-based sensing.
 Color Ripple Experiment
 In this experiment, the communication between robots and the interaction between robots (virtual/real) and the simulator were tested.
 First, all the robots were placed in different locations in the simulation environment. Physical robots were placed on the arena with an image-based localization system. Then we assigned some coordinations to the virtual robots and deployed them into the same environment through the mixed reality simulator.
 Then chose a robot at random and sent the initial message to that robot via the SandBox application as in the following format
 [HopID][r] [G][b]
 HopID: the number that indicates how many robots passed the message using a virtual communication protocol. For the first message, this is 0.
 R, G, B: values of the Red, Green and Blue components of the color should be shown in the robot. The value must be between 0 and 255.
 Example: 0 255 0 0 (Robots will show the red color)\newline
 Then HopID of the robot was increased by one and re-transmitted the color values to nearby robots using its simple communication channel. Nearby robots also followed the same procedure and it made a color ripple-like behavior in the swarm of robots.
 The Figure 4.1 shows the results of one experiment done with 10 robots, which were placed in a circle. The starting message of “0 255 0 0” was given to robot number 2, and then it was indicated the Red color as shown in slide 2 of the figure. Then next two adjacent robots, robot number 3 and 1 were colored in red, as shown in slide 3, while robot 1 turned off its own red color. The same procedure was continued by other robots as shown in other slides.
 Figure 4.1 Results on Color Ripple Experiment
 Here, robots with IDs 0,1,2,6 and 7 were physical robots while the rest are virtual ones. In these experiments, the message initially transmitted by robot number 2 will propagate in two directions, clockwise and counterclockwise. When observing the message propagation, it shows messages propagate through the chain differently, by taking different amounts of times between each hop. We can assume it is because of propagation delays, querying delays and other delays in MQTT packet transmission.
 Discover an Object Experiment
 This experiment was specially designed to test the functionality of the distance and color sensors, the functionality of the localization system, and the interactive behaviors of robots in both realities.
 In this experiment, all ten robots were assigned into fixed coordinates and asked to discover a red-colored cylinder available in the simulation arena. We located one red color cylinder physically and one virtual red color cylinder placed in the arena on two fixed coordinates, as shown in the Figure 4.2. During this experiment, the robots moved in random directions and once they discover an obstacle (detect from the front distance sensor), it measured the color of the obstacle (using an RGB color sensor). If the color of the obstacle is equal to the color we assigned to discover, it informs the other robots that the discovery is completed, and other robots stop their movements and indicate the red color on their LED rings.
 Initially, this experiment was started with five physical robots and five virtual robots. Then for each next trial, one physical robot was removed and replaced by a virtual robot. The experiment was continued until all the robots in the experiment became virtual robots, and the behaviors of the robots were recorded for analytical purposes.
 Figure 4.2 Mixed reality and physical setups of the experiment
 Although these robots started from the same place, they followed random movements. Therefore, the time taken to complete this task for each experiment varied.
 During this experiment, we observed that sometimes physical robots moved through virtual obstacles and virtual robots moved through physical obstacles. We hypothesized that this was due to a transmission delay between the robots and the simulator.
 Most of the research works we observed, did not involve an experiment of swarm intelligence with both realities at once. However, we were able to run a few experiments with the robots in both realities side by side.
 As we mentioned earlier we did not consider the successful delivery of the MQTT packets. Therefore we experienced some robots missing the message that we were sending and had to re-transmit the message.
 Conclusions and Future Works
 Swarm Robotic experiments and applications are relatively expensive compared to other types of robotic representations including multi-robot environments which usually comprehend the physical environment. This study has introduced a method to reduce the cost as well as to avoid difficulties that occur in high-risk tasks using virtual robots and high-level implementation of a swarm robotic simulator platform with a collection of extensible libraries. To test the workability of the system and validate the integration of the whole architecture, a few physical robots have been built over the given period.
 Combining both virtual and physical robots, the mixed reality swarm robotics platform has been successfully validated with the ability for the robots (regardless of the reality) to move and navigate to a particular point and an experiment consisting of an exploration-based algorithm has been carried out. Finally, a web-based simulator has been implemented to visualize the movements and monitor the simulator’s behavior. This work has proven that the traditional limitations of swarm robotics could be further realized with the help of virtual reality integration including virtual robot units and a simulator to overcome the difficulties and to provide additional performances in more comprehensive and extended environmental configurations such as virtual reality and augmented reality.
 Introducing virtual robot units or instances to the swarm robotic environment for the conducted experiment was carried out using our local computer hardware configurations despite the fact that more scalability and performance could be achieved in a cloud computing environment to address the performance overhead. Our experiment was conducted on a smaller scale (5 to 10 robots) swarm including both physical and virtual robot units, because our specific experiment was sufficient of such a scale.
 Moreover, the work could be further expanded to integrate augmented reality environments that are helpful in such experiments to deliver promising results. The augmented reality functionalities are limitless given the fact that using our simulator platform, libraries could integrate much easily with such experiments. However, these experiments could also be expanded to more complex real-world applications such as worker robots for automated industrial environments, a swarm of robots to explore and analyze unreachable environments, etc.
 Links and References
 Website: pera-swarm.ce.pdn.ac.lk
 Pera-swarm - GitHub Organization
 Project Repository
 Project Page
 Department of Computer Engineering
 University of Peradeniya
 Back to top",The main objective of this project is to implement a mixed reality-based simulator application with an efficient and reliable architecture for allowing both physical and virtual robots to collaboratively run swarm intelligence based algorithms in real-time,E15,Undergraduate Research Projects,https://projects.ce.pdn.ac.lk/4yp/e15/Mixed-Reality-based-Simulation-Platform-for-Swarm-Robotics/,https://github.com/cepdnaclk/e15-4yp-Mixed-Reality-based-Simulation-Platform-for-Swarm-Robotics,https://cepdnaclk.github.io/e15-4yp-Mixed-Reality-based-Simulation-Platform-for-Swarm-Robotics,https://cepdnaclk.github.io/e15-4yp-Mixed-Reality-based-Simulation-Platform-for-Swarm-Robotics/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/4yp/E15/Mixed-Reality-based-Simulation-Platform-for-Swarm-Robotics/
88,Optimizing Mitochondria Genome Assembly And Annotation With Skim Sequencing Data,"Optimizing the procedure of mitochondrial genome assembly and annotation
 Projects - Department of Computer Engineering
 University of Peradeniya, Sri Lanka
 Project Title
 Team
 E/15/330, K.Sathurshan
 E/15/366, S.Thinesh
 E/15/373, R.Vaheesan
 Supervisors
 Dr. Asitha Bandaranayake, email
 Prof. Pradeepa Bandaranayake, email
 Table of content
 Abstract
 Related works
 Methodology
 Experiment Setup and Implementation
 Results and Analysis-
 Conclusion
 Links
 Abstract
 Genome sequencing and genome assembly are the computational process of converting the sequence composition of the gene within the cell of an organism in a human readable form. Mitochondria is an important genome in the cell and there is a need to study this genome for various reasons.
 The process of determining the order of bases A,G,T,C in the genome is known as genome sequencing.While sequencing the genome the original genome is separated into huge number of small parts known as reads and the end results of sequencing is a huge pool of reads(strings of A,G,T,C).These reads must be assembled back in a computer so that a biologist can identify and annotate the functionality of it.
 There are several techniques were developed to sequence the genome,
 the modern approach is next generation sequencing.
 Ilumina sequencing is one of the broadly used next generation sequencing method and this method produce large number of high precision sequencing
 short reads whereas other older methods produce longer reads.So the computational complexity of assembling back this large amount of read is high but cost efficient.
 Here we mainly discuss on low coverage sequencing data assembly.The sequencing data consist of multiple copies of same genome in low coverage data the number of copies are relatively lower than high coverage data.
 In this project we examined the tools used for mitochondrial genome assembly by assembling differnt datasets and measured the parameters that make impact in the assembly process. From the results we obtained from the experiment we made decisions of doing mitochondrial genome assembly.
 Related works
 There are several assembly pipelines suggested for assembling genome, below are the narrow downed pipelines/ tools used for mitochondrial genome assembly. There are two common approaches for assembly genome one is De novo assembly another one is Reference based alignment.Some tools use one of these approaches and some use hybrid approach consisting both.
 Here let’s discuss the tools that are used specifically for mitochondrial genome assembly
 NovoPlasty
 In NOVOPlasty there is a way to assemble genome from skim sequenced data, we can either assemble a whole organelle genome or isolate mitochondrial genome first and assemble it later. The best strategy for assembling depends on available data-set,computational power and reference genome availability. This tool uses a new algorithm to do the de novo assembly which is known as seed-extension, by this method NOVOPlasty claims that it assemble genome as accurate as other reference based assemblers such as MITObim,MIRA,ARC,SOAPdenova2.
 NOVOPlasty decreases computational complexity by storing the reads in a hash table and seed-extend it. As this tool claims it took 11 minutes duration and 15 GB memory to assemble 99.98% of G.intermedia mitochondrion genome with 100 % accuracy where MITObim took 4777 minutes,63.4 GB memory to assemble 99.95% of genome with 99.93% accuracy and SOAPdenovo2 took
 19 minutes,27 GB memory to assemble 99.98% of genome with 99.98% accuracy.
 Norgal
 Norgal is a tool used to extract mitochondria genome from WGS data and assemble it using de nova assembly. In previous methods like in MToolBox there is a need for reference mtDNA either from database or user input, but in Norgal it identifies the high frequency kmers (string fragmentations of genome) from WGS data to extract and assemble the mtDNA. Norgal is able to extract mtDNA by calculate the read depth of genome and assume the reads that have more read depth (frequency) than a specified threshold (nuclear read depth) are from mtDNA. Then it performs de nova assembly on those reads to assemble the genome.
 The whole pipeline of Norgal is
 Pre-processing reads -> Estimating nuclear read depth threshold -> Remove reads based on k-mer occurrences -> Assembly with high-frequency k-mers -> Annotation and validation
 Norgal has more advantages than its earlier predecessors like MToolBox,MITOBim and NOVOPlasty. Still it has some flaws in its pipelines, like the high run time, Norgal is slower than above mention tools also it consume more memory (the peak usage was 38-48GB where MITOBim needs only 1-13 GB) and it cannot be applied to organisms that have low copy numbers of samples cause Norgal needs a high depth coverage of reads as the current project we are working mainly focus on extract and assemble genome with low coverage reads which will be an advancement to this tool. Overall as they claim Norgal is the current best when there are no reference genomes available.
 SMART: Statistical Mitogenome Assembly with Repeats
 Statistical Mitogenome Assembly with Repeats (SMART) is yet another proposed pipeline for automated assembly of complete circular mitochondrial genomes from WGS data. Reference based assembly tools are faster but they can’t be used to assemble completely new mtDNA, Norgal can assemble the completely new genome by using de nova assembly but it requires high coverage reads and cause high runtime and memory. In Norgal and tools like it they attempt to remove reads from the nuclear genome by performing an assembly of the full set of reads and then using the read coverage of the longest contigs to estimate the coverage of the nuclear genome but in SMART it estimates the mean and standard deviation of mtDNA k-mers in WGS reads based on a seed sequence (This is not widely used approach of sequencing), then select the reads which has k-mer counts falling within three standard deviations of the estimated mean.
 SMART claims that it provides better results than other tools like
 Norgal, NOVOPlasty, PlasmidSPAdes and MToolBox but the run time and memory usage of this pipeline is not yet discussed.
 Methodology
 Design Overview
 This chapter covers how the work flow of the mitochondrial genome assembly optimization is planned. From the literature review we could find
 the tools that are dedicated for mitochondrial genome assembly. We then analyze the tools by reading and short listed some of them for experimental analysis.
 Genome assembly remains a challenging computational problem, the next-generation sequencing technologies which generate a greater amount of data and
 make the assembly process more complex. There is a huge demand for a pipeline that will assemble a genome quickly and accurately.
 In this next-generation sequencing, the results are short reads, so we mainly focused on the tools that do short read assembly of mitochondria genome.
 Conceptual Design
 Experiment Setup and Implementation
 Prototype
 We design an experimental setup to compare the tools that are specifically build to do mitochondrial assembly. In our experiment we first shortlist 4 tools for analysis they are: NovoPlasty: seed and extend algorithm & De novo assembly, Norgal : De novo assembly(No need for seeds),SMART : seed-and-extend algorithm & De novo assembly and Mitobim : Reference based assembly.
 Data Collection
 We choose the seed and reference genome of the datasets by analyzing their phylogeny. Available closest relatives of the species are used as seeds and reference genome. Here we used Cinnamomum Verum which is 1.5GB, Solanum Melongena which is 5GB, Homosapien Sapien which is 12MB and Oncorhynchus mykiss which is 12GB. Here we used Cinnamon Verum, Arabidopsis Thaliana, Oryza sativa, Solanum Melongena and output from Norgal assembly as seeds for NovoPlasty while assembling Cinnamomum Verum and Solanum Melongena. We used Arabidopsis Thaliana as a seed for SMART while assembling Cinnamomum Verum. We used Pan troglodytes seed for NovaPlasty while assembling Homosapien Sapien. For the NovaPlasty assembly of Oncorhynchus mykiss, we used T.thymallus COI sequence as seed. For mitobim we didn’t use seeds but used references genome of close species so for Cinnamomum Verum only available closest species is Arabidopsis Thaliana mitochondrial genome, Solanum Melongena (egg plant) we used Solanum macrocarpon(African eggplant) mitochondrial genome, for Homosapien Sapien (human) we used Pan troglodytes(Chimpanzee) mitochondrial genome and for Oncorhynchus mykiss we used T.thymallus mitochondrial genome.
 Tools
 In our experiment we first shortlist 4 tools for analysis they are, NovoPlasty uses seed and extend algorithm where a seed input is given along the dataset so the assembly can begin with that seed. The de novo assembly starts from these seed contigs. Norgal uses De novo assembly so there are no need for seeds. SMART uses seed and extend algorithm and follows De novo assembly. Mitobim is a reference based assembly tool. These 4 tools have their own uniqueness in assembling mitochondrial genome.
 Server Details
 University of Peradeniya, Aiken Server(RAM - 252GB Number of CPU cores - 32 cores) and AgBC Server (RAM - 262GB Number of CPU cores - 48 cores)
 Configure MitoBim Tool
 We used docker image for MitoBim tool. MITObim image contains a stripped down version of Ubuntu 16.04 and all necessary executables and dependencies to run the latest version of MITObim. Here we show how to recover the complete mitochondrial genome of Thymallus thymallus using the mitochondrial genome of Salvelinus alpinus as a starting reference. We used AgBc server for run mitobim assembly.
 Step 1 - specified a working directory on the machine that will be synced with the /home/data directory in the image and enter the self contained shell environment to run MITObim
 WORKING_DIR=/your/desired/working/dir
 sudo docker run -i -t -v $WORKING_DIR/:/home/data
 chrishah/mitobim /bin/bash
 Step 2 - Test the wrapper script by doing
 ~/PATH/TO/MITObim.pl
 Step 3 - Do the mapping assembly with MIRA 4. MIRA is a Sequence assembler and sequence mapping for whole genome shotgun and EST / RNASeq sequencing data.
 ln -s /PATH/TO/testdata1/Tthymallus-150bp-300sd50-interleaved.fastq
 reads.fastq
 ln -s /PATH/TO/testdata1/Salpinus-mt-genome-NC_000861.fasta reference.fa
 Step 4 - Create the manifest file and specifying the parameters for the MIRA assembly
 echo -e ""\n#manifest file for basic mapping assembly with
 illumina data using
 MIRA 4\n\nproject =
 initial-mapping
 -testpool-to-Salpinus-mt\n\njob=genome,mapping,accurate\n\
 nparameters = -NW:mrnl=0 -AS:nop=1 SOLEXA_SETTINGS -CO:
 msr=no\n\nreadgroup\nis_reference\ndata = reference.fa\nstrain =
 Salpinus-mt-genome\n\nreadgroup = reads\ndata = reads.fastq\
 ntechnology = solexa\nstrain = testpool\n"" > manifest.conf
 Step 5 - run MIRA 4
 mira manifest.conf
 Step 6 - Baiting and iterative mapping using the MITObim.pl script
 /PATH/TO/MITObim.pl -start 1 -end 10 -sample testpool -ref
 Salpinus_mt_genome -readpool reads.fastq -maf
 initial-mapping-testpool-to-Salpinus-mt_assembly
 /initial-mapping-testpool-to-Salpinus-mt_d_results
 /initial-mapping-testpool-to-Salpinus-mt_out.maf &> log
 Step 7 - After the process has finished looking into the log file
 tail log
 Configure Norgal Tool
 Norgal uses kmer frequencies to try to assemble the mitochondrial genome from NGS sequencing reads (currently only Illumina paired end reads are supported). It requires Python2.7+ or Python3, Java and matplotlib. The size of our input data determines how much memory we’ll use. Norgal has been tested on computers with 16GB, 32GB, and 64GB of RAM. In other words, if our reads are just a few GB each, it should work; but, if our reads are 12 GB each, and our machine only has 12 GB of RAM, it will almost certainly not work, and we’ll have to run it on a node or anything similar. But in our case, we used Aiken and Agbc server both are 256GB RAM. So we did not address any issue during the assembly.
 Step 1 - Download the program
 git clone https://github.com/kosaidtu/norgal.git
 Step 2 - Execute the norgal.py script
 python norgal/norgal.py -h
 Step 3 - Run the paired end data(f.fq and r.fq)
 python norgal.py -i f.fq r.fq -o norgal_output --blast
 Configure NOVOPlasty Tool
 NOVOPlasty is a de novo assembler and heteroplasmy/variance caller for short circular genomes. First, we have to find a suitable seed and the seed file should be formatted in the same way as a regular fasta file. We need to concern the seed sequences that are identical in mitochondrial and chloroplast genomes should be avoided. After that we have to create a configuration file, here we have to specify the path of the dataset, k-mer number, Reference sequence path and Seed inout path. Then finally we have to run the NOVOPlasty with configuration file.
 perl NOVOPlasty4.3.pl -c config.txt
 Configure SMART Web Interface
 The Figure below shows the SMART Web Interface here we have to input the paired end read data and seed file then it will output the assembly and annotation details.
 Results and Analysis
 Assembly Results
 As first part of the experiment a comparative analysis is done using NovoPlasty,NORGAL, MitoBim and SMART.
 Assembly Results of Cinnamomum Verum
 For the Cinnamomum Verum dataset we used 4 different seeds for assembling in NovaPlasty tool. When we used Cinnamon Verum(A small part of the dataset to be assembled) as the seed there were no N50 contigs present in the output file and it took less than 5 minutes run time. This assembly used 6.02 4GB ram and only one CPU core and one thread ran by the tool. When we used Arabidopsis Thaliana as the seed there were 3 contigs and 1 N5O contig present in the output file. The assembling process took 19 minutes to finish. This process used 6.024 GB ram and only one CPU core and one thread ran by the tool.When we used Oryza sativa as the seed there were 1 contig of length 5428 is present in the output file. The assembling process took 3 minutes to finish. This process used 6.024 GB ram and only one CPU core and one thread ran by the tool.When we used the norgal output as the seed there were 3 contigs and 1 N5O contig present in the output file. The assembling process took 21 minutes to finish. This process used 1 GB ram and only one CPU core and one thread ran by the tool.
 When we assembled Cinnamomum Verum using Norgal tool, there were no N50 contig present on the output file. It tooks 376 minutes to finish and it used 1GB ram. It took 2CPU cores and ran 3 threads.
 When we assembled Cinnamomum Verum using SMART, we used Arabidopsis Thaliana as the seed and no N50 contigs present in the output file. It approximately took 60 minutes to finish. After we submitted the data to the web server to assemble and annotate it choose 16 threads to run.
 When we asembled Cinnamomum Verum using MitoBim tool, there was only one N50 contig present in the output. It took approximately 45 minutes and used 2GB of RAM. It took 2CPU core and used 4 threads.
 Assembly Results of Solanum Melongena
 For the Solanum Melongena dataset we used 4 different seeds for assembling in NovaPlasty tool. When we used Solanum Melongena (A small part of the dataset to be assembled) as the seed there were no N50 contigs present in the output file and it took 15 minutes run time. This assembly used 20 4GB ram and only one CPU core and one thread ran by the tool. When we used Arabidopsis Thaliana as the seed there were no contigs in the output but it had hits while the annotation process. The assembling process took 31 minutes to finish. This process used 12 GB ram and only one CPU core and one thread ran by the tool.When we used Oryza sativa as the seed there were 1 contig with no hits present in the output file. The assembling process took 18 minutes to finish. This process used 17.5 GB ram and only one CPU core and one thread ran by the tool. When we used the norgal output as the seed there were 3 contigs and 1 N5O contig present in the output file. The assembling process took 42 minutes to finish. This process used 12.5 GB ram and only one CPU core and on thread ran by the tool.
 When we assembled Solanum Melongena using Norgal tool, there were intermediate contigs present on the output file. It tooks 780 minutes to finish and it used 10GB ram. It took 2CPU cores and ran 3 threads.
 When we assembled Solanum Melongena using SMART, one N50 contig present in the output file. It approximately took 90 minutes to finish. After we submitted the data to the web server to assemble and annotate it choose 16 threads to run.
 When we asembled Solanum Melongena using MitoBim tool, there was only one N50 contig present in the output. It took approximately 68 minutes and used 2GB of RAM. It took 2CPU core and used 4 threads.
 Assembly Results of Homosapien Sapien
 For the Homosapien Sapien dataset we used Pan troglodytes seed for assembling in NovaPlasty tool. We got Complete Circular genome in the output and it took 6 minutes to finish. It used 2GB ram and 1 core CPU and ran 1 thread.
 When we assembled Homosapien Sapien using Norgal tool, we got Complete Circular genome in the output and it took 50 minutes to finish. It used 1GB ram and 2 core CPUs and ran 3 threads.
 When we assembled Homosapien Sapien using SMART, we got Complete Circular genome in the output and it took 30 minutes to finish. After we submitted the data to the web server to assemble and annotate it choose 16 threads to run.
 When we asembled Homosapien Sapien using MitoBim tool, we got Complete Circular genome in the output and it took 8 minutes to finish. It used 2GB ram and 2 core CPUs and ran 4 threads.
 Assembly Results of Oncorhynchus mykiss
 For the Oncorhynchus mykiss dataset we used T.thymallus COI sequence as seed for assembling in NovaPlasty tool. We got 1N50 contig in the output and it took 70 minutes to finish. It used 2.772GB ram and 1 core CPU and ran 1 thread.
 When we assembled Oncorhynchus mykiss using Norgal tool, there were no N50 contigs in the output and it took 1080 minutes to finish. It used 2.016GB ram and 2 core CPUs and ran 3 threads.
 When we assembled Oncorhynchus mykiss using SMART, there were no N50 contigs in the output and it took 90 minutes to finish. After we submitted the data to the web server to assemble and annotate it choose 16 threads to run.
 When we asembled Oncorhynchus mykiss using MitoBim tool, we got Complete Circular genome in the output and it took 80 minutes to finish. It used 2GB ram and 2 core CPUs and ran 4 threads.
 Conclusion
 Assembling a mitochondrial genome very accurately and efficiently for all kind of datasets is what everyone involved with genomics are looking forwards and this study also going in that way. We have assembled 4 type of dataset with four tools and analyzed it’s performance and accuracy, from the results we obtained we can come to some conclusions on assembling mitochondrial genome. The conclusions we derived from the results can be divided into 3 scenarios which are Dataset based, Runtime based and Resource Requirement. From the 1st scenario we can conclude that all a good assembly needed is a good dataset, in the sense of dataset it means the seeds,reference genome and the size of the dataset. If the size is small the accuracy is much better.If the seeds are good enough we can get good acccurate and efficient assembly from NovoPlasty and SMART tools.But finding such seeds is a crucial task. The second scenario is the run time of the assembly , if there are neither reference genome nor seeds we can still assemble the dataset using Norgal but it consumes more time. As from these two scenarios a pipeline can be developed as shown in the below figure that in case if there are no good seeds we can use the Norgal assembly output of the same species as seed and it will give better assembly.
 Links
 Project Repository
 Project Page
 Department of Computer Engineering
 University of Peradeniya
 Back to top","This assembly is to find an efficient procedure to identify the optimal set of mitochondria genome data from skim sequencing data and assemble, annotate those data to form a complete mitochondria genome.",E15,Undergraduate Research Projects,https://projects.ce.pdn.ac.lk/4yp/e15/Optimizing-Mitochondria-Genome-Assembly-And-Annotation-With-Skim-Sequencing-Data/,https://github.com/cepdnaclk/e15-4yp-Optimizing-Mitochondria-Genome-Assembly-And-Annotation-With-Skim-Sequencing-Data,https://cepdnaclk.github.io/e15-4yp-Optimizing-Mitochondria-Genome-Assembly-And-Annotation-With-Skim-Sequencing-Data,https://cepdnaclk.github.io/e15-4yp-Optimizing-Mitochondria-Genome-Assembly-And-Annotation-With-Skim-Sequencing-Data/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/4yp/E15/Optimizing-Mitochondria-Genome-Assembly-And-Annotation-With-Skim-Sequencing-Data/
89,Optimizing chloroplast genome assembly and annotation with skim sequencing data,"Optimizing chloroplast genome assembly and annotation with skim sequencing data
 Projects - Department of Computer Engineering
 University of Peradeniya, Sri Lanka
 Optimizing chloroplast genome assembly and annotation with skim sequencing data
 Team
 E/15/209, H. Kithma Madhushani, email
 E/15/233, Nipuni Muthucumarana, email
 E/15/325, Chalani Weerarathna, email
 Supervisors
 Dr. Asitha Bandaranayake, email
 Prof. Pradeepa Bandaranayake, email
 Table of content
 Abstract
 Related works
 Methodology
 Experiment Setup and Implementation
 Results and Analysis
 Conclusion
 Links
 Abstract
 Chloroplast genes and genomes play an important role in plant phylogeny and speciesidentification. Skim sequencing is getting low coverage genome sequencing data that has,nuclear, choloroplast and mitochondria genome sequences. Since the fast developmentof high throughput sequencing technologies, it’s low cost to urge the low coverage dataof the whole genome (usually concerning 20-30GB data), that is enough to assemblea whole chloroplast genome. To date, there are several assembly processes/pipelinesdesigned to assemble a whole chloroplast genome. However, what proportion knowledgeis required or really utilized in such analysis is a problem. Having such information canfacilitate biologists to style their experiments properly and cost-effectively. Biologistsexpect a straightforward, quick and easy procedure to assemble and annotate a circularchloroplast genome from Illumina NGS data.In this project, we’ll analysis the present procedures for chloroplast genome assemblyand annotation, and work on developing the strategies to spot and choose the best set(s)of data and the procedure(s) to assemble a given chloroplast genome as accurately andefficiently, by statistical, computational and heuristical strategies.
 Related works
 The process of chloroplast genomes assembly has different strategies. Consider WholeGenome Sequencing (WGS) data and there are two main steps required for the process.First one is the extraction of chloroplast reads from the sequencing data and the secondstep is resolution and assembly of the special circular structure including the IRs. Thefirst step can be achieved by mapping the reads to a chloroplast reference. To do thisprocess, assemblers can use k-mers. that are highly repeated in the chloroplast reads.Another method is to use the highly represented reads from the data set without usingreference-based approach. Apart from those two methods. There is another methodcalled NOVOPlasty, which combines the above two approaches by using a chloroplastreference as seed and then trying a simultaneous assemble of reads based on list of k-mers.
 Genome assembly process is called as a sequence assembly. There are two ways to do asequence assembly.
 De Novo assembly
 Reference mapping assembly
 ‘De Novo’ means start from the beginning. As the name of it, ’De Novo’ assemblersare the type of program that assembling a large of short DNA sequence and create fullsequences of the original chromosomes from which the DNA originated without the useof a reference genome. As an example in ’De novo assemblers’ does not use any priorknowledge of the source DNA sequence length, layout, or composition.There are two common types of ’De Novo assemblers’ named as ’greedy algorithmassemblers’ which aim for local optima in alignments of smaller reads and ’graph methodassemblers’ which aim for global optima. String graph and De Bruijn graph [6] are twocommon graph methods. There is a common protocol for the De Novo assembly.
 There are several bioinformatic tools available for the De Novo genome assemblyprotocol [7]. Those tools can be used in three areas.
 Tools for reading and quality control
 FastQC
 NGS QC
 Trimmomatic
 Tools for assembly
 NOVOPlasty
 Velvet Optimizer
 Faster Statics
 Spades
 Soap-denovo
 Tools for determining the suitability of a draft set of contigs
 QUAST
 Manuve assembly metric
 CLC BioWorkbench
 On the other hand ‘reference mapping assemblers’ are a type of program that assem-bling reads against and existing backbone sequence. It builds a sequence that is similarbut not necessarily identical to the backbone sequence. In this assembly method, partswith multiple or no matches are usually left for another assembling technique to lookinto.Instead of using De Novo assembler and ‘Reference mapping assembler’ separately,the combination of these two methods provides an effective and powerful tool to improvegenome assembly by integrating information of a related genome. This method is calledReference-guided de novo assembly approach.
 Main Assembly Softwares
 1.GetOrganelle
 2.Fast-plast
 3.NovoPlasty
 Some factors were taken into consideration when testing the tools and those factors areassembly time, memory and CPU utilization. Time requirement for the assembly is agood measure as it shows huge differences from tool to tool. Variation in run time differsfrom few minutes to several hours. Input data and number of threads used also affectedfor the time requirement. In the experiment, tools are tested with a time limit of 48h.Some assemblies exceeded that time limit. According to the test results, IOGA andFast-Plast. followed by ORG.Asm and GetOrganelle took the longest time periodfor the assembly generation. ChloroExtractor can be considered as the most timeefficient tool and it was somewhat faster than NOVOPlasty and Chloroplast assemblyprotocol.Having access to multithreading is beneficial for the tools. Chloroplast assemblyprotocol, chloroExtractor, GetOrganelle and Fast-Plast methods profited from havingmultiple threads. But NOVOPlasty and ORG-Asm cannot be recognized independentlyby using multithreading because both of them required almost the same time to utilize 1,2, 4 or 8 threads.When considering the memory and CPU usage, for the same input data set and forthe same number of threads, disk usage and peak memory were recorded and also, meanand peak values of CPU usage recorded for each an every assembler. Although memoryusage patterns shown by ChloroExtractor and IOGA assemblers influenced a little bythe size of the input data, other assemblers’ peak memory usage influenced up to aconsiderable level. If the input date size is higher related to their memory and CPUusage, assemblers are profited from having higher number of threads. But disk usage ofall assemblers does not depend on either size of the input data or the number of threads.
 Methodology
 The main goal of this research is to find the most optimal chloroplast assembly tool toassemble whole genome sequencing data. As an example, we are going to assemble acinnamon data set using the best three tools which we identified from the research andthen the results from each tool will be compared. From the comparison we can identifythe weak points and strengths of each tool. Then we build a new pipeline includingall identified strengths, by combining different methods together. Using the new tool,we can assemble the same cinnamon data set and check whether the results are moreaccurate than the results obtained previously from each tool. We can repeat the sameprocess for different newly built pipelines with different strengths.
 Experiment Setup and Implementation
 We tested for several assembly tools for their run time, cpu usage and memory usage with different datasets considering their accuracy. Based on thee results
 given we develop a workflow for Optimizing chloroplast genome assembly and annotation with skim sequencing data.
 Results and Analysis
 We have attach our results and comparison in the root folder of the repository.
 Conclusion
 About nearly half of the analyzed WGS data without available chloroplast genome,complete assemblies can be generated using the assembly tools.There are many tools for genome assembly by the present. GetOrganelle , Fast-Plast , NOVOPlasty , ORG.Asm , IOGA and chloroExtractor are someof them. Scientsts tend to compare overall performance of the different chloroplastassemblers depending on the various assessment criteria.When we compared the general performance of the different chloroplast assemblingtools, we need to consider various criteria. The most straightforward assembler in general,both on recreated and genuine information, was accomplished by GetOrganelle. Fast-Plast performed almost likewise on most information. These two devices supplementeach other, together with the instrument can do successful assemblies of full chloroplastsin situations where the contrary instrument comes up short. GetOrgane is the onlytool that can generate assemblies for 15 different datasets . Fast-Plast can generateassemblies for only 3 datasets that vanquished every single other tool. NOVOPlasty was the sole another device that would produce a get together that wasn’t created withthe other constructing assembly tool. Fast- Plast, NOVOPlasty, and ORG.Asm delivered the most valuable outcomes, and along these lines, rerunning the device after afailed endeavor could be an authentic methodology. ChloroExtractor
 has producedvery few chloroplast assemblies, but requires few materials and is straightforward toinstall and use. Both IOGA and Chloroplast protocol protocols had unsatisfactoryperformance and did not return to reliable chloroplast conventions. These tools canreconstruct the chloroplast genome even without available reference genomes.
 Therefore, among the above-mentioned tools, GetOrganelle can be used as adefault option for chloroplast assemblies. Fast-Plast is the second option and the thirdoption can be NOVOPlasty.But all tools do not succeed in generating complete chloroplast assemblies andtherefore, we have to determine the strengths and weaknesses of the specific tools.Sometimes it may be necessary to combine different methods or manually explore theparameter space for generating complete chloroplast assemblies. But most of the time,reconstructing thousands of chloroplast genomes is feasible using the currently availabletools.When considering the annotation tools, most of the time, Dogma has been widelyused for gene prediction in chloroplasts. Until recently, it was the only tool specific tochloroplast genomes, that explains its success for the annotation of genomes. Now moreconsistent annotation of genes is produced with GeSeq when compared to the Dogmasuggesting that annotation of most of the previously annotated chloroplast genomesshould now be updated.
 Links
 Project Repository
 Project Page
 Department of Computer Engineering
 University of Peradeniya
 Back to top","Chloroplast genes and genomes are the most important genomic data for plant phylogeny and species identification. Skim sequencing is obtaining low coverage genome sequencing data that includes, nuclear, chloroplast and mitochondria genome sequences. Since the rapid development of high throughput sequencing technologies, it is cheap to get the low coverage data of whole genome (usually about 20-30GB data), which is enough to assemble a complete chloroplast genome. To date, there are many assembly processes/pipelines described to assemble a complete chloroplast genome. However, how much data is needed or actually used in such analysis is not clear. Having such information will help biologists to design their experiments properly and cost-effectively. Biologists expect a simple, fast and user-friendly procedure to assemble and annotate a circular chloroplast genome using Illumina NGS data.   In this project, we will research the existing procedures for chloroplast genome assembly and annotation, and work on developing the methods to identify and select the optimal set(s) of data and the procedure(s) to assemble a given chloroplast genome as accurately and efficiently as possible, by using computational, statistical & heuristical methods. ",E15,Undergraduate Research Projects,https://projects.ce.pdn.ac.lk/4yp/e15/Optimizing-chloroplast-genome-assembly-and-annotation-with-skim-sequencing-data/,https://github.com/cepdnaclk/e15-4yp-Optimizing-chloroplast-genome-assembly-and-annotation-with-skim-sequencing-data,https://cepdnaclk.github.io/e15-4yp-Optimizing-chloroplast-genome-assembly-and-annotation-with-skim-sequencing-data,https://cepdnaclk.github.io/e15-4yp-Optimizing-chloroplast-genome-assembly-and-annotation-with-skim-sequencing-data/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/4yp/E15/Optimizing-chloroplast-genome-assembly-and-annotation-with-skim-sequencing-data/
90,Pipeline for Isolation of Fast evolving ITS Regions from Skim Sequencing Data,"A User-friendly Pipeline for Isolation of Fast-evolving Internal Transcribed Spacer(ITS) Regions from Skim Sequencing Data
 Projects - Department of Computer Engineering
 University of Peradeniya, Sri Lanka
 A User-friendly Pipeline for Isolation of Fast-evolving Internal Transcribed Spacer(ITS) Regions from Skim Sequencing Data
 Team
 E/15/016, Anojan S., email
 E/15/171, Kapilrajh R., email
 E/15/351, Thakshajini S., email
 Supervisors
 Dr. Asitha Bandaranayake, email
 Prof. Pradeepa Bandaranayake, email
 Table of content
 Abstract
 Related works
 Methodology
 Experiment Setup and Implementation
 Results and Analysis
 Conclusion
 Links
 Abstract
 DNA that resides within the nucleus of a cell is the primary genetic material that is responsible for genetic behaviours of various eukaryotic organisms such as plants, humans, animals, algae and fungi as well as prokaryotic organisms such as archaea and bacteria. Genome or DNA of an organism has several complex genomic regions. Specific genomic regions consist of several complex proteins. RNA is the replica of DNA involved in protein synthesis using these complex proteins and instructions carried from DNA. Ribosomes are small particles having these RNA molecules. Ribosomal DNAs are repeating units having fast-evolving as well as conserved subregions. Isolation of fast-evolving regions of rDNA is necessary for a more accurate and successful analysis of variation within and between species. ITS1 and ITS2 are the fast-evolving regions of rDNA that are widely preferred to differentiate various species including humans, plants and fungi. These regions have the highest probability of correct identification and ITS is the most frequently used molecular marker in species variation analysis. There are several computational tools and pipelines available in the literature to isolate ITS regions from different sequence datasets and to analyze the inter-species as well as intra-species variation using these fast-evolving genomic regions. Biologists don’t have a clear understanding on which is the most efficient and suitable computational tool or pipeline to extract and analyze the fast-evolving nuclear ribosomal ITS regions associated with various organisms since they don’t have sufficient computer knowledge. Hence, they find it difficult to select the best tool or technology required for the isolation and analysis process. Therefore, the purpose of our project is to compare the pros and cons of existing computational tools and pipelines based on various parameters such as the CPU time taken to run the software, CPU performance, accuracy, required computer memory and disk space and depending on other specifications to come up with an efficient procedure or pipeline to extract and analyze fast-evolving genomic regions from low coverage skim sequence data at a low cost. Hence, the primary objective of this project is to come up with a simple,user-friendly and efficient workflow or pipeline for the biologists to isolate fast-evolving genomic regions from skim sequencing data for phylogenetic studies.
 Related works
 According to our literature review, previous several researches related to ITS regions, prove that ITS is a standard accepted metabarcode marker for several species including plants, fungi and human fungal pathogens. These researches also emphasize that ITS subregions ITS1 and ITS2 have the highest probability of correct identification within and between above mentioned species.
 2.1 Justifications on ITS Regions
 One of the researches shows that it is possible to use the ITS2 region of rDNA to distinguish five different Cinnamomum species.
 Another review paper indicates that the ITS1 region is very useful for reliable identification of medicinal plants and phylogenetic analysis of Gambhari.
 One of the other articles related to DNA barcoding asserts
 ITS1 and ITS2 regions as the standard metabarcoding markers to identify fungi species such as Basidiomycota.
 One of ther papers shows the use of Inter Simple Sequence Repeat (ISSR) markers together with nrDNA ITS sequences incorporated with leaf morphological characteristics to describe cladistic relationships between twelve different Cinnamomum species in Taiwan.
 In another research paper, highly repeated units of ITS showed distinguishable variation between individual Cerastoderma species in the phylogenetic analysis.
 Another group of researchers proved a more close phylogenetic relationship between polypoid Elymus plant species and other organisms using ITS sequences.
 One of the researches approved that utilizing ITS regions as molecular targets provided the higher potential for the characterization of human fungal pathogens.
 2.2 Justifications on Software Tools and Pipelines
 According to our literature review, there are various computational tools and pipelines used in several previous researches over the last one and a half-decade to extract and analyze ITS regions from different fungal sequencing datasets. Here, we have provided justifications based on those tools and pipelines.
 One of the researches shows that ITSx is a software tool to isolate ITS1, 5.8S and ITS2 and also full-length ITS sequences from both Sanger and NGS sequencing datasets.
 One of the researches of Professor H. Kauserud shows that extracting ITS1 sequences from 12 486 raw pyrosequencing ITS1 dataset using ITSx detected 12 410 ITS1 fungal sequences and he found the low quality or short length reads when examining the rest of the 76 sequences.
 Research related to ITSxpress shows that it is an improved software tool from ITSx that extends its capability from marker gene studies which use Operational Taxonomic Units (OTUs) to studies that use Exact Sequence Variants (ESVs).
 This research further justifies that using 4 cores, ITSxpress trimmed ITS1 region samples by a median of 23 times speeder than ITSx. ITSxpress trimmed the ITS2 region 14 times speeder
 than ITSx. Clustering at 99.5 percent identity minimized the number of reads used for Hmmsearch by a median of 71 times for ITS1 region and 36 times for ITS2 region.
 Some researchers used
 ITScan as an automated software pipeline to identify and analyze the variation of fungal species using ITS sequences.
 Another research related to PIPITS shows that it is also an automated pipeline to analyze fungal ITS sequences. It uses ITSx to isolate ITS subregions and utilizes the RDP classifier for the classification of sequences with the UNITE fungal sequencing dataset.
 One of the literature uses CloVR-ITS as a portable pipeline to utilize the analysis of fungal communities using ITS amplicon pyrosequencing data.
 A review paper related to Illumina Metabarcoding Pipeline describes it as a flexible software pipeline to extract and analyze ITS rDNA from Illumina Miseq sequencing data having paired-end reads.
 One of the other review papers on PlutoF justifies it as a web-based tool that includes the software tool to isolate and classify ITS sequences obtained from high-throughput sequencing datasets.
 Research associated with CLOTU indicates that it is a software pipeline which helps to speed up the process of analyzing fungal ITS sequences by providing high performance.
 One of the other review papers emphasizes that another Perl-based software pipeline is also available to automate the BLAST process and to extract ITS subregions from various ITS sequence datasets to speed up the analysis process.
 Considering the above literature review justifications, these researches have focused on the importance of using ITS regions as a molecular marker for accurate fungal species variation analysis and the articles related to the software tool and pipelines to extract and analyze ITS regions mostly focusing on Fungal ITS sequences. Hence, these papers could not utilize these tools and pipelines to isolate ITS regions from plant and human skim sequence data. We have not found a review paper comparing the pros and cons of all the tools and pipelines. Therefore, through our research, we are going to do a comparative analysis between these software tools and pipelines to identify the best tool or technology or pipeline. Then, we are going to find whether they are applicable for plant and human skim sequence data and how much data is enough for the analysis.
 Methodology
 First, we analyzed the software and hardware requirements of the software tools and pipelines that we identified from our literature review such as
 ITSx, ITSxpress and PIPITS about how much RAM and disk space is needed to install each tool and what kind of input data is needed for each tool. Then, we identified whether we need to input skim data or contigs or scaffolds for these tools. After that, we installed these tools ITSx, ITSxpress and PIPITS with all the other required tools in our department aiken server using the anaconda environment.
 Next, we have collected the data that separately contains forward and reverse raw reads of different Cinnamomum species such as Cinnamomum Capparu Coronde, Cinnamomum Verum and Cinnamomum Zeylanicum. Then, we tested these tools using the given cinnamomum capparu coronde data containing the forward raw reads around 19 GB and reverse raw reads around 19 GB. We recorded the output and the CPU time for each tool.We earlier got empty files as output for the tools earlier and it took a very long time to obtain the output in akien server.
 As a result, we got some ITS regions and we verified those output ITS regions by blasting agianst NCBI nr/nt database to make sure we exactly got the ITS regions of cinnamomum species. We did a comparative analysis of the tools based on the CPU time and the similarity of the ITS regions to identify the better tool which is much efficient and accurate. Further, we analyzed the steps of the existing pipeline PIPITS to come up with a similar pipeline by improving it.
 We used the tool seqtk to partition different sizes of the collected data such as 1GB, 2GB,3GB and 5GB for both forward and reverse raw reads in order to test our pipeline. Earlier, We ran our pipeline in our department aiken server for 1GB and 2GB data of cinnamomum capparu coronde and recorded the respective run times of each tool that performs the relevant step. Then, we shifted to agbc server later because the aiken server was responding too slow. As a result, we have experienced much improved performance for each tool of our pipeline in agbc server compared to aiken server. Hence, we tested the same 1GB and 2GB data in agbc server and recorded the improved run times with respect to each tools of the pipeline.
 In the first step of our pipeline, we have done the quality checking of the reads to find the GC content, no of low quality reads and other relevant characteristics of the reads.Then, we filtered out the low quality reads using fastqc. However, we couldn’t filter out both forward and reverse reads simultaneously using that tool. Therefore, we found another tool afterqc to filter out both forward and reverse reads at the same time and tested it successfully.
 After quality filtering, we ran ITSx by directly using the good quality forward and reverse raw reads as input to extract ITS regions. However, we failed in the process and we found that the read length 150bp is not sufficient to extract ITS regions using ITSx. Therefore, we needed to assemble the forward and reverse reads to get contigs in order to maximize the read length. Hence, we used the assembler Spades to obtain the contigs in fasta format. In the next step, we ran ITSx using the resultant contigs in fasta format for the aforementioned different sizes of data separately.
 Meanwhile, we also converted the obtained contigs from fasta format to fastq format using the tool seqtk since ITSxpress only accepts fastq format input files. Then, we used the resultant contigs in fastq format as input to ITSxpress to extract ITS regions. Here, we looked for more tools associated with fasta to fastq conversion and found the tool bbmap(reformat.sh) in addition to seqtk. Then, we compared the performance between seqtk and bbmap(reformat.sh) for 3GB and 5GB cinnamomum capparu coronde data and observed the difference in the recorded run times. Thereafter, we input the resultant fastq files obtained form both seqtk and bbmap(reformat.sh) to ITSxpress and compared the run times taken to get the output.
 Earlier, when we ran ITSx using contigs as input for 1GB data, we got empty output file. Then, we merged the forward and reverse reads using the tool vsearch and ran ITSx again using the obtained merged reads as input. As a result, we got some ITS sequences as output. Then, we have checked the quality of the output and blasted to ensure that we got the exact ITS regions of cinnamomum species. However, we identified multiple sequences in the output.
 Further, we found that some of the sequences in contigs which are greater than 100kbp in read length is the reason behind getting these multiple sequences in the output. Therefore, we filtered out those sequences that are greater than 100kbp from the contigs file using the tool bbmap(reformat.sh). After that, we ran ITSx using the filtered contigs to extract ITS regions and we ended up getting some ITS sequences as output for cinnamomum capparu coronde 1GB data. When we checked the quality of the output this time, we found that there were no multiple sequences.
 Earlier, we ran ITS using the default mumber of threads which is only one CPU thread and later we increased the number of CPU threads from one to sixteen to run the ITSx. As a result, we found much improvement in the performance of ITSx. After that, we tried with different thread sizes for 1GB cinnamomum capparu coronde data and observed the deviation in the performance of ITSX with respect the increasing or decreasing thread sizes.
 Meanwhile, we faced no problem when directly using contigs obtained from spades as input to run ITSxpress and we got an ITS sequence as the output from ITSxpress for the same data. Then, we checked the quality of the ITSxpress output and blasted it to verify that the obtained ITS sequence belongs to cinnamomum species. Next, we compared the ITSxpress output with the ITSx output by doing multiple alignment to find whether both of them are same.
 Further, we tested our pipeline for 1GB cinnamomum verum and cinnamomum zeylanicum data as well using the same process and compared all the results of both ITSx and ITSxpress for the three cinnamomum species based on the performance and quality of the output obtained by blasting the output sequences and doing multiple alignment to obtain the distance matrix.
 Results and Analysis
 We have obtained separate results from ITSx and ITSxpress for different sizes of data associated with different cinnamomum species such as cinnamomum capparu coronde and cinnamomum zeylanicum. Then, we blasted those outputs separately against NCBI/nr/nt database to find and verify whether that the obtained blasted results contain ITS regions of cinnamomum species. After that, we compared the ITS output sequence/sequences
 that contain ITS regions of cinnamomum species obtained from ITSx with the ITSxpress output sequence/sequences which also contain the ITS regions of cinnamomum species by doing multiple alignment using MAFFT algorithm to check whether both are exactly same sequences or not using the created distance matrix.
 The results are as follows:
 Comparison 1
 Comparison 2
 Comparison 3
 Comparison 4
 Conclusion
 We conclude that if the input for ITSx are the merged sequences of reverse and forward reads having the sequence length between 150bp-300bp and the actual ITS region is greater than 300bp, then the extracted ITS region is more likely to be a partial sequence. On the other hand, if the input sequences to the ITSxpress are contigs assembled from Spades, then, there is a high probability to have a complete ITS region.
 If the blast results of the ITSx output sequences for a given data size against the NCBI nr/nt database contain the ITS regions of a relevant species and the blast results of the ITSxpress output sequences for the same data size against the NCBI nr/nt database also contain the ITS regions of the same species, then when we compare both these ITSx and ITSxpress sequences using multiple alignment, we can conclude the both the ITSx and ITSxpress output are exactly same for a given data size if the created distance matrix shows no difference.
 In future, we suggest to test our pipeline for different datasets associated with different species such as wild rice and strobilathes. Overall, we believe that our work will be very helpful for the biologists to provide them a clear idea on the
 isolation of ITS regions as efficiently and as accurately as possible from skim sequencing data for their phylogenetic studies.
 Links
 Project Repository
 Project Page
 Department of Computer Engineering
 University of Peradeniya
 Back to top","Biologists have been interested in phylogenetic studies over the years. They have used several DNA based methods for phylogeny such as DNA extraction, DNA replication and DNA sequencing. Skim sequencing is the most preferred sequencing method since it results in low coverage data at a low cost.  Biologists have identified a fast-evolving region called Internal Transcribed Spacer (ITS) inside nuclear ribosomal DNA repeats of eukaryotic cells. They prefer to isolate this ITS region from skim sequencing data since it leads to a more accurate species variation analysis. However, because of the huge of data and unclarity on how much data is minimum needed, they find it much difficult to gather ITS regions as efficiently and as accurately as possible.  Therefore, the purpose of this project is come with a simple, efficient, accurate and user-friendly procedure to isolate ITS regions from skim sequencing data to provide a clear pathway for the biologists for phylogenetic studies to solve their problems.",E15,Undergraduate Research Projects,https://projects.ce.pdn.ac.lk/4yp/e15/Pipeline-for-Isolation-of-Fast-evolving-ITS-Regions-from-Skim-Sequencing-Data/,https://github.com/cepdnaclk/e15-4yp-Pipeline-for-Isolation-of-Fast-evolving-ITS-Regions-from-Skim-Sequencing-Data,https://cepdnaclk.github.io/e15-4yp-Pipeline-for-Isolation-of-Fast-evolving-ITS-Regions-from-Skim-Sequencing-Data,https://cepdnaclk.github.io/e15-4yp-Pipeline-for-Isolation-of-Fast-evolving-ITS-Regions-from-Skim-Sequencing-Data/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/4yp/E15/Pipeline-for-Isolation-of-Fast-evolving-ITS-Regions-from-Skim-Sequencing-Data/
91,Real Time Data processing and AI for Distributed IoT,"Real-Time Data processing and AI for Distributed IoT
 Projects - Department of Computer Engineering
 University of Peradeniya, Sri Lanka
 Real-Time Data processing and AI for Distributed IoT
 Team
 E/15/246,Rajitha Opanayaka, email
 E/15/385,Amila Weerasinghe, email
 E/15/363, Rashmi Thilakarathne, email
 Supervisors
 Dr. Upul Jayasinghe, email
 Dr. Damayanthi Herath, email
 Table of content
 Abstract
 Related works
 Methodology
 Experiment Setup and Implementation
 Results and Analysis
 Conclusion
 Publications
 Links
 Abstract
 Artificial Intelligence has impacted in a variety of
 industries, leading the world towards revolutionary applications
 and services that are primarily driven by high-performance
 computation and storage facilities in the cloud. This is mainly due
 to the advantage of having higher computational power, larger
 storage capacity and scalability. But with the increase of millions
 of IoT devices, a huge amount of data is being generated by
 end devices. To process such data, the distributed end devices
 have to communicate with the cloud servers making it difficult
 to generate real-time decisions though it consumes a lot of
 resources including bandwidth, processing and storage facilities
 at the cloud. On the other hand, Edge computing architectures
 enable a distributed way to process data near the sources of
 data which leads to facilitate real-time processing. But with the
 limited resources in the end devices, it is quite challenging to
 perform complex AI algorithms. Hence to facilitate such services
 and to enable real time processing at the edge,a novel approach
 is proposed base on computation distribution, vectorization,
 computation offloading, parallelization and federated learning
 techniques.
 Related works
 Use of Map Reduce to distribute the computation
 The distribution of computation can be categorised basically under three main scenarios:
 • When the training data is large.
 • When data to be classified is large.
 • When the Neural network consists of a huge number of nodes.
 When the training data set is larger training data has to be divided using mapper function
 is Map reduce. Because training a huge volume of data costs high computational
 resources as well as a high amount of time. So different sets of data will be provided to
 each node in the cluster and the whole portion of data to be classified will be provided to
 every node, because the volume of data to be classified is small compared to the volume of
 training data. When each node in the cluster is trained with different chunks of training
 data. That will generate different AI models at the different nodes of the cluster. So
 federated learning techniques should be used in order to define the suitable training
 model out of the different models that will be generated.
 When data to be classified is huge that particular data will be distributed to all the
 nodes using the mapper function. For each node in the cluster the same chunk of training
 data will be provided because the training data is small compared to data to be classified
 in this scenario. Each node will generate its output and the reducer function will make
 the final output from all of these results.
 When the neural network consists of a large number of neurons, computation cost
 increases. So using map reduce the neural network can be distributed over multiple
 nodes. There are a number of iterations involved with the feedforward process while
 the backpropagation is done in the last iteration. In each iteration reducer collects the
 outputs and merges all outputs from each mapper. For this approach computers with
 high computation power are used. In this work clusters consist of low computation power
 nodes.
 Top Down based approach
 Basically in top down approach the AI model is trained or developed at the cloud
 computing level using the high computational resources.Then the model is deployed to
 the edge devices.There are various techniques used in the scenarios. The whole process
 can be considered as a tree like structure. Root is considered as the central cloud and
 the leaves of the tree are the edge devices. A technique to look at the AI application
 built is that it is considered as the Cognitive Processing Elements(CPE). Then build a
 chain of CPE. A CPE is operated in basic four phases:
 • Discover phase
 • Deploy phase
 • Operate phase
 • Retain phase
 Basically in the discovery phase the basic idea is to develop the model using automated
 or user defined technique. In some scenarios multiple models might be generated and
 there should be methods to decide and select the best models out of those. After the
 model is developed they should be added into edges using the Deploy phase. Here the
 model is packed into docker containers and placed in a shared repository. Therefore edges
 can access that particular repository. Above mentioned chain of CPE is implemented as
 a Node-Red flow.
 Under the operation phase microservices are implemented on edges which are respon-
 sible for instantiating the chain of CPE flow. Docker containers in the shared repository
 will be executed at the edges by their microservices. Retaining phase is designed such
 that feedback mechanisms will be provided towards the cloud and then models can be
 updated in the cloud computing level.
 This top down approach uses high computational resources available at the cloud
 level and then deploys the models to edges. In contrast what we propose is to use the
 limited resources available at the edges, so our solution is a bottom up approach.
 Vectorization
 There are different types of approaches that have been taken to improve the performance
 of CNNs. Some are pruning, quantization and vectorization. Pruning removes the
 number of connections of a CNN. This method weakens the CNN as some weights are
 removed in this process. In quantization approach word length of weights and activations
 are reduced. FFT based convolution and Winograd convolution are some other approaches
 which improve the CNN performance. In FFT convolution operations perform in
 frequency domain. This method is suitable for larger filter sizes. For the FFT based
 convolution additional transformations are required which is a down side of a FFT
 convolution. Winograd Convolution also involves the transformation of input matrices
 and kernel matrices to perform the convolution. In vectorization approach input
 matrices and kernels are transformed inorder to perform the matrix multiplication which
 improves the performance of the convolutional operations. As the input matrices
 and kernels are transformed this method requires more memory.
 Computation offloading
 There are different studies involved in computation offloading. In offloading process
 computation is immigrated to the resourceful server or device from the limited resource
 device. This migration involves communication delays and energy consumptions so
 in order to perform the migration need to make decisions. Different approaches are
 proposed in different studies for this scenario. Some are Q learning based approaches,
 linear programming based approaches, and approaches based on defining cost functions
 for the transmission delays.
 Q learning based implementation requires additional computation power in order
 to make the decisions. Linear programming and cost function approaches transmission
 time and energy consumption for each convolution layer need be calculated before the
 decision making process. Which is an additional overhead for the decision making process.
 Simple policy based approach is discussed in these papers. By defining a simple
 policy, limited resource devices can make decisions based on the computation time and
 the transmission delays. The calculation of the computation time of the convolution
 operation using CPI gives some disadvantages. As the different devices have different
 CPIs with different pipelines and different architectures. GFLOPs base approach gives
 the advantage over the CPI methods as the manufacturers provide the information of
 the GFLOPs of the devices.
 Methodology
 Vectorization using im2col technique
 When consider the CNN network, the major operations are,
 • Convolutional operation
 • Pooling operation
 In convolution operation, the input matrix is multiplied by the kernel. As shown in figure
 3.2 the kernel needs to move through the whole input matrix based on the given stride
 which defines how many columns or rows the kernel should move next. To move the
 kernel through the whole matrix consumes time which slows down the operation. Im2col
 technique can be used to overcome this problem which basically removes the need of
 moving the kernel through the whole input matrix. We represent input matrices and
 kernels using numpy matrices. So the implementation of the im2col technique is done
 using the numpy strides. Which gave the ability to reshape the matrices in order to do
 the convolution operation in a vectorized manner.
 Using numpy strides each receptive field is turned into a column. As shown in the
 figure 1.1 2x2 receptive field is converted to a column matrix and each kernel is reshaped
 into a row matrix. Figure 3.3 shows the converted input matrix where each receptive
 field is stacked side by side in a single matrix, Then the output is multiplied by the
 kernel matrix which is reshaped into row matrices. For the pooling operation the same
 procedure is used where for a given kernel shape input matrix is reshaped and gets the
 output which depends on whether max pooling or average pooling.
 Multi threads for computation
 Raspberry pi have four cores,so any computation in a raspberry pi can be utilized to compute using all the four cores.Quad core device contains four cores.Here
 multi threads can be applied on the computational costly parts like OpenMP but available only for C,C++ and FORTRAN.
 But Python Global Interpreter Lock lets only one thread at a time to be executed.
 As OpenMP can create threads on C ,Cython converts to a separate executed c file but again the code conversion is expensive.
 Therefore we use PyMP . It uses the Fork and join model enabling use of multiple threads. Using PyMP most computationally costly parts of the algorithm is executed parallel using threads to utilize the four available cores within a raspberry pi node. While the rest of the algorithm is executed sequentially. While the threads make certain parts of the convolutional network the shared variables of certain threads should be considered. Assigning too many threads causes overhead . Therefore optimal number of threads should be assigned. The most computationally costly parts of the YOLO algorithm are selected and then different number of threads were applied. Then the optimal number of threads were selected by measuring the best execution time.
 Vectorization with multi threads
 When we combine the two approaches used to optimize the computations we had to use multi threads to compute vectorized CNN computations. Here we used an optimized BLAS library. So vectorized CNN will be performed using multi threads in OpenBLAS.This methodology combines both resource utilization using multi threads and performance improvement using vectorization techniques together which we used for the optimization.
 Computation Offloading
 Most of the IoT devices have limited resources compared to the cloud. When it comes
 to the AI and ML processing it requires more computation power and resources. Even
 though the computation distribution, vectorization and parallelization techniques apply
 some times it may not give the maximum benefits. We used offloading techniques to get
 the advantage of more resource full servers.We compared the execution time in raspberry
 pis with the execution time on the server and the communication delays based on that
 we defined a policy to determine whether to offload or execute locally.
 We compute the amount of computation using the number of floating point operations
 and GFLOPs of the executing device. When multiplying to vectors of n elements there
 involves 2n-1 arithmetic operations, then we divided it by the GFLOPs and compute
 computation time. Then we use equation (3.3) with the communications delays to
 determine the offloading decisions. As these IoT devices (Raspberry pies) have limited
 amounts of memory we also considered the memory usage.
 As shown in the equation (3.4), given a threshold value, we computed the memory
 usage of the operation and if the above equation satisfies then we offload the computation
 to the upper layers.
 Federated Learning
 The main advantage of the Federated learning is, it helps to train a model while preserving the privacy of the model data. In here
 model is train though model aggregation other than data aggression by keeping the local data private (within the local device )
 Since all the data is collected from an edge device, this is a better approach for doing computer vision tasks. Because all the annotations are done on the edge devices but the model parameters are aggregated in a central cloud server.
 This method ensures the privacy of the users. Once the model parameters are aggregated, then the global model is pushed to the user’s devices. So there is low latency in the predictions. As this is going to be a collaborative training process, the model gets smarter over time.
 Experiment Setup and Implementation
 Prototype
 In this work we choose object detection as our use case which needs high computation
 power for the training and prediction phases. Raspberry pis are used as the end devices,
 Which runs the YOLO algorithm. YOLO algorithm is based on convolutional neural
 networks. Given an image, it feeds to an convolutional network and get an output based
 on partitioning make on the image and number class. Figure 4.1 shows a input and output
 example where 600x600x3 input is feed into the CNN where output is 19x19x425.In this
 example number of predicting classes are 80 while 5 anchor boxes are used for each grid
 cell.
 The convolutional neural network consists of multiple filters in each layer so
 by dividing kernels among multiple raspberry pis, the computation can be distributed
 and parallelized. After computation done parallely then the output is merged and
 forward to the next layer. We use the kernels with the shape of (h, w, nCprev, nC) where
 the h and w represent the height and the width and the nCprev represent the number of
 channels of the input matrix and the nC represent the number of kernel of the shape
 of (h ,w, nCprev). So the computation distribution we partition the nCprev depending
 on the available nodes and then perform the convolution on the input matrix and then
 combine the each output of the each node.
 For the pooling layers we distribute the input matrix between the available nodes
 and perform the pooling operation. As shown in the figure 4.1 the master gets the input
 image and it distributes the computation among the slave nodes.
 Figure 4.1
 Master acts as a client where the slave nodes act as servers. So we use client server
 architecture to communicate between the slave nodes and the master nodes. Server nodes
 are always listening to the incoming data , when the master node receives an image
 then it distributes the computation to the slave nodes which are always listening for the
 requests.
 For convolution operations each node uses vectorization techniques and parallelization
 techniques. For matrix implementation we used numpy matrices. For the vectorization
 we used im2col technique and we got the advantage of numpy strides to manipulate the
 matrices in order to perform the convolution operations in a vectorized manner.
 Federated learning methods are used for the training network and object detection,
 Where each raspberry pi or a raspberry pi cluster trains a model based on local data and
 then the learned parameters are sent to higher layers (Fog, ROOF). Then the aggregation
 done on that layer to train a global model.
 For the data set we used We randomly captured these images of different scenes at
 different times from 26 street monitoring videos with 704×576 pixels. Eventually, we
 select a total of 2,544 items from these images with 7 object categories. Each image has
 at least one labeled object, and may have multiple labels of this same category in one
 image. The object labels are basket, carton, chair ,electromobile, gas tank,sunshade and
 table.
 while training YOLOv3 was via Adam with an initialization learning rate of 1e-3.
 We adapt the original Federated Averaging (FedAvg) algorithm to framework, we
 modified FedAvg algorithm to a pseudo FedAvg algorithm because there is an effect of
 data division for the Federated learning
 Results and Analysis
 Results
 The Darknet’s different YOLO versions were instatiated on a single Raspberry Pi node.
 The Segmentation faults occur when the program tries to access memory beyond its
 reach. That implies Darknet’s YOLO is computationally excessive for Raspberry Pi
 devices. Also YOLOv3 stops at calculating weights while Tiny YOLOv3 stops at CNN.
 Which shows optimized version can do better computation, but even Tiny YOLO can
 not complete its task.
 Then our custom YOLO implementation was tested on single node with Map reduced
 version on multiple nodes.
 The distribution of computation with multiple nodes reduces execution time. After
 the computation is divided among the cluster nodes using the Map Reduce techniques,
 we used multi threads to utilize the resources.The computation is parallized within the
 cores of each device.
 Here a variation of total execution time can be seen with respect to the number of
 threads. Therefore to find the optimal number of number of threads the results were
 tabulated.
 Fig. 5.1 Number of threads Vs the Total execution time
 Here increasing the number of threads reduces the execution time. But after a point
 the execution time increases, because the synchronization overhead happens in a limited
 cores available environment. According to the diagram the optimal number of threads
 per node is 10. Then the optimized code was compared with the same algorithm tested
 on Cloud with very high resources.
 The resource utilization with multi threads was combined with the vectorization
 approach and measured the performance gain in the distributed computation in raspberry
 Pi.
 Fig. 5.2 Total execution time vs input size for Pymp multi threads and vectorization
 with OpenBLAS optimization.
 From figure 5.2 ,with the increase of the input size, the total execution time for
 increases for both approaches. But for all the input sizes the vectorization optimized
 with OpenBLAS performs better than as it utilizes the resources efficiently.This approach
 further enables the real time computations.
 We used the equation (3.3) to make offloading decisions in the raspberry pi. The
 computation time is calculated using the GFLOPs in the given device. For the amount
 of computation, the number of floating point operations in the given convolution is
 considered and then it divided by the GFLOPs of the device. Fig. 5.3 shows the results
 of the estimation with actual time.
 Fig. 5.3 Actual processing time vs Estimated processing time with the varying channel
 size.Input matrix (64, 64, channels) with kernel shape (9, 9,channels, 256).
 Analysis
 Based on the results it is evident that the object detection algorithm we choose, YOLO
 which has a complex Convolutional Neural Network cannot be run on a single raspberry
 pi 3 board due to resource constraints. Both Darknet’s YOLO and Tiny YOLO cannot
 perform their computations on a single Raspberry Pi board. We have implemented
 a custom YOLO to run on a less-resourced environment. In a High resource enabled
 environment like Google Colab Cloud. Also, This custom YOLO implementation can
 be run on a single raspberry Pi board but the time for execution is comparatively high.
 The custom implementation was focused on the core CNN computation of YOLO. In the Cloud environment, the GPU and CPU which have capabilities up to 12 GB YOLO perform in separate efficiency. We ran our custom optimized YOLO algorithm in the High computationally capable CPU and GPU.
 Our YOLO Object detection algorithm performs nearly 9 times better than in
 GPU enabled cloud than Raspberry Pi. At the next instance, we implemented the
 optimized algorithm in distributed cluster of Raspberry Pi nodes. Here we distributed
 the computation to a cluster of two raspberry pi nodes. When the execution time in the
 Colab cloud is compared with execution time is any raspberry Pi implementation Colab
 cloud performs well. Because of the availability of high computational resources. But
 the goal is to perform the complex CNN operations at the edge. So we have successfully
 deployed our YOLO implementation which consists of the optimized CNN and performed
 the tasks with the very constrained and resource-limited environment of the Raspberry
 Pi.
 The reason YOLO performs better at the Colab Cloud is that Colab Loud provides
 around 24 times better Computational resources compared to a single edge Raspberry Pi.
 But our implementation of YOLO with the optimized CNN performed within around
 270.51 seconds even in the very resource-constrained environment. Further, the optimized
 YOLO algorithm was distributed to two parallel Raspberry Pis.
 With the parallel implementation, the Execution time is reduced nearly by a factor
 of two. This parallel implementation which distributes the computation among the Edge
 nodes performs better even with the very constrained and limited resources available at the
 edge devices. For further optimization, we used multi-threads to perform computation
 utilizing each raspberry pi cores. To find the optimal number of threads to use we
 tabulated the number of threads vs total execution time in Fig 5.1. The optimal number
 of threads per raspberry pi node was 10. With the optimal number of threads applied
 with the distributed computation using Map reduce we achieved the total execution time
 to 41.74 seconds.
 We could achieve nearly the same performance at edge when we apply distributed
 computing together with optimization using multi threads. Furthermore we applied
 vectorization for CNN which also improves the performance. Then to combine both
 of these optimization techniques we used multi threads upon vectorized CNN. Here to
 enable multi core resource utilization on raspberry pi when using vectorization, we used
 the OpenBLAS, an optimized Basic Linear Algebra Subprograms library. From Fig Fig
 5.2 we can see that for any given input size the execution time for vectorization with
 OpenBLAS is less than multi threading with PyMP implementation. Therefore we used
 vectorization with OpenBLAS for the core CNN computation of YOLO to enable real
 time computations.
 As previously explained we use the policy defined by the equation (3.3) to make the
 offloading decisions. For that we needed to calculate the execution time on the raspberry
 pis. We used GFLOPs based approach with the number of floating point operations to
 calculate the execution time. Figure 5.3 shows the execution time with different input
 matrices. Estimated times are shown in the red columns, when compare with the actual
 execution time our GFLOPs based method was able to estimate the execution time close
 to the actual execution time.
 Table 5.1 shows the vectorized convolution execution time with the non vectorized
 implementation.For the input matrix (64,64, channels) with the kernel shape (3,3,chan-
 nels,256). There is a high improvement with the vectorization. When consider the
 channel size of 128,
 Vectorization improved the convolution operation by a factor of 85.
 Conclusion
 The aim of the project is to enable real time processing at the edge with limited
 resources.So with combining above mentioned techniques, real time processing at the
 edge is achievable. In this study we have developed a distributed CNN using map reduce
 which can be used to implement YOLO.Furthermore, we propose a novel approach where
 end devices consisting limited resources can train and generate real time decision in
 distributed manner, where their computations are distributed among other multiple
 nodes or offload the computation to the upper layer when the resources run out. We
 have distributed the CNN over multiple Raspberry Pis.The performance of the CNN has
 been measured under different conditions and platforms. Based on the results it can be
 concluded that by distributing CNN over multiple nodes the computation latency can be
 reduced.Then we optimized our implementation using vectorization and multi threads
 together so the execution time is reduced further enabling the real time computations
 using limited resources.
 Offloading decisions were taken using a policy where the computation time and
 network transmissions delays were considered. Also we considered the memory usage for
 the computation which also affected the offloading decisions. We also apply federated
 learning to train a global model where each device is able to train a local model which
 addresses the privacy issues. These techniques provide the path to achieve better results
 using limited resource devices.
 Publications
 Semester 7 report
 Semester 7 slides
 Semester 8 report
 Semester 8 slides
 Links
 Project Repository
 Project Page
 Department of Computer Engineering
 University of Peradeniya
 Back to top","To facilitate AI,ML computations and to enable real time processing at the edge,a novel approach is proposed base on computation distribution, vectorization, computation offloading, parallelization and federated learning techniques.",E15,Undergraduate Research Projects,https://projects.ce.pdn.ac.lk/4yp/e15/Real-Time-Data-processing-and-AI-for-Distributed-IoT/,https://github.com/cepdnaclk/e15-4yp-Real-Time-Data-processing-and-AI-for-Distributed-IoT,https://cepdnaclk.github.io/e15-4yp-Real-Time-Data-processing-and-AI-for-Distributed-IoT,https://cepdnaclk.github.io/e15-4yp-Real-Time-Data-processing-and-AI-for-Distributed-IoT/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/4yp/E15/Real-Time-Data-processing-and-AI-for-Distributed-IoT/
92,Real Time Emotion Recognition using Electrocardiogram Analysis,"Real Time Emotion Recognition using Electrocardiogram Analysis
 Projects - Department of Computer Engineering
 University of Peradeniya, Sri Lanka
 Project Title
 Team
 E/15/139, Ishanthi D.S. , e15139@eng.pdn.ac.lk
 E/15/249, Pamoda W.A.D. , dasunip2@gmail.com
 E/15/299, Ranushka L.M. , e15299@eng.pdn.ac.lk
 Supervisors
 Dr. Isuru Nawinne, isurunawinne@eng.pdn.ac.lk
 Prof. Roshan Ragel, roshanr@eng.pdn.ac.lk
 Dr. Suranji Wijekoon, suranjisk@gmail.com
 Mr. Theekshana Dissanayake, theekshanadis@eng.pdn.ac.lk
 Table of content
 Abstract
 Methodology
 Experiment Setup and Implementation
 Results and Analysis
 Conclusion
 Links
 Abstract
 Most of the ECG analysis based human emotion recognition studies use different types of machine learning techniques. Main problem with these methods is lack of accuracy and not having the ability to classify emotions real-time. The proposed method uses a large public dataset to increase accuracy and implements a Convolutional Neural Network to identify emotions. ECG data signals are preprocessed to increase the number of instances and important features are extracted using feature extraction methods and then features are fed to the CNN. Three CNN models are trained to predict the valence, arousal and the dominance values of the ECG signal, which are used to finalize the emotion by mapping those values to the valence-arousal-dominance 3D plane.
 The classification CNN models implemented in this proposed method result in a maximum accuracy of 80%.
 Methodology
 This research is planned to do in 2 phases to implement the human emotion recognition model and to do a preliminary analysis on animal emotion recognition.
 Phase 1 - Human Emotion Recognition
 As the first phase of the research, human emotion recognition model was implemented using ECG signals. The bio signals such as ECG signals are nonlinear, complex and contain noises. Since neural networks handle such data in a more efficient way than machine learning methods, neural network is a more suitable method for the classification of emotions using ECG signals.
 In neural networks, convolutional neural networks (CNN) are more suitable for time series analyzing since they identify and extract important features that support the classification process from raw input data. Therefore, the model for the recognition of human emotions using ECG signals was implemented using CNNs.
 For the human emotion recognition model, a public dataset, which consists of ECG signal data of human subjects was used. These data signals are preprocessed to increase the number of instances and to extract the important features to feed to the CNN. The CNN is trained to predict the valence, arousal and the dominance values of the ECG signal, which is used to finalize the emotion by mapping those values on the valence-arousal-dominance 3D plane.
 Experiment Setup and Implementation
 Dataset
 For the training of a neural network, a large amount of ECG signal data was needed. Therefore, the DREAMER dataset, which is a multi-model dataset for emotion recognition through ECG and EEG signals, was used for this research.
 This dataset consists of the ECG signals of 23 human subjects, each subject’s ECG signals have been recorded using 2 ECG channels under 256 Hz sampling rate while watching 18 video clips. They have been asked to score their emotions using valence, arousal and dominance values in a range of 0 to 5. Therefore, the total number of 414 labeled signals were obtained from this dataset.
 To get an initial idea of the ECG signals, signals with different valence-arousal-dominance values are visualized. The output graphs
 indicate that the ECG signal changes over a 5 seconds period 8 different valence, arousal, dominance value combinations. Each combination of these values are related to a different emotion of the human subject. It is visible that the amplitude, shape and the pattern of the signal are different for different emotional states.
 Preprocessing
 To increase the number of instances fed to the CNN, some preprocessing steps were done on the above mentioned ECG data signals.
 Therefore, as the first step, those signals were split into sub segments. The emotion changes of a subject can be identified within a period of 3 – 15 seconds. Considering this fact, the above 414 signals were split into segments of 10 seconds period considering 1 second overlapping time. This process created 82 018 input instances for the CNN.
 Feature Extraction
 To extract the unique features of these ECG signals, Mel frequency cepstral coefficients (MFCC) algorithm, which is an efficient technique for signal processing based on Discrete Fourier Transform (DFT), was used. This calculates a MFCC feature vector with coefficient values for the input signal.
 Each signal consisted of 2 ECG channels therefore each segment of the signal also consisted of 2 ECG channels and the MFCC feature extraction was done for both channels.
 13 MFCC coefficients were extracted and this calculated feature vectors of size (853 x 13) for each segment. Since these segments have 2 channels, the input instance shape was (853 x 13 x 2). Both the input instances and the array of valence, arousal and dominance values were normalized. Each normalized input instance and the relevant normalized valence, arousal and dominance values were stored in separate files before training the CNN.
 CNN
 Finally, 82018 data instances of shape (853 x 13 x 2) were fed to CNN to predict the values of valence, arousal and dominance.
 The dataset that was used had 2 channels for the ECG signal. And as the MFCC feature extraction method creates a 2D output of coefficients of time and frequency variation, a 2D convolutional Neural Network is used for the classification of ECG time series data.
 Neural Network structure:
 Convolutional Neural Network
 Conv2d - Input
 MaxPooling2d
 Dropout – 20%
 Conv2d
 MaxPooling2d
 Conv2d
 MaxPooling2d
 Flatten
 Dense layer - 200 nodes with ReLu activation
 Dense layer - 150 nodes with ReLu activation
 Dense layer - 75 nodes with ReLu activation
 Dense layer - 25 nodes with ReLu activation
 Dense layer - 3nodes with ReLu activation
 Convolution Layers:
 Convolution is a linear operation and it is done in parallel in the Conv2D layers. Conv2D layers are used for feature Mapping. Each Emotion elicited ECG signals are transformed into a set of 2D Coefficients and those coefficients contain the patterns related to the emotion. Therefore, in the training phase the filters of the convolution layers are trained to map the features so that they will act as the feature detectors.
 MaxPooling Layers:
 Pooling layers are used to down sample the given input to extract the features in another position. MaxPooling is used to extract the most activated feature among several features. Therefore before a convolutional layer a MaxPooling layer is placed.
 Dropout Layer:
 Dropout layers are used to delete some trained neurons. This process is done to train a model more accurately. It is expected to have a better training when 20% of trained neurons are reset to initial state.
 Flatten layer:
 This layer is used to prepare the inputs to the dense input layer.
 Dense layer:
 These layers are used to classify the inputs into 3 classes. Final dense layer has 3 output neurons. One or more hidden dense layers are expected to be used until a better accuracy is gained.
 3 types of CNNs using regression CNN models and classification CNN models were implemented as different approaches to predict valence, arousal and dominance values.
 Single Regression CNN model
 In this approach, a single regression CNN model was implemented to predict 3 values of valence, arousal and dominance at the same time. Three labels of valence arousal and dominance in the range of 1-5 were used. The output of each model will be three normalized values in the range of 0-1 which describes valence, arousal and dominance values accordingly.
 Separate Regression CNN models
 In this approach, 3 separate regression CNN models were implemented to predict valence, arousal and dominance. The labels of valence arousal and dominance in the range of 1-5 were normalized into the values of 0.00 , 0.25, 0.50, 0.75, 1.00. The output of each model will be a normalized value in the range of 0-1 which describes valence, arousal and dominance values accordingly.
 Separate Classification CNN models
 In this approach, 3 separate classification CNN models were implemented to predict valence, arousal and dominance. The labels of valence arousal and dominance in the range of 1-5 were one hot encoded. The output of each model will be a value in the range of 0-4 which describes valence, arousal and dominance values accordingly.
 Discrete emotional model
 To identify and label the emotions related to predicted arousal, valence and dominance values, 3D- valence-arousal-dominance-plane is used. 4 negative discrete emotions as angry, fear, unconcerned, sad and 4 positive emotions as happy, surprise, satisfied and protected can be classified using this model.
 Pitfalls and workarounds
 This dataset was a large dataset with 82 018 instances. Initially we tried to load the whole datasets at once, but it required a lot of processing. Therefore, instead of having one large dataset, we created single data files for each instance and used python data generators to load data to the CNN.
 When training the neural network we faced an issue of insufficient memory and low speed in the machine. This problem was solved by handling the training of the CNN on the GPU of the kepler server.
 Phase 2 - Preliminary Analysis of Animal ECG data
 Heart rate variability (HRV) is the physiological event of the variation in the time interval between consecutive heartbeats in milliseconds using ECG signals. Normally heart rate variability measure rises when a person is engaged in a relaxing activity and it reduces when a person is under stress. Therefore, HRV can be used as a measurement to assess the emotions of a person by evaluating his or her autonomic nervous system. Therefore, for this preliminary study of animal ECG signals, HRV parameters were used. Typically, HRV is analysed using time domain, frequency domain and non linear metrics.
 Background
 Time-Domain Parameters
 In this method, QRS complex is identified and the heart rate at any point in time or the intervals between successive normal complexes are determined and the following parameters are calculated.
 Some of the most widely calculated time domain parameters are:
 mean_nni: Mean time interval between two heartbeats, here normal heartbeats are considered.
 sdnn: the standard deviation of all the NN intervals. It can be described as a total variability or total power.
 sdsd: the standard deviation of the differences between successive NN intervals
 nni_50: the number of pairs of successive NN intervals that differ by more than 50 ms in the entire recording
 pnni_50: the percentage of successive intervals that differ by more than 50 ms (higher values indicate increased parasympathetic activity)
 rmssd: the square root of the root mean square of the sum of all differences between successive NN intervals
 median_nni : Median Absolute values of the successive differences between the RR-intervals.
 range_nni: difference between the maximum and minimum nn_interval.
 Frequency-Domain Parameters
 In frequency domain analysis, frequency components of the ECG signals are obtained as VLF (Very Low Frequency – 0.00 Hz- 0.04 Hz), LF (Low Frequency– 0.04 Hz- 0.15 Hz) and HF (High Frequency– 0.15 Hz- 0.40 Hz). This is performed by decomposing RR intervals of an ECG signal using Fast Fourier Transformation (FFT).
 Dataset
 The PhysioZoo database consists of ECG signal recordings taken from multiple types of mammals such as dogs, rabbits, mice, etc. It has dog ECG recordings of an average length 05.31 (min:sec), rabbit ECG recordings of an average length 10.34 (min:sec) and mouse ECG recordings of an average length 29.44 (min:sec). It has recorded these ECG data at a sampling rate of 500Hz for dog subjects, 1000 Hz for mouse and rabbit subjects.
 Methodological approach
 For animal ECG analysis, ECG signal data of dog subjects, mouse subjects and rabbit subjects were used from the above mentioned publicly available dataset. Initially they were downsampled to a frequency of 256 Hz. Then the HRV parameters were extracted from 134 s duration of ECG signals using time domain and frequency domain methods using python hrv-analysis library.
 Then these parameters were compared with the human HRV parameters.
 Data visualization
 Initially the data was visualized to get a basic idea of the QRS complex and the shape of ECG signal recordings of each animal type.
 Results and Analysis
 The CNN was trained using
 60000 instances and 20000 testing data. The results were taken by training the model changing its parameters as well as the hyper-parameters such as batch size, steps per epoch, epochs etc.
 CNN for classification of Arousal. Valence and Dominance Values
 The dataset consisted of 82000 samples of MFCC features of ECG signals with 5 labels for Arousal category. The dataset was divided as follows,
 Training Set - 70%		Validation Set - 20%
 Test Set - 10%
 In each three figures above has two graphs representing Model accuracy and Loss. If the Accuracy graphs (Top graph) are considered the orange line which represents the validation set accuracy has come to an instance which is called a valley after 1000 epochs. This implies that the model accuracy will not considerably increase further. All the three models show this kind of variation.
 On the other hand, If the start of the graphs were considered, there has been no change in accuracy or loss in the first 180 epochs approximately. The optimizer used for the models was Adam optimizer. In general, in the search for the solution for a Machine learning problem, the models come across with many solution points which are seemingly better. But only one of them is the best solution. That solution is also called the “Global Minimum”. At that point the loss is minimum.
 The other solutions are called “Local Minimas”. Therefore, the model must be able to travel in the path of global minima for a successful result. Following figure (Figure: 03) describes the difference between the global and local minimas. Therefore, It is clear that there is one global minimum and there could be one or more local minimas.
 Clearly, the model tries to find the path for global minima in the beginning of the training time. As a result there will not be any change in the accuracy and the loss during that time. This is performed by the optimizer. Therefore, better optimizers must be used according to the model. There are different types of optimizers and each one has different properties. Adam optimizer is better for all the cases. Therefore it is used here.
 Finally, The loss variation of each graph shows another great fact. As you can see the validation loss (Orange line) started reducing when the model has found the global minima and after a long time it starts to increase. But the loss of the training set never decreases. This instance shows the overfitting of the model. As the training loss decreases the test loss or the validation loss increases. By the end of the 1000th epoch the model has been trained only for the training set. Therefore, the moment the test loss starts to increase can be used as the end of the training if the model is going to be used for ECG data recorded by other devices. Therefore the models may actually have lesser accuracy than mentioned here. For example, The model developed for Dominance may have an accuracy of 70% instead of 80.87%.
 Conclusion
 Emotion recognition is a powerful and very useful technique in the modern world since it has a large
 scale of uses in various areas. We can say a lot of research has been conducted on this
 subject using different methods.
 Some methods give higher accuracy but some do not. Scientists have
 come up with new techniques to increase the accuracy by inventing new feature extraction methods,
 classification methods, machine learning models and neural networks. Even though some methods give
 higher accuracy they may be practically hard to use because of the non- wearable nature of the
 hardware implementation. Since emotion recognition is a key feature of Human Computer Interaction, as the field grows
 in sophistication people need easily usable methods. That is why the combined bio signal method is not
 so used even though it has a good accuracy. In the near future by these emotion recognition methods the Human Computer Interaction would be
 more effective and by that the productivity will increase in every computer using field including
 healthcare, education, production industry, entertainment and automotive industry etc, making human
 and animal lives better and easier.
 The highest emotion classification accuracy obtained by this study is 80.87%. This was achieved by overfitting the model of that dataset. Although the model was overfitted the model showed considerable accuracy before it overfits. MFCC features of the ECG signal were extracted. That produced a 2D feature vector of time and frequency. However, different feature extraction methods should be tried to increase the accuracy.
 Links
 Project Repository
 Project Page
 Department of Computer Engineering
 University of Peradeniya
 Back to top","Emotion recognition is a powerful and very useful technique in the modern world in almost every field including healthcare, entertainment, automotive industry etc. The main objective of this project is to implement a real time model, which can recognise an extended range of emotions with a higher accuracy using ECG signals to recognize sudden and complex emotional changes of humans. While identifying human emotions, identifying emotions of animal subjects is equally important since it will help to understand their perspectives, needs and problems related to their physical and mental well-being while safeguarding their welfare. Therefore, a preliminary analysis to identify similarities and differences between human and animal emotion recognition is done.",E15,Undergraduate Research Projects,https://projects.ce.pdn.ac.lk/4yp/e15/Real-Time-Emotion-Recognition-using-Electrocardiogram-Analysis/,https://github.com/cepdnaclk/e15-4yp-Real-Time-Emotion-Recognition-using-Electrocardiogram-Analysis,https://cepdnaclk.github.io/e15-4yp-Real-Time-Emotion-Recognition-using-Electrocardiogram-Analysis,https://cepdnaclk.github.io/e15-4yp-Real-Time-Emotion-Recognition-using-Electrocardiogram-Analysis/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/4yp/E15/Real-Time-Emotion-Recognition-using-Electrocardiogram-Analysis/
93,Revealing miRNA Biomarkers for Alzheimer s Disease using NGS,"Revealing miRNA Biomarkers for Alzheimer's Disease using Next Generation Sequencing data
 Projects - Department of Computer Engineering
 University of Peradeniya, Sri Lanka
 Revealing MicroRNA Biomarkers for Alzheimer’s Disease Using Next Generation Sequencing Data
 Team
 e15362, Hasini Thilakarathna, email
 e15345, Vidwa Sripadi, email
 e15081, Imalsha Dinuwanthi, email
 Supervisors
 Dr. Damayanthi Herath, email
 Prof. Roshan Ragel, email
 Table of content
 Abstract
 Related works
 Methodology
 Results and Analysis
 Conclusion
 Publications
 Links
 Abstract
 Alzheimer’s disease is recognized as one of the
 common diseases found among older people, which still has no
 successful cure. In this study, our goal is to determine the best set
 of miRNA biomarkers which are highly differentially expressed
 in Alzheimer’s disease. Using statistical analysis followed by
 machine learning techniques, we establish 25 microRNAs as
 biomarkers for AD. Furthermore, we provide an analysis of the
 selected 25 microRNAs with area under the receiver operating
 curve and classification algorithms.
 Related works
 Sample selection
 When detecting biomarkers for Alzheimer’s disease, initially we have to select a sample for
 performing analysis. Mostly blood samples are used due to the high availability. Different
 types of blood samples including whole blood, serum and plasma
 are used by many previous researchers where they have tried to find miRNA biomarkers.
 If we use brain samples it would give most accurate results than blood samples since
 AD is most prominently active in brain. We would be able to give more accurate
 results if both blood and brain samples are used. Samples can be taken from participants
 generally as, AD and controls and also they can be taken considering the
 different stages as severe, moderate, mild AD and controls. Another approach in
 collecting samples is taking then from participants with HC, MCI and AD. The
 number of samples used when developing a diagnosis method can be identified as one of
 the main factors which could affect the final results. Next generation sequencing platform
 is the most trending method used for gathering samples for various disease diagnosis
 researches. Many techniques like Illumina sequencing technique are
 introduced for working with NGS data. Preprocessing the raw sequence counts can be
 done using a bioinformatic pipeline, which gives the read counts for each miRNA as the
 final outcome.
 Normalization
 Normalization of sequencing read counts can be performed using several normalization
 methods. Quantile normalization is one way that we can do normalization when we are
 having a high dimensional dataset. It excludes selected samples to minimize noise.
 Mean normalized read counts also can be used to filter out the miRNAs. Also we can
 follow a stepwise procedure to do normalization as below.
 From all the samples, find sequences which are common.
 Build a reference dataset using those common sequences.
 Apply logarithmic transformation
 Calculate the logarithmic difference between each sample and reference dataset.
 Form a subset by taking sequences which has a difference<2.
 Perform linear regression.
 Calculate the mid value
 Looking at the results obtained from the study which used the above normalization
 method, it can conclude that this type of step wise normalization method can be used
 for obtaining the best set of miRNAs. Data visualization can be used for selecting which
 normalization method is best suit for a given dataset.
 Statistical Analysis
 Initial detection of miRNA can be done by initially calculating a significance value(p
 value). P value is a value between 0 and 1, which shows the level of statistical significance.
 If a p value is less than the significance level (0.05), it is considered as a nominally
 significant p value and we can select those miRNA as the most impacting miRNAs.
 WMW test, Wald test and Fisher’s exact test can be used to calculate the p
 values and these p values can be adjusted for multiple testing using an approach like
 Benjamini-Hochberg approach. Other than that, t test and kruskal test can also
 be used to calculate significance values.
 Validation of samples
 Validation of the samples makes it easier for the next steps in the investigation and
 also it makes the final results more accurate. After the statistical analysis process,
 for validating the obtained samples, quantitative real time-polymerase chain reaction
 (qRT-PCR) method is used by many researchers. It analyzes the expression of single
 miRNAs by applying the method on previously used samples for sequencing.
 But in a previous study, they have additionally included patients with AD and also
 patients with other neurological disorders in the validation step, to analyze the the set
 of miRNAs they obtained in the previous step. After the validation is carried out, the
 miRNAs can be further filtered out to obtain the most significant miRNAs.
 Receiver operating characteristic curves
 Receiver operating characteristic curve analysis is used to evaluate the performance or
 accuracy of a classification model. ROC is a plot of sensitivity against specificity for
 selected samples. It is also used to initially detect the dysregulation of miRNAs and
 to discriminate between AD and NC sample groups. The area under the curve is the
 degree of separability. If the AUC is high, that means that particular miRNA is better
 to distinguish patients with AD and control.
 Feature selection and Classification
 If we use a classification model without using feature selection, it will take more run time
 due to the huge size with redundant features. Therefore it is required to apply some
 feature selection method to reduce those redundant features. Hierarchical clustering
 is a feature selection method which can be used to statistically analyze the dataset.
 It will build clusters of miRNAs having similar patterns. Principal
 Component Analysis is another approach which can be used for the feature selection.
 Machine learning classifier models are used to predict whether a sample belongs
 to AD or control. AdaboostM1, J48 decision tree, random forest and support vector
 machines and radial basis SVM are some machine learning approaches that can be used
 for building prediction models. In a previously done study, they have built a separate
 model by performing 7-way cross validation using 7 randomly picked partitions of 5
 positive and 5 negative samples each for the feature selection.
 Summary
 According to the review we have done, we identified how we can use miRNAs to diagnosis
 AD and what are the miRNA diagnostic biomarkers which can be found in AD patients.
 In each study, for filtering out the candidate miRNA, step wise procedures including
 initial detection and statistical analysis have performed. When consider about previous
 studies, there are several limitations. The most common limitation of most of the research
 is they used a limited number of the cohort to their experiments. It is hard to find a large
 number of Alzheimer’s disease patients to do massive experiments. But we can obtain
 better results if we expand the cohort size. In many studies, samples with analyzed
 dementia and controls have used. But not discussing about the possibility to discover
 pre-clinical biomarkers for Alzheimer disease is a limitation of most of the previous
 studies. A model which was built in a one previously done study, does not develop
 to anticipate movement from HC to MCI or MCI to AD. Also, this model was incapable
 of applying for late-stage AD findings. In another study, they have mentioned that
 they were unable to recognize a mechanism to identify the variation of miRNAs in serum
 samples. Considering all the drawbacks, limitations and also the developments found in
 the previous studies, in this research, we are focusing on finding a more accurate solution
 for detecting AD biomarkers.
 Following Figure shows a summary of different methods used andthe results obtained in previous studies. According to this diagram, only 4 studies haveused machine learning algorithms and only 5 studies have used statistical methods intheir studies. Out of the results obtained from above mentioned 9 studies, 7 miRNAswere identified as common for those 9 studies.
 Methodology
 Data collection
 We used a data set available in National Center for Biotechnology Information (NCBI) database under the access number
 GSE46579. It includes 70 samples with 22 control and 48 AD
 and 2652 miRNAs.
 Preprocessing
 The Next Generation Sequencing data preprocessing was
 done using the Galaxy platform. Galaxy is a web-based,
 opensource platform for scientific data analysis. First, the quality
 report of sequencing data was generated using the FastQC
 tool. Then, using the tool Trim Galore, data trimming was
 performed. The package Trim Galore allows both quality
 trimming and adapter trimming at once. Low-quality reads and
 adapters were removed from sequence read in the trimming
 procedure. Trimming increases the quality of sequences. Next,
 the data filtering procedure was performed using the Filter
 FASTQ tool. Short read sequences and low-quality sequences
 were removed in filtering. After that, the NGS reads were
 mapped against a reference genome (h38) using Bowtie2.
 Bowtie2 tool aligns sequences to the long reference sequences.
 Then, the reads were mapped against the hsa.gff3 miRNA
 precursor sequences from the miRBase database (v22) and
 the number of read counts of each miRNA was found using
 the htseq-count tool. This preprocessing procedure was done
 for every sample using the galaxy platform and then the
 summarized dataset was created with miRNA read counts of
 each sample. For the analysis purposes we used a data set with
 highly abundant miRNAs. To do that we considered miRNAs
 with read counts less than 50 across all samples of AD and
 control separately, as lowly abundant and removed them from
 the data set. Considering the mean distribution of the data
 set, we normalized the data set using quantile normalization
 technique instead of general normalization technique.
 Statistical Analysis
 Normalized data set obtained from the previous stage was
 further analyzed with significance value and fold change to
 reduce the number of features. We calculated the pValues for
 each miRNA using Wilicoxon-Mann-Whitney (WMW) test.
 Generally, fold change is a technique which is used to get
 an idea of how much change occurs going from one value
 to another. In this project we tried to get fold change values
 (log2) for each miRNA, to check the significant changes of
 each miRNA across AD and control samples. We used cut
 off values for pValues and fold changes values as 0.05 and
 1 respectively, to obtain the highly expressed set of miRNAs.
 For each of those filtered features, we calculated the AUC
 values. Using those AUC values, another set of features were
 filtered out. Features with AUC score less than or equal to 0.5
 were ignored as they don’t make a significant impact on the
 classification of the data set.
 Feature Selection
 Initially we used two different methods as PCA and Random
 Forest for selecting the best set of features. For the data
 set we obtained from the previous stage, we separately did
 PCA analysis and Random Forest analysis. Univariate feature
 selection method was used to decide how many features we
 needed to select from each. Features which have a significant
 relationship with the class value were identified from this
 univariate feature selection method. Next, the set of overlapped
 miRNAs from those two methods was identified as the best set
 of features which could be obtained from this part of feature
 selection. In the next part of the feature selection stage, we
 used correlation coefficient. As the correlation coefficient, we
 used Pearson correlation coefficient.
 Classification
 Classification accuracy was used to see how accurate our
 predictions were. A set of machine learning algorithms were
 modelled for the initial data set and out of those the most
 accurate algorithms were identified. Those each pre-identified
 algorithms were used for obtaining the classification accuracy
 of the final data set with biomarker miRNAs.
 Validation
 For validating the results we used Human MiRNA Disease
 Database version 3.2 (HMDD v3.2). HMDD contains a large
 set of miRNAs and related diseases collected from the literature. There are 35547 miRNA-disease associations in version
 3.2 and it includes 1206 miRNAs, 893 diseases from 19280
 related publications.
 Results and Analysis
 At the end of the preprocessing stage, a data set with
 513 highly abundant miRNAs was obtained after removing
 miRNAs with less than 50 read counts across all samples.
 Considering the cut-off significance value as 0.05 and fold
 change (log2) as 1, the number of features were reduced up to 228.
 With the AUC analysis we further reduced the number of
 miRNAs and at the end of the statistical analysis, we identified
 219 miRNAs as a set of highly expressed miRNAs. From the
 univariate feature selection method, we identified 50 miRNAs
 which have a significance relationship with class value. We
 identified 14 common miRNAs from two sets of miRNAs
 selected from PCA and random forest analysis.
 They are hsa-miR-186-5p, hsa-miR-144-3p, hsa-miR-151a-3p,
 hsa-miR-99b-5p, hsa-miR-98, hsa-miR-148a-3p, hsa-let-7g-5p,
 hsa-let-7f-5p, hsa-let-7a-5p, hsa-miR-30d-5p, hsa-miR-15a-5p,
 hsa-miR-589-5p, hsa-miR-144-5p, and hsa-let-7f-5p.
 We used heat maps for making a judgement on the correlation of each features obtained from previously mentioned two
 methods. Figure 2 and Figure 3 show how different features obtained from PCA and Random Forest analysis, are correlated.
 With the help of the heat maps, we decided to use 36 less
 correlated features for further analysis using correlation coefficients. Out of the different machine learning algorithms which
 we modelled for our initial data set, we identified the three
 most accurate algorithms namely, Support Vector Machine,
 Logistic Regression and Random Forest. For the data set with
 the 36 miRNAs, we calculated classification accuracy using
 those three models by varying the correlation coefficients. For
 each model we identified a correlation coefficient which gave
 the highest accuracy. Fig. 4 shows the plots with correlation
 coefficient against the classification accuracy of three models The three correlation coefficients were 0.9975, 0.5875 and
 0.5300 for SVM, Logistic Regression and Random Forest
 respectively. Using those 3 correlation coefficients, three sub
 sets of miRNAs were obtained from the earlier used 36. As we
 calculated classification accuracy for the previously mentioned
 three subsets, we identified 11 miRNAs which provided the
 highest classification accuracy. They are, hsa-miR-4781-3p, brain-miR-112, hsa-let-7a-5p,
 hsa-miR-148b-5p, hsa-miR-29b-3p, brain-miR-431, hsa-miR-378a-5p, hsa-miR-548h-5p, hsa-miR-3909,
 hsa-miR-625-5p, and hsa-miR-24-3p.
 Conclusion
 In this report we have discussed about how to detect miRNA biomarkers for Alzheimer’s disease using next generation sequencing. Initially we have discussed about the need of a solution to identify Alzheimer’s disease in the early stage. Then we have mentioned about the literature review we have done. When we were doing the literature review, we have identified several miRNA biomarkers in different studies which used NGS. In these studies there were some limitations.
 In our approach so far, initially we have taken samples from participants with AD and control. Then samples were preprocessed and statistically analyzed. Significance values were calculated using Wilcoxon-Mann-Whitney (WMW) test. Also we have used ROC analysis. Using this procedure, here we have identified a set of significant miRNAs for AD. Using PCA, Random Forests and Correlation coefficient we identified 25 biomarker miRNAs for AD. In the next phase we validated the the result using HMDD v3.2.
 Leidinger et al., who have carried out a different method to find biomarkers using the same data set, have stated that they have obtained an accuracy of 93.3\% where we obtained an accuracy of 95.24\%. Addition to that we evaluated the results with specificity, sensitivity and AUC values as discussed previously. In addition to diagnosis of AD patients with the final set of biomarkers, the followed methodology can be used to identify different cures for other neurological diseases including AD, by effortlessly analyzing various data sets.
 Publications
 Semester 7 report
 Semester 7 slides
 Semester 8 report
 Semester 8 slides
 Links
 Project Repository
 Project Page
 Department of Computer Engineering
 University of Peradeniya
 Back to top","This main aim of this project is to find a set of miRNA biomarkers for Alzheimer's Disease using Next Generation Sequencing Data. A dataset obtained from NCBI database under the access number GSE46579 is used. To achieve the goal of finding the best set of differentially expressed miRNAs, we followed a step wise procedure including five main steps as data collection, preprocessing, statistical analysis, feature selection and classification. Under the data collection stage, initial data set was chosen. We used Galaxy platform for preprocessing. The statistical analysis included significance value (pValue) analysis, fold change analysis followed by the area under receiver operating curve (AUC) analysis. For the features reduced from the statistical methods, feature selection was done under three methods as PCA, Random Forest and correlation coefficient analysis. We used classification algorithms for presenting the possibilty of miRNAs in making better predictions. In this project, we used python programming language for both statistical analysis and machine learning analysis of the data.",E15,Undergraduate Research Projects,https://projects.ce.pdn.ac.lk/4yp/e15/Revealing-miRNA-Biomarkers-for-Alzheimer-s-Disease-using-NGS/,https://github.com/cepdnaclk/e15-4yp-Revealing-miRNA-Biomarkers-for-Alzheimer-s-Disease-using-NGS,https://cepdnaclk.github.io/e15-4yp-Revealing-miRNA-Biomarkers-for-Alzheimer-s-Disease-using-NGS,https://cepdnaclk.github.io/e15-4yp-Revealing-miRNA-Biomarkers-for-Alzheimer-s-Disease-using-NGS/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/4yp/E15/Revealing-miRNA-Biomarkers-for-Alzheimer-s-Disease-using-NGS/
94,Wireles SDN Ad hoc Network For Vehicular Communication,No project page,,E15,Undergraduate Research Projects,https://projects.ce.pdn.ac.lk/4yp/e15/Wireles-SDN-Ad-hoc-Network-For-Vehicular-Communication/,https://github.com/cepdnaclk/e15-4yp-Wireles-SDN-Ad-hoc-Network-For-Vehicular-Communication,#,#,http://api.ce.pdn.ac.lk/projects/v1/4yp/E15/Wireles-SDN-Ad-hoc-Network-For-Vehicular-Communication/
95,anonymous authentication,"Anonymous and Distributed Authentication for Peer to Peer Networks
 Projects - Department of Computer Engineering
 University of Peradeniya, Sri Lanka
 Anonymous and Distributed Authentication for Peer to Peer Networks
 Team
 E/15/350, Pasan Tennakoon, email
 E/15/180, Supipi Karunathilaka, email
 Supervisors
 Dr. Janaka Alawathugoda, email
 Table of content
 Abstract
 Related works
 Methodology
 Experiment Setup and Implementation
 Results and Analysis
 Conclusion
 Links
 Abstract
 The traditional authentication mechanisms like PKI and ID-PKC are difficult to integrate
 with a P2P like decentralized network environment. This task becomes even more difficult
 in an anonymous P2P environment. This research proposes three novel authentication
 protocols such that users can authenticate themselves in an anonymous P2P network
 without revealing his/her identity. First, we suggest a way to use existing ring signature
 schemes to obtain anonymous authentication. Then we propose an anonymous authentication scheme utilizing secret sharing schemes. Finally, we propose a zero-knowledge
 proof based anonymous authentication protocol. We provide security proofs of the three
 protocols including anonymity, completeness, soundness, resilience to impersonation and
 resilient to replay attacks. Then we compare the performance of the new protocols. We
 deploy these protocols in a P2P environment build using the .Net framework. We utilize
 Shamir’s secret sharing algorithm to manage certificates in the distributed environment.
 Introduction
 The concept of Peer to peer (P2P) communication has gained significant attention in the network community over the years. Since the release of Napster in 1998 many P2P applications have been introduced. Bitcoin[1], BitTorrent, TOR[2], Freenet[3], etc are some of the more popular P2P applications. The absence of centralized authority is the main reason behind the popularity of P2P applications. This eliminates the need for an expensive central server. Also removes the vulnerability of a single point of failure. P2P networks are considered to be more efficient and scalable than traditional client-server applications.
 The decentralized nature of P2P networks makes it difficult to integrate traditional authentication mechanisms. Due to this many such networks focus on providing user anonymity rather than authentication. The reduced security of these networks has created a lot of possible threats [4]. These threats can vary from uploading malicious files to famous Sybil attacks [5]. Also, the anonymity feature of these networks has created a safe house for cyber criminals [6]. Not being accountable for his/her actions, held responsible and punished for malicious actions, P2P users have the freedom to misbehave. This can cause harm to the network and its users.
 We suggest that even in an anonymous network there should be some level of accountability to protect the network and its users. Accountability is achieved through authentication.
 To integrate an authentication mechanism into an anonymous P2P environment we need to solve two main challenges.
 •Authenticate in a decentralized environment.
 •Authenticate without revealing identity.
 These two points have been discussed separately since the start of the internet. Each point has its own difficulties and challenges. Authentication needs to tackle problems like the absence of a central server, certificate management in a distributed environment, the semi-trusted nature of peers, the unpredictable availability of peers, etc. Authentication needs to solve problems like not revealing sensitive information about authenticating party’s identity, secure against misbehaving parties (cheating verifiers and cheating provers), unlinkability of authentication sessions, practicality, etc.
 In this paper, we propose three new approaches for anonymous authentication in P2Pnetworks to solve the above problems.
 Ring signature based approach.
 Authenticated key sharing based approach.
 Zero knowledge proof based approach.
 We deploy these protocol in a P2P environment where certificates are managed by peers with elevated privileges (super peers). To solve the problems of semi-trusted nature and unpredictable availability of peers we utilize Shamir’s secret sharing[7] technique. Amore detailed explanation of these cryptographic primitives is given in section 3. Then we test the performance of these ideas in a practical environment developed using the.Net framework.
 Related works
 2.1 Authentication in P2P
 Absence of a central server makes authentication in peer to peer (P2P) networks complex.
 Traditional cryptographic principles like Public Key Infrastructure (PKI) or Identity
 based Public Key Certificates (ID-PKC) are based on a trusted third party. Establishing
 a trusted third party in a semi-trusted network like P2P is a questionable task. Many
 P2P networks propose trust and reputation management schemes to solve this problem.[8]
 ,[9], [10] use trust and reputation schemes to discover peers that can be considered as
 trusted peers of the network. These trusted peers are used in authentication as trusted
 third parties.
 The idea of reputation management systems is to evaluate a peer’s trustworthiness
 based on its interactions with other peers. There exist plenty of research in this area;
 EigenTurst [11], NICE[12], Regret [13], PeerTurst[14], FuzzyTrust [15].
 P2P systems that use reputation managements schemes to assist in authentication
 suffer from an obvious flow. These schemes assume that the reputation system is intelligent
 enough not to select malicious users as trusted peers. Trusting malicious peers to protect
 sensitive information can harm the system. For example, CST [? ] elect a set of peers as
 RPs (Reputed Peers) using EigenTrust. CST creates a pseudo-identity to hide the real
 identity of the user. The link between the identity and the pseudo-identity is broken into
 parts and stored in randomly selected RPs to protect users’ privacy. CST trusts RPs to
 protect the users’ identity. However, EigenTrust is vulnerable to collaborative attacks
 and therefore there exist a possibility that malicious peers are elected as RPs. Malicious
 RPs can reveal the identity of a user and exploit their privacy.
 Some researches suggest using a modified PKI for authentication in P2P networks
 [16], [17]. Rather than having a single centralized authority, the responsibility of the
 Certificate Authority (CA) is distributed across multiple peers in the network. This
 improves the scalability and robustness of the authentication process.
 The downside of using PKI in P2P is certificate management becomes complex. The
 authentication process becomes difficult to implement effectively. [17] use a set of peers as
 Authentication Servers (ASs). Even though this improves the scalability of the network,
 introduce new security risks like unreliability in certificate access and verification.
 To solve the problem of the absence of a centralized authority and at the same
 time keep the authentication process reliable, modern authentication schemes utilize
 blockchain technology. There is a lot of literature that proposes the idea of using
 blockchain technology to create a Distributed PKI [18], [19], [20], [21]. This seems to be a
 good solution to overcome the limitations of having a central trusted certificate authority.
 Blockchain can make the process of a CA distributed, immutable and transparent.
 Therefore can successfully solve the problems of malicious CAs, MITM attacks and single
 point of failure. Blockchain is used as a distributed key-value data storage. The data
 is public and readable to everyone.[22] propose the idea of using smart contracts to
 certificate management.
 The DPKI is only secure as long as honest nodes control collectively more than 51%
 of computing power. Also some argues the need of blockchain to decentralized PKI since
 the technology of blockchain is still new to the industry.
 The PGP Web of Trust [23] is another way to navigate the problem of not having a
 trusted central authority. WoT distribute the responsibility of a CA among users. The
 core concept of WoT is trust chains. For a simpler explanation, assume A wants to
 authenticate himself to B. There is a user C who trusts B. C can sign A’s certificate
 after verifying its authenticity. Then A can send the signed certificate to B. Since C
 has signed A’s certificate and B trusts C. B can trust A’s certificate is authentic. Using
 indirect trust chains WoT creates a community of trusted users. However WoT is not
 suitable P2P networks since, it is difficult for a new peer to join the network without
 personally knowing a existing user of the network.
 2.2 Anonymous Authentication in P2P
 The concept of anonymous authentication has been around for sometime. Pseudo
 Trust[24] has been one of the more popular publications of this topic. Pseudo Trust
 (PT) utilize the concept of double pseudonyms combine with zero knowledge proofs to
 authenticate users anonymously. PT also uses onion routing[2] and EigenTrust[11] trust
 management to provide a complete file delivery system with anonymous authentication.
 The anonymity comes from the one way property of the cryptographic hash functions. PT
 neglect one important feature of using the concept of pseudonyms to obtain anonymity.
 PT does not change PI (pseudo identity) prior to each authentication process. The PT
 protocol requires PIC (certificate of pseudo identity) to be send to the other party to
 start the authentication. Since PIC is same for a user, an eavesdropper can link two
 communication sessions to a specific user.
 [25] proposes an similar authentication scheme to PT for Internet of Vehicles (IoV).
 The only difference is the slight change of the zero knowledge proof and absence of onion
 routing the trust management. This also suffers from the same vulnerabilities as PT.
 [10] present an interesting approach to anonymous authentication. PPAA uses tags
 to obtain anonymity and at the same time link communication sessions. The idea is
 to use IDs of the two parties involved in the communication session create a tag. The
 two parties will not learn any knowledge other than the tag from running the protocol.
 To avoid having the same tag for different communication sessions between the same
 parties PPAA propose to include an event id into the tag design. Therefore only a party
 involved in the communication will be able to link a communication session to a previous
 session with the same party. The PPAA is secure in random oracle model if eXternal
 Diffie-Hellmam (XDH) and q-SDH assumption holds.
 CST[9] uses collaboration signature to authenticate users anonymously. As mentioned
 in section 2.1 CST uses EigenTrust[11] reputation system to select trusted peers (RPs).
 This is not safe in a semi-trusted environment like P2P networks. Other than that CST
 is said to be resilient against impersonate attacks, traceability and collaboration attacks.
 [26] presents a similar method as CST. They use FBST[27][Fair Blind Signatures] to
 present novel authentication scheme that keep the anonymity of honest users. Similar to
 CST this uses a trust management system called SOBIE to elect peers as super peers
 (SPs) and reputed peers (RPs). They are assumed to be trust worthy and play and
 important role in authentication. However, as mentioned previously trust management
 systems are not perfect. Malicious peers can get elected as SPs and RPs and they are
 able to revoke users’ anonymity. Similar to CST, [26] uses the concept of Shamir’s secret
 sharing [7] to reduce the vulnerability of exposed RPs. [7][How to share a secret] present
 a way to break a key and store it in multiple places and recreate the key when required.
 [26] use this technique to break the key (link between ID and pseudo ID) and store it
 among multiple RPs. Therefore even if few RPs got compromise it does not reveal user’s
 identity. Also a user use anonymous multicast to communicate with a SP. This makes it
 impossible for a SP to reveal an identity of a user.
 [28] uses an combination of Merkle’s puzzles[29] and zero knowledge proofs to provide
 anonymous authentication.
 Methodology
 3.1 Cryptographic Primitives
 3.1.1 Zero Knowledge Proof
 Zero knowledge Proof (ZKP) is a protocol that allows a prover to prove the possession of some secret to a verifier without revealing the secret or any information related to the secret. The first idea of ZKP was introduced by Shafi Goldwasser, Silvio Micali, and Charles Rackoff in [30]. Since then many different ZKPs have been published [31], [32], [33], [34]. ZKPs are widely used in cryptography to implement cryptographic protocols due to its privacy, authentication and low complexity.
 A zero knowledge proof consists of a prover and verifier. In a zero knowledge protocol, a prover must prove the knowledge of some secret using an interactive challenge-response scheme. The protocol must not reveal any information regarding the secret other than the knowledge of prover has the secret. A secure zkp must satisfy soundness, completeness and zero knowledge properties.
 There are two types of zero knowledge systems; interactive zero knowledge proofs and non-interactive zero knowledge proofs [35]. Our proposed protocol uses a zkp that utilize quadratic residues in modular arithmetic.
 3.1.2 Ring Signatures
 The notion of ring signature was first introduced in 2001 by Ron Rivest, Adi Shamir and Yael Tauman Kalai in [36]. Ring signatures are used to digitally sign messages on behalf of a group. At the same time, makes it computationally difficult to find the exact signer.
 Ring signatures are designed to provide anonymity to the message signer. The same functionality is provided by group signatures [37]. The only difference in group signature is that it needs an authoritative entity to generate the signature. Therefore that entity can revoke the anonymity of the signer. Ring signatures do not depend on a third party to generate a signature. Ring signatures are spontaneous and provide unconditional anonymity.
 Over the years different ring signature schemes have been published with different features; threshold ring signatures [38], linkable ring signatures[39], revocable ring signatures[40], traceable ring signatures[41].
 Consider a scenario where a group of k entities where each entity has a public key Pi and a corresponding secret key Si. An entity r can generate a ring signature on a message m using (m, P1, . . . , Pk, Sr). Anyone with the knowledge of m, P1, . . . , Pk can verify the ring signature. No one outside the group (without a secret key Si) can generate a valid ring signature for the same group.
 3.1.3 Shamir’s Secret Sharing
 In 1979 Adi Shamir introduced the concept of Shamir’s secret sharing[7][How to Share a Secret]. This allows a secret to be divided into n parts. The secret can be reconstructed with atleast t parts where (1 ≤ t ≤ n). No knowledge about the secret can be learnt with (t-1) parts. The concept is based on polynomial interpolation. The idea is to generate a polynomial f(x) of (t-1) points. First we select (t-1) random positive integers such that (a1, a2, .., at−1). Then set a0 to the secret we want to share. These points are used to generate the polynomial f(x). f(x) = a0 + a1x + a2x2 + … + at−1xt−1 Then we get n points (xi , yi) corresponding to the polynomial. Given any subset of t points a0 can be found by lagrange basis interpolation.
 li =
 {x − x0} {xi − x0} </box> /times \frac{x − x1} {xi − x1} \times ... \times \frac{x − xt−1} {xi − xt−1}
 f(x) = X t−1 i=0 yili(x)
 The idea of Shamir’s secret sharing is a popular concept in p2p systems. A p2p network does not have an centralized database to store peers’ keys. Storing keys in a selected set of peers might not be a good idea since p2p is a semi-trusted environment. For an example when a peer request a key from another peer, he might not respond. Therefore keys need to be broken into parts and distributed among multiple peers. A peer should be able to reconstruct a key without the knowledge of all the parts. [9], [26] are p2p anonymous authentication mechanisms that use the concept of Shamir’s secret sharing.
 3.2 Conceptual design
 3.2.1 P2P network design
 Using the .Net framework we implemented a hybrid P2P network[42]. A traditional hybrid peer to peer network consists of peers and super peers. Hybrid P2P systems is a combination of purely distributed P2P systems and mediated P2P systems. Hybrid systems are designed to overcome the problems of the two mentioned systems. These systems provide search efficiency of mediated P2P systems while maintaining the reliability of decentralization similar to pure P2P systems[43].
 Our P2P network consists of three types of entities; the main server, ordinary peers (hereafter mentioned as peers) and super peers. A peer communicates with the main server only at the time of registration. Users join the network as peers. Peers are ordinary service requestors. They are connected to the system through super peers. Every peer is assumed to be behind a NAT environment. Peers with public IP addresses and higher computational power are promoted to be super peers.
 Super peers have more responsibility for the system. A super peer is connected to one or more other super peers in the network and responsible for one or more peers. They can communicate among other super peers using the super network. Super peers can join or leave the network at any time. Dynamic behaviour of super peers should not affect the connectivity of the network. Our design of the network is able to change the topology according to this dynamic behaviour of peers and maintain connectivity among existing super peers.
 A super peer is only responsible for nodes under his scope and does not know any information regarding other peers of the system. Therefore a node discovery process becomes an exhaustive task. This can be accomplished in two ways; flooding search and random walk. We utilize flooding search in this project since the random walk is not guaranteed to produce results[44].
 3.2.2 Distributed Certificate Management
 The decentralized environment of the P2P network does not allow traditional methods of authentication. It’s difficult to maintain a centralized database of certificates where the availability of peers cannot be predicted. Distributing certificates among super peers is not a viable solution since super peers are not always available. Therefore all the certificates under this super peer remains not accessible. Also, malicious super peers might delete certificates from the network. The obvious solution is to keep multiple copies of the certificates. We propose a different solution by using Shamir’s secret sharing algorithm. The idea is to break the certificates into multiple parts and distribute across the P2P network. When needed, the certificates can be reconstructed from a minimal subset of the parts. A more detailed explanation of the implementation is given in section 3.3.1 .
 3.2.3 Proposed Authentication Schemes
 Ring Signature Based approach
 The characteristics of ring signatures make it an interesting primitive in obtaining anonymous authentication. Ring signatures allows a message to be signed by a group of public keys. Making it impossible to identify the exact signer. The original ring signature scheme[36] and most of the proposed ring signatures provide complete anonymity. This is not suitable for authentication. This make it impossible to revoke the anonymity of malicious peers. Therefore we used the revocable ring signature scheme proposed in [40] to create a simple authentication protocol that protect users’ privacy. This is just a simple suggestion, of a way to obtain anonymous authentication using existing ring signature schemes. The idea is to challenge prover to generate a ring signature using a random nonce generated by a verifier. If the prover is able to accomplish this he can successfully authenticate himself.
 Authenticated key sharing based approach
 We propose a novel authentication mechanism that allows a peer to authenticate without revealing their identity. The basic idea of the protocol is to present prover a set of public keys and challenge to prove the knowledge of atleast one secret key corresponding to a public key from the set. This idea is simple but the protocol should not reveal any information related to the prover’s identity. Also a prover without a valid key pair should not be able to authenticate himself. To accomplish that we employ a authenticated key sharing scheme introduced in [45].
 Zero Knowledge Proof Based Approach
 Zkp is a popular approach to obtain anonymous authentication in p2p networks. This technique has been utilized in [24], [25] and [10]. Many of these approaches relies on pseudonyms to hide the identity. We propose a new authentication protocol that uses zero knowledge proofs to hide the identity among a group of users. The protocol achieves properties similar to ring signatures. This is an modification of the Schnorr’s zero knowledge proof [46]. The method is similar to the authenticated key sharing based approach in the sense that the challenge is to prove the knowledge of a secret key in a set of public keys. However unlike previous method, we use zero knowledge proofs to do that. Therefore this method achieve k anonymity.
 3.3 Methodological approach
 3.3.1 Distributed Certificate Management
 During the initial interaction of a peer, the corresponding super peer obtains the peer’s certificate. The super peer breaks the certificate into n parts using Shamir’s algorithm. The super peer then floods these parts across the network. Once a request to recreate the certificate(s) received. Super peer again floods a request(s) to collect the parts of the certificate. The super peers that are holding these parts will send them to the corresponding super peers. The original certificate can be recreated as long as r parts are received by the super peer (r ≤ n).
 This technique allows distributing certificates in a more dynamic way. As long as r super peers can be accessed, the certificate can be recreated. This method only requires minimal storage. That is, the size of a single part does not exceed the size of the certificate. This is also the more flexible approach. n and r can be changed for each certificate without affecting other certificates. However, then there needs to be a way to identify n and r for each certificate.
 n and r are performance metrics. Increasing n will increase the average key storage size in super peers. In section 6 we analyze the performance of increasing r while n is kept as a constant.
 3.3.2 Proposed Authentication Schemes
 Ring Signature Based approach
 The protocol starts by the prover collecting a set of certificates from the super peer. Prover then randomly select a subset of the certificates. Then he verify the authenticity of the certificates and obtains the set of public keys from the subset of certificates using main server’s public key. Prover hides his own certificate among this subset of certificates and send them to the verifier to initiate the authentication. After authenticating the certificates, verifier obtains the set of public keys using main server’s public key. Then verifier generates a random nonce and challenge prover to generate a ring signature for this random nonce, using the above set of public keys. Prover use his secret key, the set of public keys and main server’s public key to generate a ring signature according to the algorithm proposed in [40]. Prover then sends the ring signature to the verifier. Verifier verifies the authenticity of the ring signature according to the random nonce he sent at the previous step. If the verification is successful, authentication is complete. Otherwise verifier sends a fail message. Authenticated key sharing based approach As same as the previous approach prover collects a set of certificates from the super peer. Then randomly select a subset out of them. After verifying the authenticity of the certificates prover extract the corresponding public keys. Prover mix his certificate into the subset of certificates and send them to the verifier. Verifier obtains the public keys after verifying the authenticity of the certificates. Then generate X = gx by selecting a random x. Then use the set of public keys to encrypt X. Thus creating a set of ciphertexts where each corresponds to a different public key from the set. Since one of the public key is prover’s, he will be able decrypt X with his secret key. After decrypting X, prover selects a random y and calculates Y = gy . Then generate K = Xy . K is the shared key. Then he sends Y to the verifier encrypted with verifier’s public key. Verifier decrypts Y. Then compute K = Yx . At this stage both parties have the same shared key K. Verifier encrypts a random number R using a symmetric key encryption scheme using K as the key. Then challenge prover to decrypt this and send R back. If the prover generated the correct K at the previous steps, he will be able to decrypt R. Therefore prover can successfully authenticate himself. Otherwise verifier sends a fail message.
 Authenticated key sharing based approach
 As same as the previous approach prover collects a set of certificates from the super peer. Then randomly select a subset out of them. After verifying the authenticity of the certificates prover extract the corresponding public keys. Prover mix his certificate into the subset of certificates and send them to the verifier. Verifier obtains the public keys after verifying the authenticity of the certificates. Then generate X = gx by selecting a random x. Then use the set of public keys to encrypt X. Thus creating a set of ciphertexts where each corresponds to a different public key from the set. Since one of the public key is prover’s, he will be able decrypt X with his secret key. After decrypting X, prover selects a random y and calculates Y = gy . Then generate K = Xy . K is the shared key. Then he sends Y to the verifier encrypted with verifier’s public key. Verifier decrypts Y. Then compute K = Yx . At this stage both parties have the same shared key K. Verifier encrypts a random number R using a symmetric key encryption scheme using K as the key. Then challenge prover to decrypt this and send R back. If the prover generated the correct K at the previous steps, he will be able to decrypt R. Therefore prover can successfully authenticate himself. Otherwise verifier sends a fail message.
 Zero Knowledge Proof Based Approach
 As same as the above two methods prover collects k certificates from the super peer. Then randomly select n-1 certificates and create C and P vectors as the above methods. However in this method public key is Au = gap where au is the private key. Similar to Schnorr’s protocol prover generates U. The difference is U contains factors of Avii where vi is a random number. This is generated only using the collected public keys (Prover’s public key is not in U). Prover then send U to the verifier. Verifier sends a challenge c to the prover. Prover xor all elements of vi with c to obtain vp. Then mix vp among the set of vi s and send them along with the set of public keys (including prover’s public key) to the verifier. Prover also sends r which is s − apvpmodp. Then prover does two steps of verification. First he xor vi s and check if it’s equal to c. If it is not terminate the authentication. Otherwise generate U′ using r, A and vi s. If U = U′ authentication is successful. Otherwise sends a fail message to the prover. A more detailed explanation is given in section 4.1.3 .
 Experiment Setup and Implementation
 4.1 Proposed Schemes
 4.1.1 Ring Signature Based approach
 Registration
 A user has an ID which can be anything related to the identity of the user. Selects
 a random number ru. Then generate a public key Pu such that
 Pu = H1(ID, ru)
 User then generates the private key Su corresponding to Pu
 User sends the registration request along with his ID, Pu to the main server.
 Main server verifies the identity of the user. Then the server signs Pu with his
 private key Ss to generate Certu. Then sends Certu to the user.
 Authentication
 Prover collects k certificates from the super peer. Then randomly selects n-1
 certificates from the the set. After verifying the authenticity of the selected
 certificates prover generates C = {Cert1, Cert2, .., Certn} which includes prover’s
 certificate Certp as well. Prover then obtain each corresponding public key from the
 certificates to generate P = {P1, P2, .., Pn}. Then send C to the verifier, encrypted
 with verifier’s public key Pv.
 Verifier decrypts the message to obtain C. After verifying the authenticity of each
 Certi
 , verifier generates each Pi using main servers public key Ps. Then generate
 H = Hash(P). Then sends H and a random nonce N to the prover.
 Prover generate H′ = Hash(P) and if H ̸= H′
 terminate the authentication.
 Otherwise use his secret key Sp, P and Ps to sign N and generate ring signature
 σ using [40] ring signature scheme. Then send σ to the verifier, encrypted with
 verifier’s public key Pv.
 Verifier decrypts the message to obtain σ. Then verify whether σ corresponds to N
 using P set of public keys (obtained in step 2). If the verification is success prover
 is successfully authenticated. Otherwise verifier sends a fail message.
 4.1.2 Authenticated key sharing based approach
 Registration
 A user has an ID which can be anything related to the identity of the user. Selects
 a public ru. Then generate
 Pu = H1(ID, ru)
 Pu is the public key of the user. User then generates the private key Su corresponding
 to Pu
 User sends the registration request along with his ID, Pu to the main server.
 Main server verifies the identity of the user. Then the server signs Pu with his
 private key Ss to generate Certu. Then sends Certu to the user.
 Authentication
 Prover collects k certificates from the super peer. Then randomly selects n-1
 certificates from the the set. After verifying the authenticity of the selected
 certificates prover generates C = {Cert1, Cert2, .., Certn} | C includes Certp as
 well. Then send C to the verifier, encrypted with verifier’s public key (Pv).
 Verifier decrypts P using his secret key (Sv). Generate H = Hash(P). Then
 generate a random number x and obtain X = gx. Then generate n ciphertexts
 CT = {C1, C2, …, Cn}|Ci = EPi
 (X|H). Verifier sends CT to the prover.
 Prover selects the Ci corresponding to his public key. Decrypt it using his secret
 key (Sp) to obtain X and H. Generate H′ = Hash(P). Check if H = H′
 . If not
 terminate the session. Otherwise select a random number y to generate Y = gy
 .
 Then compute K = Xy
 . Prover sends Y back to the verifier encrypted with Pv.
 Verifier decrypts Y. Compute K = Yx
 . Then generate another random number R,
 generate E1K(R). E1(.) is a symmetric key encryption scheme. Then generate
 H1 = Hash(R|K). Then send E1k(R) and H1 to the prover.
 Prover decrypts the message with his knowledge of K to obtain R. Then use R and
 his K to generate H1′ = Hash(R|K). If H1 = H1′ , prover sends R back to the
 verifier. Otherwise terminate the authentication session.
 Authentication is successful if the verifier obtains the same R. If not verifier sends
 a fail message to the prover.
 4.1.3 Zero Knowledge Proof Based Approach
 Setup
 P and Q are two large prime number where P-1 |Q.gisageneratorof acyclicgroupofZ∗p
 where order of the group is Q. P, Q and g are group parameters.
 Registration
 A user has an ID which can be anything related to the identity of the user. Selects
 a random integer ru. Then generate au
 au = H1(ID, ru)
 such that au is from [0, Q-1]. au is the private key of the user. Then to generate
 the public key Au user calculates
 Au = gaumodp
 User sends the registration request along with his ID, Au to the main server.
 Main server verifies the identity of the user. Then the server signs Au with his
 private key Ks to generate Certu. Then sends Certu to the user.
 Authentication
 Prover collects k certificates from the super peer. Then randomly selects n-1
 certificates from the the set. After verifying the authenticity of the selected certificates prover generates C = {Cert1, Cert2, .., Certn−1}. Prover then obtain each
 corresponding public key from the certificates to generate P = {A1, A2, . . . An−1}.
 Prover then selects a random number s from the range [0, Q-1]. Then selects another
 n-1 random numbers from the range [0, Q-1] to generate the V = {v1, v2, . . . , vn−1}.
 Prover calculates
 U = gsAv1Av2
 . . . Avn−1
 Prover sends U to the verifier to initiate the authentication.
 Verifier selects a random number c from the range [0, Q-1] and sends it to the
 prover.
 Prover calculates
 vp = v1 ⊕ v2 ⊕ . . . vn−1 ⊕ c
 Then insert vp to the vector V such that V = {v1, . . . vp, . . . vn−1}. Prover also
 update C = {Cert1, …, Certp, …, Certn−1} where Certp is prover’s certificate. Then
 calculates
 r = s − apvpmodp
 Prover sends r, V, C to the verifier.
 After verifying the authenticity of the certificates in C. Verifier calculates
 c′ = v1 ⊕ v2 ⊕ . . . ⊕ vn
 If c ̸= c′
 , terminate the authentication session. Otherwise calculates
 U′ = grAv1Av2. . . Avn
 If U = U′
 , authentication is successful. Otherwise terminate the authentication.
 4.2 Testing
 Testing of the system was done to understand the capabilities of the system. The testing was done cloud servers located in different countries. Intention was to mimic a world wide distributed network. Although a simulation environment would be ideal to do load testing on the system, absence of open-source platforms to simulate network environments which could run c sharp scripts was a problem. However, a real-world environment helps to understand the system performance in its operating environment. The tests were done to find the limitations of key sharing mechanism and to compare the performance of the three authentication protocols in a real environment. The first test was done to understand the performance of the key sharing mechanism.
 4.2.1 Performance of key sharing
 Key sharing technique is an integral part of our implementation. The number of parts the key can be broken into (n) and the number of parts required to reconstruct a certificate (r) decides the availability of certificates. A high n value and low r value obtains a higher availability. Since distributing the parts of the certificates happens only once, in this experiment we measure the latency of the certificate reconstruction. Specifically, the experiment was done to identify how the latency of a successful certificate reconstruction varies with increasing n and r. The experiment was done by setting n = r. That is all parts of the certificate are required to reconstruct the certificate. First we break a randomly created certificate into n parts and distribute across the P2P network. Then we floods a SEARCH message requesting the parts of the certificate. The time was measured from the time of flooding the SEARCH message until the successful reconstruction of the certificate. We started with breaking the certificate into 2 parts and at each step we increased n by 2. We continued the experiment until n reached 20. For each n the experiment was done three times and we measured the average time. We also removed any outliers that could affect the results. Due to limited resources, we used only four publicly available servers each in a different country. The selected servers were located in Singapore, India, America and France. Multiple super-node instances were created at each server and super-nodes 20 were connected so that no two neighbour-nodes reside in the same country. This is to intentionally increase the latency of communication.
 4.2.2 Performance of authentication protocols
 The anonymity of the authentication protocols depends on the number of certificates.
 Higher the number of certificates used in the protocol higher the anonymity of the prover.
 Therefore it is important that a protocol can handle a higher number of certificates. This
 experiment was done to measure the latency of a complete successful authentication
 session between a prover and a verifier. At each step, we increased the number of
 certificates used in the protocol to measure how the latency varies. The experiment was
 done for the three proposed protocols in the hope to compare the performance. The time
 was measured from moment that the prover received the requested keys and successful
 finish the authentication protocol at the verifier’s side. The experiment was started using
 only 10 keys and at each step, we increased the number of keys by 10. The experiment
 was carried out until 200 keys are used in the authentication protocols. Similar to the
 previous test, the experiment was done three times and we measured the average time.
 We also removed any outliers that could affect the results.
 Due to limited resources, we used only four publicly available servers each in a
 different country. The selected servers were located in Singapore, India, America and
 France. Multiple super-node instances were created at each server and super-nodes
 were connected so that no two neighbour-nodes reside in the same country. This is to
 intentionally increase the latency of communication.
 Results and Analysis
 5.1 Proofs of security
 5.1.1 Ring Signature Based approach
 The security of the protocol depends on the security of the ring
 signature scheme[40]. The authors have proven the correctness,
 revocation correctness, unforgeability and signer anonymity of the
 signature scheme. They directly corresponds to the anonymity,
 completeness, soundness of our suggested protocol.
 Anonymity
 Anonymity of the protocol depends on the properties of the ring
 signature scheme. The scheme proves it obtains signer anonymity. The
 proposed protocol does not reveal any information other than the set of
 public keys P. The only information verifier can deduce is prover’s
 public key Pp is among the set P. Therefore this obtains k
 anonymity.
 Completeness
 If a protocol has completeness, the protocol is said to be
 comprehensive; an honest verifier will always be able to authenticate
 himself.
 The completeness of the protocol comes from the correctness of the ring
 signature scheme[40]. The authors of the paper have mathematically
 proven the correctness of the ring signature scheme. Therefore our
 protocol is complete.
 soundness
 If a protocol has soundness property, the protocol is said to be
 truthful; a cheating prover will never be able to authenticate himself.
 Since the ring signature scheme has proven it’s unforgeability, a
 cheating prover will not be able to forge a ring signature. The proposed
 protocol obtains soundness.
 Impersonation
 Impersonation is when a malicious user (M) impersonates another user. A
 protocol that accomplish soundness and completeness is secure against
 impersonation attacks. Therefore this protocol is secure against
 impersonation.
 Replay Attacks
 A replay attack is when an adversary saves a previously sent message(s)
 and replay it later to gain an advantage. Let’s assume a scenario where
 a malicious user (hereafter mentioned as M) is eavesdropping on a
 authentication session. M can save message in step 1 (Msg1) and message
 in step 3 (Msg3), replay it later in the hope to authenticate himself.
 Msg1 is encrypted. Therefore M will not be able to reveal it’s content.
 When Msg1 is replayed, verifier will respond with a random N and H.
 Without the knowledge of P or C prover will not be able to generate the
 correct ring signature. Therefore will not be able to authenticate
 himself. Replaying Msg3 will not gain anything unless verifier generates
 the same N as the original authentication. Probability of this scenario
 is 1/N, which can be reduced by increasing the domain of N.
 5.1.2 Authenticated key sharing based approach
 Anonymity
 The protocol hides the identity of the prover among a group of selected
 peers. The group is selected by the prover at random. Therefore verifier
 cannot manipulate P to obtain a knowledge about the prover.
 A cheating verifier may use different x values to obtain prover’s
 identity. Verifier will generate a set of x = {x1, x2, …, xn} and
 generate X = {X1, X2, …, Xn} | Xi = gxi. Then
 verifier can generate CT = {C1, C2, …, Cn} |
 Ci = Epi(Xi | H). By doing so, verifier hope to identify
 which Ci prover was able to decrypt. Then verifier can link that
 Ci to corresponding Pi to reveal provers identity.
 However, this will not allow verifier to reveal prover’s identity since
 at step 4 verifier needs to generate K without the knowledge of exact X
 the verifier received. Therefore will not reveal any information about
 the prover unless verifier can successfully guess the Xi prover
 decrypted. Successfully random guessing Xi has a probability of 1/n.
 Another possibility is using the above method and generating a vector of
 K = {K1, K2, …, Kn} where each Ki correspond to a different
 xi. Then at step 4 select a random Kv and send E1kv(R). By
 this verifier hopes to find which Ki the prover generated. This can
 be done by replicating the decryption process using the elements of K
 vector. Then check what Ki generate a similar output. However this is
 not possible due to H1 hash. Since this must include the correct key,
 prover will know the malicious intentions of the verifier and terminate
 the authentication process.
 This methods does not provide k anonymity. Since prover always terminate
 the authentication whenever the protocol was not correctly followed,
 verifier can use this knowledge to reduce the scope of prover’s
 identity. For an example, verifier generate CT as half of the Ci are
 incorrectly formed and other half is correctly formed. If the prover
 terminate the authentication process, prover’s public key is one of the
 misformed public keys. If the prover continues the authentication
 process, prover’s public key is one of the correctly formed public keys.
 Completeness
 If the prover indeed has a secret key corresponding to any one of the
 public keys in set P, prover can successfully decrypt X. Therefore can
 obtain the correct key (k) for step 5.
 K’ = Xy
 K’ = (gx)y
 K’ = (gy)x
 K’ = K
 Since prover generate the correct key (K). He can successfully decrypt
 R. Therefore can successfully authenticate himself.
 Soundness
 A cheating prover does not have a secret key corresponding to any of the
 public keys in P. To authenticate himself as a member he has to
 correctly guess X at step 3 or correctly guess R at step 5. Both it is
 statistically impossible since X and R are generated randomly by the
 verifier for each communication session.
 Therefore unless prover can obtain a secret key and a corresponding
 public key from a another registered user, it is not possible to
 authenticate himself.
 Impersonation
 Since protocol accomplish both soundness and completeness, this protocol
 is secure against impersonation attacks.
 Replay Attacks
 A replay attack is when an adversary saves a previously sent message(s)
 and uses it again to gain an advantage. Let’s assume a scenario where a
 malicious user (hereafter mentioned as M) is eavesdropping on a
 communication session. M can save message in step 1 (Msg1) , message in
 step 3 (Msg3) and/or message in step 5 (Msg5), replay it later in the
 hope to authenticate himself.
 If Msg1 was replayed this will not gain any advantage for M. Since M
 does not know any secret key corresponding to the set P, he will not be
 able to authenticate unless by random guessing X or R in step 3 and step 5.
 Storing Msg3 will not help since without the knowledge of y, M will
 not able to generate K. Only possibility of succeeding in a replay
 attack is if the verifier generate the same R as the original
 authentication. Then M can replay Msg5 to successfully authenticate
 himself as a valid prover.
 5.1.3 Zero Knowledge Proof Based Approach
 Anonymity
 The only information the protocol reveals is that the prover has the
 knowledge of an ap. Protocol hides the Ap (public key) corresponds
 to that ap among the set of P public keys. Identifying the exact
 public key of the prover is not feasible. Therefore the protocol obtains
 k anonymity.
 Completeness
 If the prover possesses the correct ap; the secret key corresponding
 to Ap, only then the prover will be able to generate r such that the
 U generated by the verifier will be equal to the U received to the
 verifier at step 1.
 U’ = gr Av1 … Avp … Avn-1
 U’ = g(s - apvp) Av1 … Avp … Avn-1
 U’ = gs g-apvp Av1 … (gap)vp … Avn-1
 U’ = gs g-apvp … gapvp … Avn-1
 U’ = gs Av1 … Avn-1
 U’ = U
 Soundness
 Let’s consider a cheating prover as a prover who does not possess a
 private key ap corresponding to a public key Ap.
 Without a ap a prover will not be able to generate
 r = s - apvp mod p.
 At step 3, prover is required to generate vp by xoring elements of V
 with the challenge c. This operation ensures that xoring elements in V
 vector (including vp) at the verifier’s side would generate c.
 Therefore to pass the first step of verification V must be well formed.
 Without the knowledge of the valid ap a prover will not be able to
 generate r to cancel out the gapvp component at the last step of
 the verification.
 The only possibility is random guessing. The probability of guessing
 $a_p$ without any information is 1/Q. Since Q is selected to be a large
 prime number, probability of that happening is statistically
 insignificant.
 Impersonation
 As we explained previously a protocol that accomplish soundness and
 completeness is secure against impersonation attacks. Therefore this
 protocol is secure against impersonation.
 Replay Attacks
 Let’s assume a scenario where a malicious user (hereafter mentioned as
 M) is eavesdropping on a communication session. M can save Msg1 at step
 1 and Msg3 at step 3, and replay the messages later in the hope to
 authenticate himself.
 When M replays Msg1 verifier will respond with a random challenge.
 Without the knowledge of s, ap, V and P vectors M will not able to
 continue further. Therefore only replaying Msg1 will not be successful.
 Replaying Msg3 as the response for the challenge will cause the first
 step of the verification to fail. Since c is chosen randomly by the
 verifier, the old vp will not correspond to the new c. Therefore
 xoring elements of V will not be equal to c and verifier will terminate
 the authentication process. This will only be successful if the same c
 is chosen at the two authentication processes. The probability of this
 happening is 1/Q. As mentioned in previous cases this is statistically
 insignificant.
 Modifying the Msg3 will not gain any advantage to M. As mentioned under
 soundness proof, without a valid ap authenticating will be
 infeasible.
 5.2 Performance Testing
 5.2.1 Performance of key sharing
 The test was carried out by increasing the number of parts the key is broken into and
 measuring the latency of successful key reconstruction.
 Number of parts per key
 Average Latency (ms)
 2
 540.26
 4
 559.08
 6
 591.55
 8
 712.93
 10
 1012.48
 12
 1120.95
 14
 1323.59
 16
 1718.33
 18
 1404.40
 20
 1404.40
 Table 5.1 Latency of the key reconstruction with increasing number of parts per key
 5.2.2 Performance of authentication protocols
 The test was carried out by increasing the number of keys used in the authentication
 process and measuring the latency of a successful authentication process. The test was
 carried out separately for the three authentication protocols in similar environments.
 Number of parts per key
 Latency in Authenticated key sharing based approach (ms)
 Latency in Zero knowledge proof based approach (ms)
 Latency in Ring signature based approach (ms)
 10
 82.00
 58.33
 98.67
 20
 87.00
 56.33
 107.00
 30
 88.00
 62.00
 104.00
 40
 89.33
 80.00
 106.67
 50
 99.67
 58.00
 114.67
 60
 99.67
 56.33
 109.33
 70
 105.00
 58.67
 119.67
 80
 111.33
 68.33
 121.33
 90
 112.67
 63.67
 127.33
 100
 116.00
 57.67
 120.67
 110
 105.33
 59.33
 124.33
 120
 115.00
 64.33
 127.33
 130
 119.67
 55.67
 142.67
 140
 124.67
 58.67
 144.33
 150
 127.33
 56.33
 144.67
 160
 113.33
 67.33
 138.67
 170
 134.33
 58.67
 140.00
 180
 128.00
 58.33
 144.00
 190
 125.67
 56.00
 142.00
 200
 133.00
 63.33
 144.00
 Table 5.2 Latency of the authentication protocols with increasing number of keys.
 Fig. 5.2 Performance comparison of the three authentication protocols
 5.3 Performance Analysis
 According to Fig. 5.1, the latency of the key reconstruction exponentially increases
 with an increasing number of key parts. The maximum number of possible parts per
 key without increasing a threshold of 5 seconds is 20. The reason for such exponential
 increment is due to the linear distribution of unique keys. That is to request the mth part
 of the key, the request needs to travel through m supernodes. Increasing the number of
 parts per key will increase the overall distance the request message has to travel through.
 Increasing the number of key parts increases the overall latency of the authentication
 process. The higher the parts the key is broken to, the higher the availability of the keys.
 Therefore need to understand the optimal trade-off between the availability of keys and
 the latency of the key reconstruction process.
 It is also important to notice that the network congestion at the time of the experiment,
 the physical location of super-nodes, the performance of the super-nodes can have an
 effect on the results. Nonetheless, the experiment is still essential to get an overall idea
 of the performance of the key reconstruction mechanism.
 Fig 5.2 compare the performance of the three authentication protocols with respect
 to time. The latency of the authenticated key sharing based approach and the ring
 signature-based approach increases with the increasing number of keys. However, the
 latency of the zero-knowledge proof-based approach remains constant. The reason is the
 encryption and decryption steps of the first two protocols. Higher the number of keys,
 the higher the number of encryptions and decryptions the protocol has to done. The
 zero-knowledge proof-based approach has no explicit encryption and decryption steps.
 Thus it is faster and no visible increment in the latency with the increasing number of
 keys compared to the other two approaches.
 Higher the number of keys used in the authentication process higher the anonymity.
 Since the zero-knowledge proof-based approach can increase the number of keys used
 in the protocol without increasing the latency of the authentication process it is more
 efficient compared to the other two protocols.
 Conclusions and Future Works
 We have proposed three novel protocols to achieve anonymous authentication in peer to
 peer networks. The ring signature-based approach provides a suggestion of how to utilized
 already implemented ring signatures to obtain anonymous authentication. Then we propose an
 authentication method that utilizes a key sharing mechanism. This method does
 not provide zero-knowledge. That is a verifier can obtain some knowledge of the prover
 identity. To solve this we introduce a zero-knowledge proof-based approach that utilizes
 Schnorr’s protocol to achieve anonymous authentication. This method is more efficient,
 secure and most importantly achieve zero knowledge. We have proven the security of each
 protocol including anonymity, completeness, soundness, resilience to impersonation and
 resilience to replay attacks. The protocols were tested in a peer to peer overlay network
 build using the .Net framework. The peer to peer network utilizes a distributed certificate
 management mechanism build using Shamir’s secret sharing algorithm. This allows
 us to access certificates in a more efficient manner compared to the traditional approaches.
 As for future works, we hope to modify the zero-knowledge proof-based approach for
 certificate revocation. That is, give an authoritative entity the privilege to revoke the
 anonymity of a user when required. We also hope to integrate the proposed authentication
 protocols in real-world peer to peer transactions.
 Links
 Project Repository
 Project Page
 Department of Computer Engineering
 University of Peradeniya
 Back to top",We propose three authentication protocols that allow a user to anonymously authenticate in a P2P environment. ,E15,Undergraduate Research Projects,https://projects.ce.pdn.ac.lk/4yp/e15/anonymous-authentication/,https://github.com/cepdnaclk/e15-4yp-anonymous-authentication,https://cepdnaclk.github.io/e15-4yp-anonymous-authentication,https://cepdnaclk.github.io/e15-4yp-anonymous-authentication/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/4yp/E15/anonymous-authentication/
96,cricket analysis,"Data Mining System for Selecting a Winning Cricket Team
 Projects - Department of Computer Engineering
 University of Peradeniya, Sri Lanka
 Project Title
 Data Mining System for Selecting a Winning Cricket Team
 Team
 E/15/119, Dinithi Hasanika, dinithiliyanage.95@gmail.com
 E/15/202, Dulanjali Liyanage, preethi.du1995@gmail.com
 E/15/208, Roshani Dilhara, roshanidilhara7@gmail.com
 Supervisors
 Dr. Asitha Bandaranayake, asithab@eng.pdn.ac.lk
 Sampath Deegalla, sampath@eng.pdn.ac.lk
 Table of content
 Abstract
 Related works
 Methodology
 Experiment Setup and Implementation
 Results and Analysis
 Conclusion
 Links
 Abstract
 Cricket is a two-team game which was originated in south-east England and developed globally in the 19th century. This world’s second most popular game is played for a limited number of overs as twenty20 for twenty overs and ODI(One Day International) matches for 50 overs. Test matches are played for five days. Due to the availability of ball-by-ball data of this bat-and-ball game, researchers were able to do statistical analysis of data for pattern recognition, to find factors affecting the game and for outcome prediction of a match. But due to the high uncertainty of the game, it has become very difficult to come up with a stable and accurate model for the predictions. Outcome model also depends on the number of overs, match type, considering time period and players combination. This research focus only the ODI matches and considering only the ICC full members; England, Australia, New Zealand, Sri Lanka, Bangladesh, Pakistan, India, Zimbabwe, West Indies, Afghanistan, South Africa
 and Ireland. This outcome prediction is based on players performances in a team and some features specific to the team and the match. The individual performance of batsmen, bowlers and fielders are analysed separately considering all-time ODI data. Combined performance of batsmen and bowlers were analysed and compared with individual performances using statistical method. Association rule mining method was used to find frequent winning player combinations. Match data from 2015 to 2020 were considered for the combined performance analysis and outcome prediction. For all these predictions we use data mining and machine learning techniques.
 Related works
 1. Player Performance
 Each cricket team is a combination of batsmen, bowlers, fielders, and all-rounders including the wicket-keeper. For the winnability of the team, individual batsmen should score maximum runs, individual bowlers should take the maximum wickets possible while restricting the opponent team from scoring either by runs or offering extras. For the victory, the wicket keeper should also contribute as a batsman, other than playing behind the wickets. All-rounders contribute by scoring runs and taking wickets since they have the ability of both batting and bowling. Fielders should contribute to the team by getting the batsmen out of the game or by limiting the scoring of batsmen.
 Individual players’ performance has a significant impact on the winnability of a cricket team. So, the accurate metrics that affect the winnability of a specific team should be found to evaluate the individual players. The basis of modeling the strength of a team is enhancing the strength of batting and bowling of individual players. Many researchers have analyzed different methods for this player performance evaluation, and the below sections will go through them.
 2. All-Rounder Performance
 The players who show talents in both batting and bowling are called all-rounders in a match. This also can be categorized into two as bowl all-rounders who are more skilled for bowling than batting and batting all-rounders who are more skilled for batting than bowling. It is better to have at least one all-rounder for one team.
 3. Player Ranking and Player Order
 Cricket player ranking is an important point when directing a match towards winning. Since batting order and bowling order matters in winning. Open batsmen and bowlers who bowl the maiden overs and last overs are also considered as important factors that can be influenced to the final outcome of the match. Open batsmen do not need to be the best players but need to be the experienced players in open batting. From positions, 3 to 6 should consist of the best batsmen from better to poor. Most of the time 7th position is for the wicket keeper. Positions 8 to 11 for bowlers.
 P. Premkumar et al. came up with a model to rank batsman and bowlers according to their performance in ODI matches. They considered runs scored by a batsman on a match-by-match basis and took the average for the considered time duration. They considered wickets taken by a bowler when ranking bowlers. Other than that they considered location impact, pitch impact, Batting innings impact, Opposition impact, team impact, and strike rate impact when ranking both the batsmen and bowlers.
 V. Kanungo et al. visualized data of best-performed players in IPL. They ranked players considering four factors; the number of man of the matches, number of centuries recorded batsmen with top strike rates, top 10 players with maximum total runs.
 T. B. Swartz et al. developed a model for the Indian team to find optimal or nearly optimal batting orders to play in ODI matches. It used a simulated annealing approach and developed an algorithm to select the optimal batting team. They considered features like batsmen, the number of wickets lost, the number of balls bowled, the bowler, the opposing team, and the condition of the pitch. This method suggested some batting orders that have never been tried by the Indian team.
 J. M. Norman et al. came up with two models to find the optimal batting order for the Australian team. Model 1 is for limited-overs matches and Model 2 is for unlimited overs matches. They categorized player positions 1 to 3 as the top, 4 to 6 as good, 7 to 9 as average, and 10 to 11 as slogger. When grouping Australian players to the above-mentioned categories, they considered average runs, run rate, and dismissal probability. They built a model to decide when there is a sudden dismissal. They found that it is more advantageous than having a fixed batting order.
 Harsha Perera et al. built up an algorithm using simulated annealing to determine the optimal player for the Twenty20 matches. They created models for India and South Africa. And this model selects two separate player orders as the optimal batting order and optimal bowling order.
 4. Team Performance
 Cricket is played by 2 teams against each other. A team consists of 11 players. But it is not a fixed team. Players in a team can be changed over time and also team may change match by match. In that case, it is important to consider the influence of team performance in the final outcome of a match. The performance of a team is mainly based on each individual player’s performance and some other factors. Researchers have attempted to include evaluations of team performance in predicting the outcome of a cricket match. So the following sections discuss the various methods used by past researches to evaluate team performance.
 5. Team Performance metrics and overall performance
 Better performance of a team is mainly affected by each individual player’s performance in it. These factors impact on predicting the outcome of the match.
 Madan Gopal Jhawar et al. has addressed this situation in their research using ODI match results. They have suggested that the relative team strength is a distinctive feature in predicting the outcome of a cricket match. Since a team can be changed due to various reasons, they have suggested a parameter to express the team performance. That is the relative strength of a team. Means strength of team A against team B. To calculate this they have used two factors as batting and bowling to model each individual player. Then depending on related factors, they have given a score for each of them. Since these scores have different ranges they have been normalized to lie in the same range. Then the batting strength of a team is calculated as the summation of batting scores of all the players in the team. The bowling strength of a team is calculated as the summation of bowling scores of all the players in the team. Depending on these scores, the strength of team A against team B is calculated. Where A and B are the two opposing teams playing the match. This calculation is used as one of the attributes to predict the outcome of the match.
 N. Siva et al. has proposed a method to evaluate a team based on their past data, without considering its players. In this evaluation, they have considered 556 matches played from 2006 to 2017. The analysis has been done only on the Sri Lankan cricket team. To evaluate the team performance they have considered ten attributes with categorical values. Those attributes are city, venue, match type, outcomes, number of overs, player of the match, opposition, toss winner, toss decision, and winner. Using these attributes they have come up with a classification based model to predict the outcome of a match. They have used machine learning algorithms in predicting the outcome of a match using the overall performance of a team.
 N. Pathak et al. has used the factors analyzed by A. Bandulasiri in creating a model for each team to predict the outcome of an ODI cricket match from the time duration of 2001 to 2015. Those factors are Toss outcome, Day/Night effect, Home game advantage, and bat first. In this analysis, they have not considered individual player performance. The Naïve Bayesian classification technique has shown the best results.
 A. C. Kaluarachchi et al. have selected the attributes team, opponent team, home/away, day/night, toss, bat first and result to predict the outcome of an ODI cricket match. They have considered all the ODI matches from 1971 up to 2010. The outcome was displayed through a software tool called CricAI. Since Bayesian classifiers have shown the best results, it has been used in this tool. This analysis also has not considered the impact of individual player performance on the team performances.
 M. Bailey et al. has considered the factors home ground advantage, past performance, match experience, performance at a specific venue, performance against a specific opposition, experience at the specific venue, and current form, in predicting the ODI match outcome while the match is in progress. They have considered 2200 ODI matches prior to 2005. Using these results they have created models to predict Margin of Victory(MOV) and the team totals. For the prediction of team totals, only 100 ODI matches of 2015 have been used. For the rain-interrupted matches DL method has been used in predicting the team total. Team totals are compared using AAE(Absolute Average Error) between the actual and predicted values. Results are obtained as totals for the team batting first and totals for the team batting second. From that first innings, totals were more accurate and the second innings totals predictions were more accurate as the game reached the end.
 B. Morley et al. has focused on the team’s decision on whether to bat first or second considering factors of home-field effect and winning the toss. They have considered only the English ODI match data during the period 1996 to 1997 league season. 57\% of matches have been won by the home team and 51\% of matches have been won by the team winning the toss. 56\% of matches have been won by the home team where they have won the toss and chooses the batting order. Only 43\% of matches have been won by the away team where they won the toss and decide the batting order. Therefore this suggests that winning the toss and deciding the batting order is of great advantage for the home team. But the effect of winning the toss can be nullified by adding the team quality and match importance and these factors are more important in predicting the outcome of a match.
 S. A. D. P. Subasingha et al. has proposed a novel method in predicting the outcome of an ODI cricket match. They have used two sub-data models in predicting the outcome of the match. One model is used in predicting the outcome of a match, based on pre-match data. And the other model is used in predicting the outcome of the match using the batting partnership of both teams. The first model is team-based. They have considered the attributes toss effect, ground condition, day/night effect, and opponent in the first model. A Naïve Bayes algorithm has been used to predict the final outcome.
 A statistical analysis has been proposed by A. Nimmagadda et al. predicted the result of a Twenty20 match while in progress. In this analysis they have not considered the number of wickets fallen, venue of the match, and the toss at the first phase. Instead of that their prediction of
 scores for a team was
 based on runs scored, and looking at different totals at the end of innings and various run rates. But since considering only run rates did not give good results, they also have estimated the batting and bowling potentials of 22 players using their career statics and active participation in recent games. These player potentials have been used to get the relative dominance one team has over the other. Prediction modeled using Multiple linear regression and has included the batsman increase, bowler, wickets, and run rates as attributes.
 K. Kapadia et al. have used IPL match data to come up with a machine learning model to predict the result of a match, based on historical match data. Based on features related to home ground and toss decision of team, two sets of different models have been obtained. Naïve Bayes has shown 57\% accuracy and 60.5\% precision and Model Trees has a 68.6\% recall rate, when the home team is the winner of the match when considering only the home team feature sets. kNN model has shown 62\% accuracy, 64.2\% precision, 58.4\% recall rate when the toss winning team is the winner of the match when considering only the toss winner feature set.
 P. Somaskandhan et al. has considered IPL matches from 2008 to 2016, 350 matches and 700 innings included to their consideration. They have considered 23 features like total runs scored in an innings, the total number of wickets in an innings, highest individual score in an innings, runs in the power-play of the innings which affect on the team performance of a IPL match.They used feature selection techniques in machine learning to identify
 the
 best set of features which impose significant impact on the end results of the match.
 P.A. Gregory et al. have chosen the IPL match details as their domain since it provided them with a satisfactory amount of data for their analysis in predicting the outcome of a match based on a specific feature set. Their data set was comprised of 501 instances of IPL matches and have used relational mapping framework to store data in a database. Their main component of the analysis was the feature set which includes 14 different features. Namely Number of Wickets Lost, Four Hitting Frequency, Six Hitting Frequency, Boundary Run Percentage, Dot Ball Percentage, Dot Ball to Runs Ratio, Run Rate, Average Partnership Score, Number of Batting Segments, Batting Segment to Wicket Ratio, Average Runs in a Batting Segment, Average Pressure Factor, Pressure of Wickets, Final Score. First they have obtain an optimum subset of attributes for first and second innings of an IPL match separately. For first inning Dot Ball to Runs Ratio, Dot Ball Percentage, Number of Wickets Lost has been chosen with 70.46\% accuracy and for the second inning Pressure of Losing Wickets has been chosen with 88.82\% accuracy. They also have obtained optimum features by dividing innings to three segments as Powerplay, Middle and Death. Then by combining these segments with complete inning, an optimum set of features have been derived with 71.65\% accuracy. The optimum subset of attributes that they have selected for first inning are Four Frequency, Number of Batting Segments, Final Score, Batting Segment to Wicket Ratio (PP), Six Hitting Frequency (PP), Boundary Run Percentage (Middle), Average Runs in Batting Segment (Middle), Average Pressure Factor (Middle), Dot Ball Percentage (Middle), Run Rate (Middle).
 6.Outcome Prediction
 A cricket match has four possible outcomes; a win, loss, draw, tie. But in ODI matches it is impossible to end the match in a draw.
 If the second batting team scores more than the first batting team, the match will be won by the second batting team. In such a case the match may end without completing the overs limit and with some wickets on the hand of the second batting team. When the second batting team is unable to score more than the first batting team, the first batting team is won by some runs. A match is said to be tied when both the teams score the same amount of runs. In either case, the match ends with all the ten batsmen being out or completing the overs limit. In some tournaments when a match is tied with the overs limit, both the teams get a chance of a super over. If the super over is again tied, then the team with the highest number of boundaries wins the match. A number of researches have been conducted to predict the outcome of the cricket match.
 V. V. Sankaranarayanan et al. came up with a 68\% - 70\% accurate data mining approach to predict the future state and predict the winner of an on-going ODI match. Predicting the future state of the game included predicting the number of runs scored for the next segment. They used two separate models to predict home runs and non-home runs. They considered six historical features and five instantaneous features. The considered historical features are average runs scored by the team in an innings, the average number of wickets lost in an innings, frequency of being all-out, average runs conceded in an innings, the average number of opponent wickets taken in an innings, and Frequency of getting opposition all-out. The considered instantaneous features are home or away, powerplay, target, batsmen performance features, and game snapshot (current score and fallen wickets).
 M. Bailey et al. also developed a similar model to the above model to predict the outcome of an on-going ODI match. The factors they considered are a home ground advantage, past performance, match experience, performance at a specific venue, and current form. To predict the match outcome they weighted those factors according to the statistical significance of them. They used the Duckworth-Lewis method to determine resources remaining at the end of each over and used that result to predict the final score of the batting team.
 M. Gopal Jhanwar et al. developed a 71\% accurate model to predict the outcome of an ODI match using a team composition based supervised learning approach. First, it modeled the potentials of batsmen and bowlers in both the teams. The model then predicted the winning team of the match using the player performance, toss decision, venue, and relative team strength. The model also showed that both the historical data of the players and instantaneous data are needed to predict the outcome of a match.
 S. A. D. P. Subasingha et al. came up with a tool to predict the outcome of an ODI match. Their run rate is the only considering factor to predict the final score of the team.
 Neeraj Pathak et al. developed a tool COP (Cricket Outcome Predictor) which outputs the probability of winning an ODI match. The model used instantaneous factors analyzed by Ananda Bandulasiri. Those factors were toss outcome, day/night effect, home game advantage, and bat first. They analyzed ten full member nations of ICC and prepared a separate model for each team considering their opposition teams separately. This tool gives the prediction before the match is started Since factors considered in this approach do not change after the match started. The critical tool developed by A. C. Kaluarachchi et al. to predict the outcome of an ODI match is also similar to the COP tool. They also considered the features analyzed by Ananda Bandulasiri. But they revealed that toss winning does not have a major impact on match outcome. But They found that losing the toss and batting second increases the chance of winning while winning the toss and batting second reduces the chance of winning.
 A. Nimmagadda et al. proposed a model to predict the winner of the Twenty20 match while the match is in progress. The proposed model projects run scored by the batting team considering the current run rate and other different run rates. They considered relative team strength and venue since the run rate is not enough to predict the final outcome.
 7. Sentiment Analysis
 Sentiment analysis is analyzing positive, negative, or neutral mentions within text data using text analysis techniques. Apart from the text analysis, this refers to computational linguistics, natural language processing and systematically identifying, extracting, quantifying, and studying affective states and subjective information. Sentimental analysis is also known as name opinion mining. Emotions of cricket fans change when their home country scores run, taking wickets and when losing wickets. Sentiment analysis can be performed with a number of related keywords to analyze cricket fan’s emotions. This can be done using social media platforms like Twitter, Facebook. This approach used to evaluate the popularity of a team.
 N. Rodrigues et al. used twitter data and distinguished tweets as positive, negative, and neutral and obtained the popularity of each player. They included that as an IPL franchise league player selecting metrics in their model. The same criteria used in the IPL dream team software developed by Jayshree Hajgude et al. for the use of IPL franchise team owners for the selection of players.
 Dinesh Samariya et al. illustrated Indian cricket team fans mood change continuously during the cricket match using the twitter data. They proposed an approach which is a combination of corpus-based and dictionary-based techniques. They visualized emotion changes of Indian cricket team fans in separate graphs when the India team is playing with other selected opposite teams.
 S. Arafin Mahtab et al. analyzed facebook Bangladesh cricket fan group comments in the Bengali language to analyze the fan emotions of the Bangladesh team. They used a machine learning approach and prepared three sentiment classes about Bangladesh cricket as praise, criticism, and sadness.
 P. Lakkaraju et al. used SAS (Sentiment Analysis Studio) to extract textual opinions about cricketers. Using this method they ranked players according to the number of times they mentioned in the considered textual data.
 8. Methodologies
 Analyzing and coming up with the different techniques to model the cricket game made easy by the availability of ball by ball data of matches in the public domain. Some popular sites that provide these data are ESPN CricInfo, Cricsheet, kaggle and Statsguru. These sites are updated for every match. From this data, many types of research have been carried out from time to time and come up with different types of models to evaluate the performances of a player and a team, and predict the outcome of a match. They have used data mining techniques, machine learning techniques, etc. to model the game cricket.
 M. M. Rahman et al. has provided an analysis of Bangladesh ODI cricket data of the time period 2005 to 2015. They have collected all these data records from the ESPN CricInfo website. In this analysis, they have used decision tree algorithm C5.0 to predict the outcome of the match while the game is in progress. 10\% of the data have been randomly selected to predict the result(or the test data) and the remaining data have been used in creating the model.
 V. V. Sankaranarayanan et al. have used the ODI cricket data from January 2011 to July 2012 in their analysis. All these records have been taken from the ESPN CricInfo website. They have used these data in creating a model to predict the future states while the game is in progress. But 20 matches with rain interruption have been removed from the analysis. In creating the model they have used only data mining techniques and some formula. They have not used any machine learning algorithms.
 Madan Gopal Jhawar et al. has proposed a team composition based supervised learning approach to predict the outcome of a match. They also have obtained data of ODI matches from 2010 to 2014 from the ESPN CricInfo website. Since they have restricted their study to the top 9 ODI playing teams some data have been excluded. And they have 109 matches that were interrupted by rain. They have used Binary classifiers like SVM(Support Vector Machine), Random Forest, Logistic Regression, Decision Tree, and kNN(k- Nearest Neighbour) with sweep features and no cross-validation in their modeling. The kNN algorithm has shown the best accuracy.
 N. Pathak et al. have used classification techniques in predicting the outcome of the ODI cricket match. They also have considered ODI data from the ESPN CricInfo website in the time duration of 2001 to 2015. 80\% of data is used for training the models and 20\% to test. Naïve Bayesian, Random Forest, SVM, classifiers used in modeling, and Naïve Bayesian have shown better performance. Kappa statistics and balanced accuracy has been used for better classification performances. Higher values of these have increased the performances of classification.
 A. C. Kaluarachchi et al. has used Bayesian classifiers, Decision Tree Classifiers using C4.5, Bagging, and Boosting to predict the winning team of an ODI cricket match. Since Bayesian classifiers have shown the best results, it was used to develop a software tool called CricAI to output the probability of victory. They have used machine learning techniques like association rule mining, clustering, and classification. They have considered data from 1971 to 2010 of ODI matches obtained from the ESPN CricInfo website.
 P. Somaskandhan et al. has used machine learning techniques to come up with an optimal set of attributes that impose a high impact on the end result of a match. In this analysis, they have used IPL ball by ball data from 2008 to 2016. They have trained Extra-tree, Naïve Bayes, and Support Vector Machine (SVM) machine learning algorithms with 80\% of data, and the remaining 20\% have been used for testing. SVM has shown the best results in their analysis.
 M. Bailey et al. has used a multiple linear regression model to predict the outcome of an ODI match while the game is in progress. Prior to predicting they have numerically weighted the variables according to their statistical significance. The Margin of Victory(MOV) has been obtained by the multiple linear regression model. This model has shown 71\% of accuracy. Then for 100 matches from 2015 ODI, AAE(Absolute Average Error) between actual and predicted MOV of the team batting first and team batting second has been considered. According to the obtained results, first-innings totals are more accurate than the second innings. But the reduction in AAE has increased in the second inning when the game draws nearer to the end.
 A. Bandulasiri predicted the winner of an ODI match using a logistic regression model. They have considered World Cup matches from 1995 to 2007.
 B. Morley et al. also used a logistic regression model to investigate home advantage and other factors affecting the outcomes in English ODI matches. Logistic regression and graphical “classification and regression tree” approach are used by K. P. Jayalath in his analysis of predicting the outcome of an ODI cricket match.
 T. B. Swartz et al. have used a non-machine learning approach called simulated annealing in searching for optimal or nearly optimal batting orders in an ODI match. They have considered only the performance of the Indian cricket team in an ODI match.
 N. Siva et al. have used the data mining and machine learning approach in performance analysis of the Sri Lankan cricket team. They have considered only IPL and Twenty20 match data obtained from Cricsheet, ESPNcricinfo, and online data mining community Kaggle. K. Passi et al. also has used a machine learning approach to predict the player performance in an ODI match. And the data source is ESPNcricinfo. They also have used the WEKA tool.
 S. A. D. P. Subasingha et al. has used data mining techniques to predict the outcome of an ODI cricket match. They have used data from ESPNcricinfo and sites. WEKA tool has been used to build the classifier models.
 P. Shah et al. have used exponentially decaying average (EDMA) to evaluate individual player performances. It is a statistical approach to measure the form of an individual player. In calculating the form, simple logic of short term and long term EDMA has been used. The form of a batsman is depicted as a percentage of the ratio of short term EDMA and long term EDMA.
 Concepts of Multiple Random Forest Regression have been used by N. Rodrigues et al. in predicting a score for a batsman or a bowler in a given match. This has been used to model the ODI match data.
 P. Premkumar et al. proposed a dynamic approach using factor analysis to rank the batsman and bowlers using the data obtained from 2015 ODI matches. In this dynamic model, factor scores are calculated for players using
 Principal Component Analysis techniques. Since performance metrics of players are highly correlated they have obtained only one factor consisting of all the variables.
 P.A. Gregory et al. has used an iterative approach to obtain an optimum set of attributes to which can predict the outcome of IPL cricket matches (501 instances) . Analysis have been carried out using two different approaches. In first approach they have ranked the features using a specific mathematical model. In the second approach accuracies are calculated for different subset of features against a classification algorithm. Feature selection and modeling analysis has been carried out using Filter method. There were three attribute selection algortihms namely CfsSubsetEval (selecting attributes with high ccorrelation and low inter-correlatio), InfoGainAttributeEval (ranking attributes according to information gain and then selecting attibutes) and ReliefFAttributeEval (Selects attributes by repeated sampling). Classification model
 J48 decision tree algorithm with 10-fold cross validation was trained using the above selected subset of features and accuracies were improved by combining various attributes of a given subset of features. Then they have used both wrapper and filter methods with J48 classifier to analyze and obtain the optimum set of features by segmenting innings. Finally WrapperSubsetEval selection algorithm has return the subset with highest accuracy when combining
 segmented innings with complete innings.
 As discussed above there are many types of research that have been carried out by data mining and machine learning approaches in team and player evaluations and predicting the outcome of the match. Although most of them have used the data mining and machine learning approaches the final model to predict the results have shown some differences due to the different attributes used or due to the difference in considered time period and type of match.
 Methodology
 Proposed Methodology
 The main objectives of this research are recognizing patterns of player combinations that result in the match outcome and predicting the match outcome when the players of two teams and match conditions are provided.
 This research is carried under five perspectives.
 Identify features impact on the individual performance
 In this, the features affecting the individual performance of a player is considered. This research considered new features that were not used in earlier researches. This is carried under three divisions as batsmen, bowlers, and fielders. Using machine learning regression models the feature importance was obtained.\par
 Ranking players considering their individual performance
 The ranking of players is carried under three divisions as batsmen, bowlers, and fielders. Wicket-keepers also considered as fielders. All-rounders are considered under all three divisions. Considering the features impact on player’s performance a score is calculated for each player. Based on the calculated score player is ranked.
 Identify the combined effect of players
 Not only the individual performance but also the combined effect of players also impacts the match outcome. This approach is a statistical approach to find beautiful combinations of players where their combined effect is better than the combination of their individual performances. This approach considers batsmen and bowlers separately. This considers n-grams of players like 2-grams, 3-grams.
 Identify frequent player combinations
 This is an association rule mining approach to find frequent combinations, frequent winning combinations in ODI matches. The combinations obtained by this approach is further compared with the beautiful combinations obtained by considering combined effect of players.
 Predict the match outcome
 This approach is to build a machine learning model which predicts the outcome of a given match. This model predicts the outcome of a given ODI cricket match under following conditions.
 Two teams should be one of these countries: England, Australia, New Zealand, Sri Lanka, Bangladesh, Pakistan, India, Zimbabwe, West Indies, Afghanistan
 and Ireland
 The team combination should be given.
 Toss won team should be given.
 Ground should be given.
 Match time should be given as day or day-night match or night match.
 In this approach we used the results obtained from the above 1st and 2nd perspectives.
 Data Collection
 In the first phase of our project we used the career details of batsman and bowlers. And in this phase, second phase, we collected the fielders data, and all the runs scored by all players in each match. So do all these we obtained the data sets from kaggle and ESPN Cricinfo. Those sources included most of the data but there were some required details which were not included there. So we had to enter them manually. Since there were a lot of data it was a very tiring task to gather all those data.
 In the first phase, although we gathered data for match details form 2010 to 2020, here we shorten that time span to 2015 to 2020. All these data are gathered only for selected number of countries.
 Selected countries - ICC full member countries: England, Australia, New Zealand, Sri Lanka, Bangladesh, Pakistan, India, Zimbabwe, West Indies, Afghanistan, South Africa
 and Ireland
 Data Preprocessing
 Gathered data sets have a very large amount of samples which makes it difficult to track the missing or any garbage values. Although we have collected number of features, some features are not really important. So in this stage our aim is to transform the raw data that we gathered from various sources into a useful format so that it is ready to use for analysing.
 Following are some of the things that we had to face when preprocessing the raw dataset.
 The height of some players was not stated in data sources. Therefore we treated them as missing values and replaced them with class mean of the attribute.
 Similar situation happen with the man of the match feature of a player. So we had to replace those missing values with 0.
 Missing data of batting style and bowling style were replaced using class mode of that attribute.
 Bowlers and fielders first dataset had many missing career details. So we had to remove about a lot of players since we can not predict some required features.
 Removed the matches that did not have a final result or tied matches.
 Individual Player Performance
 Individual performance of players is considered separately as batsmen, bowlers, and fielders. Different features contribute in different priorities when comparing the players. Some features show different importance levels in different match types. In prior to rank the players, the features were weighted according to their priorities among each other. For that, we used AHP(Analytic Hierarchy Process).
 Following are the steps we followed in calculating the weights for each feature after selecting the feature importance values from the model which gave highest accuracy.
 Using the feature importance, we did a pairwise comparison between each feature importance with all other features’ importance. There we created a matrix to compare each of the features.
 Then, we found the priorities of each attributes when compared to the other attributes with the help of the matrix created. Following equation was used to find priorities.
 Score Prediction
 We needed to do a comparison between the players. So, we calculated a new feature ‘score’ for each player; a score for each batsman and a score for each bowler as batsman, bowler and fielder. All-rounders get three ranks because of this. This new feature was derived using the values of previously used features; using carrier features and all features corresponding to each player and the weights of each feature calculated.
 When ranking batsmen, bowlers and fielders we considered the score predicted by the above mentioned methods. We considered three features to obtain three different scores for same fielder. We followed same approach to score batsmen and bowlers. Then the best score can be choose by considering the other factors affect on the match and the team. The three features considered for fielders are number of dismissals, average dismissals per inning, average dismissals per inning of winning matches.The three features considered for batsmen are overall runs, average and winning average. The three class attributes considered for bowlers are overall wickets, average wickets and winning average wickets. Following equation was used for this considering importance of features.
 Combined Player Performance
 For combined player performance, we did a comparative analysis of it with the sum of individual player performances.
 Players in a team are considered in batsman and bowler categories. Their combined performances were taken considering different combinations like 2-grams, 3-grams and 4-grams for a specific feature in each category.Then we took the sum of individual performances from specific feature from each category which matches the combination type of combined performances. Using those two values, we did a comparative analysis.
 Frequent Player Combinations
 For this analysis we used Association Rules. This depicted how frequent a player combination has occurred together, in a team and led the team to victory. Association rules use the support and the confidence to interpret this situation.
 Itemset :
 All items occurring in a rule
 Antecedent : Contains the different player combinations (number of players in a combination is always greater than 1). Generally we called it as the items found within the data.
 Consequent :
 In a particular rule this will always be the “won” (means the result of the considered match). Generally this means the output that happens if a particular itemset occurred.
 Confidence : Number of correct rules with the considered player combination and won the match
 Outcome Prediction
 For outcome prediction of a ODI match we build a outcome prediction model using machine learning classification models. This is a binary classification problem, since we are predicting won or loss result of the match. The draw matches and abandoned matches were excluded from the dataset. Both existing match features and derived features from the individual player performances considered for this outcome prediction. 70% of data used as the training set and other 30% used as the test set. The data set was balanced dataset.
 Experiment Setup and Implementation
 Research Tools
 To achieve our goal we have used machine learning libraries used in python.
 scikit learn - A machine learning library for python which includes algortihms of different
 classifiers, regressors etc.
 Pandas - Used for data manipulation and analysis.Handling data structures and operations
 Numpy - Supports for multidimensional arrays and matrices. Contains a large number of high-level mathematical functions
 Matplotlib - A plotting library. Helpful in visualising relations between features.
 Apriori - An algorithm for frequent itemset mining and association rule learning.
 Inorder to collabaratively work on the codes, we used Google Colab; an online tool with jupyter notebook environment.It was very helpful Since it contains all the required machine learning libraries for python.
 Data manipulation and Testing
 Our initial data sets were already in .csv which is a format that we can use in building up our models and tested. But, as mentioned before, we were not able to find all the required fields in one source. So we had to go through manual process to add the players height, and man of the match and number of hat-tricks of bowlers features to these data sets. Even after filling all the required fields there were some missing values. So first we had to go through data preprocessing process.
 Regression and Feature Importance Selection Methods
 All the class attributes we considered for the batsmen and bowlers with different feature combinations were numeric type. Therefore we had to use regression to build up machine learning models. The regressors used in our study are random forest regression, Decision tree regression, XGBoost regression, k-neighbours regression and linear regression.
 Random Forest Regression
 Random forests use ensemble learning methods to build up the regression model and construct multitude of decision trees at training time. After fitting the model feature importances property can be used to take importance of input features.
 CART - Decision Tree Regression
 Decision tree regression builds regression model in the form of a tree structure. It breaks down a dataset into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes. After fitting the model feature importances property can be used to take importance of input features.
 XGBoost regression
 XGBoost is a library that provides an efficient and effective implementation of the stochastic gradient boosting algorithm. XGBRegressor is the class in the XGBoost that can be used for regression. After fitting data to the model there is a property in model called feature importances which can be use to find the importances of features used to build up the model.
 k-neighbours regression
 k-NN regression, the k-NN algorithm is used for estimating continuous variables and the permutation testing canbe used to get the feature importance of each input feature.
 Linear Regression
 Linear Regression is in sklearn.linear model and after fitting this regression model the coeff property includes the importance in each feature that is used to build the model.
 Features Considered
 Batsmen
 Man of the match - Number of man of the match awards won by the player
 Last 4 matches runs mean - Average of runs taken by player in his last 4 matches
 Height(cm) - Height of the player
 Batting Style - Whether batsman is right handed or not
 Average - Batting average throughout his carrear
 NO - number of notouts
 HS - Highest score
 HS_NO - Whether highest score played, and out or not
 SR - Strike rate
 100s - number of 100s scored
 50s - number of 50s scored
 0s - number of times that batsmen out for 0
 Mat - Number of matches
 Inns - Number of innings
 Bowlers
 Man of the match - Number of man of the match awards won by the player
 4 - number of times 4 wickets were taken
 5 - number of times 5 wickets were taken
 Height(cm) - Height of the player
 Bowling style
 Econ - Economy of the bowler
 Hattricks - number of times 3 wickets were taken on a row
 SR - Strike rate
 Average - Batting average throughout his carrear
 BBI - Best bowling
 Mat - Number of matches
 Inns - Number of innings
 Fielders
 Man of the match - Number of man of the match awards won by the player
 Dis - Number of dismissals taken by the player
 Height(cm) - Height of the player
 Ct - Number of catches
 St - Number of stumps
 Ct Wk - Number of catches as wicket-keeper
 Ct Fi - Number of catches as fielder
 MD - Maximum number of dismissals recorded by the player in a match
 MDct - Maximum number of catches recorded by the player in a match
 Inns - Number of innings
 MDst - Maximum number of stumps recorded by the player in a match
 Outcome Prediction
 Team - Name of the country from the considered 12 ICC full member countries
 Day-Night - Whether match is a day match or day and night match or night match
 Home - Whether match is playing in the country which team belongs to or not
 Ground - Name of the ground where match is playing
 Toss - Toss result, whether toss won or loss
 Bat - Whether bat first or second
 Opposition - Opposition team name
 Batsmen Score - Sum of individual batsmen score of players of the team. This individual batsmen score is refers to the batsmen score obtained in this research’s individual performance section.
 Bowlers Score - Sum of individual bowlers score of players of the team. This individual bowlers score is refers to the bowlers score obtained in this research’s individual performance section.
 Fielders Score - Sum of individual fielders score of players of the team. This individual fielders score is refers to the fielders score obtained in this research’s individual performance section.
 Opposite Batsmen Score - Sum of individual batsmen score of players of the opposition team. This individual batsmen score is refers to the batsmen score obtained in this research’s individual performance section.
 Opposite Bowlers Score - Sum of individual bowlers score of players of the opposition team. This individual bowlers score is refers to the bowlers score obtained in this research’s individual performance section.
 Opposite Fielders Score - Sum of individual fielders score of players of the opposition team. This individual fielders score is refers to the fielders score obtained in this research’s individual performance section.
 Pitfalls and workarounds
 There were several pitfalls that we had to face during the project. The first thing was that we had to gather some background knowledge about cricket since we were not much familiar with the game and not experienced with it. Also we had to find out how different features affect a player or a team, what is measured or depicted by each feature and what features affect each cricket game type. For example, in bowling, strike rate would become an important feature for limited over matches since scoring as much as possible is very important in those matches.
 Even there was sources to collect data, data organization in the required format, new features creation according to the needs of the our method was the biggest challenge.
 One challenge was to come up with a method to find values for the new feature ‘scores’ of each player. So for that we followed some papers and their methods. In those methods, there was no way to find the feature importance comparison matrix. For that, we got the feature importance given by the highest accuracy model that we trained as an input and found the importance comparison matrix. When finding scores for bowlers, for some class values, we got nan values for priorities and weights for some models that we selected. So we had to change the model that we selected.
 For the statistical method of frequent player combination comparison, we tried to find combinations of players up to 11-grams. But we did not have enough CPU performance power to do that. Therefore we only considered 2,3,4 grams of players.
 When building the outcome prediction model, we considered the sum of scores that we calculated for the players considering their individual performance. Since we calculated several scores for each player, there were huge number of combinations of batsmen,bowler and fielder scores to consider. Therefore selection of good combination of bating, bowling, fielding scores as attributes for final model was a problem. We
 selected few scores with different behaviors and selected the best combination considering the outcome prediction model accuracy.
 Results and Analysis
 Results
 We used several methods to obtain several values for same player considering two feature combinations and different class attributes. Feature importance of each method took from the feature importance for the best accuracy model of each method. Then got pairwise feature importance matrix for each followed method for batsmen and bowlers.
 Individual Performance
 1.Batsmen
 We considered several feature combinations and different class attributes to obtain score for batsmen. But for the final outcome prediction model we used the scores set obtained using the career features and the class attribute : Overall Runs.
 Feature Importance: All Features combination Batsmen and class attribute: Overall Runs
 Feature Importance: All Features combination Batsmen and class attribute: Batting Average
 Feature Importance: All Features combination Batsmen and class attribute: Winning Average
 Feature Importance: Career Features combination Batsmen and class attribute: overall runs
 Feature Importance: Career Features combination Batsmen and class attribute: batting average
 Feature Importance: Career Features combination Batsmen and class attribute: Winning Average
 2.Bowlers
 We considered several feature combinations and different class attributes to obtain score for bowlers. But for the final outcome prediction model we used the scores set obtained using the all features and the class attribute : Overall Wickets.
 Feature Importance: All Features combination Bowlers and class attribute: Wickets
 Feature Importance: All Features combination Bowlers and class attribute: Bowling Average
 Feature Importance: All Features combination Bowlers and class attribute: Economy
 Feature Importance: Carrier Features combination Bowlers and class attribute: Wickets
 Feature Importance: Carrier Features combination Bowlers and class attribute: Bowling Average
 Feature Importance: Career Features combination Bowlers and class attribute: Economy
 3.Fielders
 -Class Attribute : Dismissals per innings
 XGBoost had the best accuracy. Therefore XGBoostalgorithm was used in calculating feature importance for each of the features,relevent to all the other features. Eventually these values are used in prioritiz-ing and assigning weights on the features. Then this weights used to calculate the fielders scores.
 -Class Attribute : Winning dismissals per innings
 Again the highest accurate model was XGBoost. Weights and priorities for this model was then calculated and took the scores of the players.
 -Class Attribute : Dismissals(Dis)
 andom Forest Regressor model was used in calculating feature importance for each of the features relevant to all the other features.Then these values were used in prioritizing and assigning weights on thefeatures. Then these weights were used to calculate the fielders scores.
 Feature Importance of Fielders and class attribute: Dismissals per Innings
 Feature Importance of Fielders and class attribute: Winning dismissals per Innings
 Feature Importance of Fielders and class attribute: Dismissals
 Impact of all the features for the outcome prediction model build using all the features
 Impact of the features for the final outcome prediction model
 From the above two fielder score sets we selected score set in Class Attribute : Dismissals(Dis) for the calculation of fielders score and opposite fielders score features for final outcome prediction model. This feature set was selected considering the accuracy of thefinal outcome prediction model.
 Team Performance
 1.Combined Average
 For the combined average, we considered both win and lose matches and only winmatches of all 12 ICC full member countries. This was found for batsmen andbowlers.
 -Batsmen
 Here, we are considering the average of runs.
 i. Win and lose Match results
 ii. Win match results
 -Bowlers
 For the bowlers the combined results were taken considering three approaches.Here, we considered the average runs conceded and wickets taken by thebowlers.
 Combinations of players that satisfy combined averages of runs given bythe considering n-gram of bowlers < Addition of their individual averages ofruns given by them
 Combinations of players that satisfy combined averages of wickets >Addition of their individual Wickets
 Combinations of players that satisfy both the 1 and 2 conditions above
 Win and lose Match results for batsmen
 Frequent Combinations
 We analysed different winning player combinations occurred in 12 countries using association rules.
 -Without Player Position
 Distribution of India association rules
 Distribution of Ireland association rules
 Distribution of South Africa association rules
 Distribution of West Indies association rules
 -With Player Position
 Comparing Frequent Combinations Rules with Batsman Combined Av-erage
 Association rules that we have obtained for wining player combinations in teamswere compared with the previously obtained batsman combined averages. Thiswas compared for 2-grams, 3-grams etc. of player combinations.
 Distribution of Confidence in India association rules against Batsman combined average
 Distribution of Support in India association rules against Batsman combined average
 Conclusion
 Cricket is a sport with a huge fan base. So winning a cricket match has become a great honor for some countries. With this popularity, there are a number of bets on predicting the outcome of a cricket match. Therefore it has become a challenge to form a cricket team with best performance player ultimately which lead the cricket team to win. So in this study our aim was to model ODI playing batsmen and bowlers and score their performances accordingly. This study has presented priorities of each
 features of players and their weights. We considered five new features which were not considered before. Namely player’s height, the number of times they have won the man of the match, not out state when playing highest score and for bowlers number of hat-tricks they have taken and the BBI. Most of the other features considered in this study have used in previous researches. From this study we can conclude that man of the match, height, not out state when playing highest score, hat-tricks and BBI features has comparatively lesser weights in all of the different methods that we used to model batsmen and bowlers. From our results up to now different co-relations among different features have given higher weights on those features.
 As for the frequent player combinations, what we can conclude is that we cannot predict that having a certain player combination in a team will win the match.
 Our outcome prediction model was considered six new features which was not used in earlier researches. The importance of these six features is, those features were derived considering the individual performance of the players. The newly introduced six features are batsmen score, bowlers score, fielders score, opposite batsmen score, opposite bowlers score, opposite fielders score. These six features showed very much considerable impact on the final model.
 Links
 Project Repository
 Project Page
 Department of Computer Engineering
 University of Peradeniya
 Back to top","Data Mining System for Selecting a Winning ODI Cricket Team. This analysis on cricket includes analysis of the individual performance of players, the combined performance of players, frequent winning combinations, and outcome prediction of an ODI match.",E15,Undergraduate Research Projects,https://projects.ce.pdn.ac.lk/4yp/e15/cricket-analysis/,https://github.com/cepdnaclk/e15-4yp-cricket-analysis,https://cepdnaclk.github.io/e15-4yp-cricket-analysis,https://cepdnaclk.github.io/e15-4yp-cricket-analysis/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/4yp/E15/cricket-analysis/
97,human behavior prediction using cctv,"Projects | Department of Computer Engineering
 Projects - Department of Computer Engineering
 University of Peradeniya, Sri Lanka
 Adaptive people movement and action prediction using CCTV to control appliances
 Team
 E/15/010, Ruchika Alwis, email
 E/15/265, Risith Perera, email
 E/15/347, Isuru Sudasinghe, email
 Supervisors
 Eng. (Dr.) Kamalanath Samarakoon, email
 Table of content
 Abstract
 Related works
 Methodology
 Experiment Setup and Implementation
 Results and Analysis
 Conclusion
 Links
 Abstract
 With the availability of high-performance processors
 and GPUs, the demand for Machine learning, Deep learning
 algorithms is growing exponentially. It has become more and
 more possible to explore the depths of fields like Computer
 vision with these trends. Detecting humans in video footage using
 computer vision is one such area. Although human detection is
 somewhat primitive when compared to today’s technology, using
 that data to produce various results like recognizing postures,
 predicting behaviors, predicting paths are very advanced fields
 and they have very much room left to grow. Various algorithms,
 approaches are available today to accomplish the above kind of
 tasks, from classical machine learning, neural networks to statistical
 approaches like Bayes theorem, Hidden Markov Models,
 Time series, etc. This paper summarize the result of a system
 that combines above technologies in order to control electric
 appliances through predictions. These predictions are deducted
 by analyzing CCTV footages of the user using computer vision.
 Related works
 various approaches to analyze video footage in order to produce results like human tracking, path prediction, action recognition, action/behavior prediction, etc. as well as used existing data to make behavior predictions as well. There are many inbuilt libraries that are widely used in these researches such as Alpha pose, Open pose, Vent that produce very good results. It can be noticed that
 most methods used to detect involves some sort of machine learning or deep learning algorithms. These models, therefore, have to be trained with a reasonable amount of data in order to get good results. Therefore, it can be observed that many propose using already trained algorithms unless the paper is about improving or proposing new algorithms itself. Another observable factor is that many approaches use various other techniques prior to the use of a machine learning or deep learning algorithm in the end. There are instances where saliency maps are used to enhance the prediction of path detection algorithms, use of separate feature extraction algorithms, etc. On the other hand, prediction is the most difficult part out of the two aspects mentioned above. Prediction algorithms are very sensitive to variation in tiny details and produce considerable deviations. Even the position of camera placement
 has a drastic impact on the final result in prediction scenarios. For prediction approaches it can observe that researchers have successfully attempted techniques like Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN)all the way to algorithms like Markov Chains, Hidden Markov
 Models (HMM), Hierarchical Representation, Time Series Analysis.
 Methodology
 As mentioned in the introduction, the complete system consists of three separate components that can be run on three separate PCs if required. These components are Human Identification, Behavior Extraction, and Behavior Prediction. Within these subsystems, there are four software components as face recognition, human detection and tracking, action recognition, and behavior prediction.
 A. Face recognition
 Different methods of image processing and machine learning techniques have been used in this component. More weight was taken on improving the accuracy and the execution time. Initially, the python face-recognition package was used which has been implemented on top of dib, which is a C++
 toolkit containing machine learning algorithms. But it has low accuracy and less performance. In order to obtain the desired outcome, the desired outcome python package is used to perform face detection using MTCNN and face identification using an Inception Resnet model. It is pre-trained on the
 VGGFace2 dataset. More advanced implementation details can be found on the GitHub Repository - GitHub the whole process is separated into three parts which are face detection, getting face embeddings, and finally, identifying faces by measuring the distance between embeddings
 in a specific frame. Face identification can be easily applied to raw images by first detecting faces using MTCNN before calculating embedding or probabilities using a before calculating model. It gives huge performance improvement rather than sending the entire frame to the Resnet model for getting
 face embeddings.
 B. Human detection and tracking
 Here YOLO v3 is used to detect humans. YOLO v3 is
 selected for the detection due to its speed while giving fairly accurate results at the same time. Although it is possible to detect humans using and track them using a model possible to, that approach was discarded due to the overhead created when running such heavy models’ side by side. Therefore, the current implementation is, first detect and crop each person in the frame using the YOLO object classification model. And then send each cropped person image to the Reid model. Since this Reid model was implemented sunporch, proper Porch implementation of YOLO has to be
 used instead of TensorFlow implementation. Because when different implementations of TensorFlow adaptors run at the same time, the GPU memory can be crashed. Further details of that Porch YOLO implementation can be found on the GitHub Repository - PyTorch-YOLOv3. The Porch Reid model is based on the works by Zheng et al. There is a Reid model Baseline (PCB) that conducts uniform partition on the conv-layer for learning part-level features. It does not explicitly partition the images. PCB takes a whole image as the input and outputs a convolutional feature. Being classification net, the architecture of PCB is concise, with slight modifications on the backbone network. The training
 procedure is standard and requires no bells and whistles. It shows that the convolutional descriptor has a much higher discriminative ability than the commonly used fully-connected (FC) descriptor. In order to identify the same person from different perspectives, it is required to have a collection of all persons’ images that we are going to identify in our domain. That’s where the face identification section comes into place. Once a person has identified from his face through the entrance camera, that person’s full-body image is saved in the human database under the current date folder. This is done because the same person can wear different clothes on different days. Then periodically generates features of every person in the current day folder and saves it as a single matrix file. Now in the human tracking part, first, the feature matrix file is loaded from the folder corresponding to the current day. Then humans in the current frame are detected using YOLO and features are extracted from the Reid model. These features are compared with each individual feature within the file collection and find the person tag. This implementation is somewhat similar to the approach we used in face identification.
 C. Action Recognition
 To identify actions performed by each individual person of the frame, first, the pose or the skeleton of the person has tube estimated. To do that, Porch implementation of opposes used. It produces 18 joint points of the person. Further implementation details can be found in the GitHub Repository- GitHub. Once the joints points are obtained then itis sent to an action classifier to determine the action. A window of five frames is used to extract features of body velocity, normalized joint positions, and joint velocities. Then mean filtering the prediction scores between 2 frames. Get a label of the person if the score is larger than a certain threshold. Here, a pre-trained action classifier provided by the GitHub Repository - Realtime-Action-Recognition is used to obtain the action identification.
 Behavior Data Extraction Interface
 D. Behavior prediction
 There are two Machine learning algorithms used in the prediction model. One Algorithm is used to get the next location while the other algorithm is used to determine the state of the electric bulbs. Due to the constraints in the timeframe to create a totally new database from scratch, a mock database has to be created for this. The Final data-set needs tube of the structure in Table I. The public data-set we selected to create the mock data set has the structure in Table II. This dataset consists of readings taken from motion sensors, pressure sensors that were taken in fixed time intervals. This raw data is then used to predict the movement of the owner inside the house. After processing the data-set based on multiple assumptions, a mock data-set is created that has the required data structure as in Table I.
 processed gain before feeding into the classifier. First of all, multiple sets of sequences of locations are made using the following inferences. To predict the state of the electric appliance, a cat Boost Classifier is used. And the above sequences, along with the result from HMM is used to make the prediction on the state of the device.
 Experiment Setup and Implementation
 Camera distortion removal
 Camera distortion removal is carried out by calculating the camera matrix that is
 described in the chapter 3. Implementation of this procedure to find the camera matrix
 is carried out as following.
 In the process of calibration, we calculate the camera parameters by a set of know 3D
 points (Xu, Xu) and their corresponding pixel location (u, v) in the image.
 For the 3D points, checkerboard pattern with known dimensions at many different
 orientations are photographed using the IP camera. Here a checkerboard pattern is used
 because Checkerboard patterns are distinct and easy to detect in an image. Not only
 that, the corners of squares on the checkerboard are ideal for localizing them because
 they have sharp gradients in two directions. In addition, these corners are also related by
 the fact that they are at the intersection of checkerboard lines. All these facts are used to
 robustly locate the corners of the squares in a checkerboard pattern
 Face Identification
 Different methods of image processing and machine learning techniques are used in this
 section. More weight is taken on improving the accuracy and the execution time. Initially
 we used face recognition python package which is implemented on top of dib that is
 a C++ toolkit containing machine learning algorithms. But it has low accuracy and
 less performance. In order to obtain our desired outcome, the dib python
 package is used to perform face detection using MTCNN and face identification using
 an Inception Resnet model. It is pre-trained on VGGFace2 dataset. More advanced
 implementation details can be found on the GitHub Repository - GitHub.
 Human detection and tracking
 In this section we want to give the same tag number to each individual person anytime
 he travels through the field of view of the CCTV camera. To do that our early plan
 was to get a bounding box using YOLO and track the person using Deep Sort and If a
 person’s track was broken then we find his previous track number using the above Reid
 model. But when looking at the performance of this Reid model we decided to get rid of
 Deep sort and use Reid to identify each person exactly in each frame.
 Therefore, the current implementation is, first detect and crop each person in the
 frame using YOLO object classification model. And then send each cropped person
 image to Reid. Since this Reid was implemented using porch, we had to use a proper
 porch implementation of YOLO instead of TensorFlow. Because when both run at the
 4.1 Design & Implementation of Prototype 30
 same time, the GPU memory can be crashed. Further details of that porch YOLO
 implementation can be found on the GitHub Repository - PyTorch-YOLOv3.
 Prediction Model
 There are two Machine learning algorithms used in the prediction model. One Algorithm
 is used to get the next location while the other algorithm is used to determine the state
 of the electric bulbs. Due to the constraints in the time frame to create a totally new
 database from scratch, a mock database has to be created for this. The Final data-set
 needs to be of the structure in table x. The public data-set we selected to create the
 mock data set has the structure in table y. This data-set consists of readings taken from
 motion sensors, pressure sensors that were taken in fixed time intervals. This raw data is
 then used to predict the movement of the owner inside the house.
 A sequence of locations would start after one trigger point to the start of another
 trigger point.
 • A set of sequences will always belong to the same day.
 • A trigger point will be described as following
 – An instance where the state of an electric appliance changes.
 – An instance where the person’s location changes to the Bed.
 These sequences are then used to train the HMM (Hidden Markov Model). The
 HMM used for this model is imported from the ‘homeland’ python package
 Overalll Design Diagram
 Results and analysis
 results obtained through the prediction model. Furthermore, this section presents some comparisons between few other models with the main models well. The main model that is used to predict the next location of an individual is a Hidden Markov Model (HMM) that has 3 hidden internal states. This prediction is then used to predict if an electric appliance should be turned on or not. When considering the results of the HMM, three types of HMMs were compared in the beginning
 Furthermore, the same data-set was tested with few other models as well. A random
 forest classifier, A basic CNN (Convolutional Neural Network), and an LSTM (Long Short-
 Term Memory) model were tested and the best performance of each model is presented
 in the following table in this comparison, the data from a single person over
 the course of six months is being used.
 In order to obtain the optimum results for each of the models, various steps had to be
 taken. It included changing the number of features that are used as input, normalization
 of the data, and fine-tuning the parameters such as the batch size and a number of
 iterations. However, a detailed explanation is excluded as the change in their performance
 with respect to varying the parameters was very insignificant.
 Location Prediction Accuracy
 After the next location is predicted, then a Cat Boost classifier is used to determine
 which electric appliances should be turned on? The model was trained to predict the
 electric bulbs of the Kitchen, Bathroom, and Television. The accuracy of the Cat Boost
 classification is as follows
 Table
 Appliance Prediction Accuracy
 Conclusion
 The main result for the lack of accuracy is due to the mock data set that was created to train the model. The original dataset was consisting of random sensor data that was taken in fixed time intervals. This raw dataset was then modified to create sequences of movements that would end up specific tasks such as sleeping in the bed, turning lights on/off, etc. However, it seems that the resultant sequences have deviated significantly from a natural routine of a human. This has resulted in a low accuracy even when an HMM is used to predict the next location. Also, it can be observed that the accuracy of the HMM
 keeps increasing when the number of hidden states in the Multinomial HMM kept increasing. But the complexity of the transformation matrix increases exponentially when the hidden states are increased. After 20 hidden states, the time to fit the model for a system with 30 hidden states takes a duration
 recorded in hours, which is a huge drawback to a real-time system that needs to be trained periodically. Therefore, the model was selected when it showed the highest accuracy of53% under 19 hidden states. The reason for obtaining low accuracy even when an Stims used is because of the nature of the dataset. This resultant dataset lacks fixed time steps between adjacent data. And lots of such redundant data was removed during data preprocessing. However, it can be assumed that an LSTM would yield much better results if the data can be recorded in fixed intervals. Itis cleared that after the data pre-processing, the problem has deviated from the context of a time series analysis. The aim of this project was to practically attempt the automation of control of basic electric appliances through the data obtained by processing CCTV footage data. Even Though the primary focus was to develop a model with the capability to successfully do the predictions, it became clear that the system to extract the data from video footage plays an important role as well. Therefore, the main focus for this project during this time-frame was to develop a robust system to extract the required data from a CCTV camera network in real-time. Finally, for future works, we would like to summarize what we have discussed above. This project clearly proves the possibility to automate household electric appliances just by observing human behavior. However, in order to implement system at the domestic level, all the existing components
 have to be further fine-tuned. And in order to achieve that goal, each component has to be optimized separately. We like to state that this project can be used as the foundation for such an attempt so that the components of this system can be separated into parts, developed, and then combined together for a better, much more accurate system in the future. And all the models used here can be trained with custom datasets as well. Therefore, the fastest way to improve the performance would be to train each component with custom datasets that are specifically designed for this system.
 Links
 Project Repository
 Project Page
 Department of Computer Engineering
 University of Peradeniya
 Back to top","In a domestic environment, people's behavior is often routine. They move place to place in a house while activating different domestic appliances and lights. On the other hand, CCTV is becoming a common surveillance system installed in many houses and buildings. A correctly trained adaptive system can predict the behavior and perform control actions automatically. In this research project, it is intended to develop an adaptive training system using CCTV video streams for controlling appliances. ",E15,Undergraduate Research Projects,https://projects.ce.pdn.ac.lk/4yp/e15/human-behavior-prediction-using-cctv/,https://github.com/cepdnaclk/e15-4yp-human-behavior-prediction-using-cctv,https://cepdnaclk.github.io/e15-4yp-human-behavior-prediction-using-cctv,https://cepdnaclk.github.io/e15-4yp-human-behavior-prediction-using-cctv/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/4yp/E15/human-behavior-prediction-using-cctv/
98,nearIR spectroscopy,"NearIR Spectroscopy
 Projects - Department of Computer Engineering
 University of Peradeniya, Sri Lanka
 Near-IR Spectroscopy
 Team
 E/15/383, Keshara Weerasinghe, email
 E/15/349, Shamal Tennakoon, email
 E/15/188, Nithya Kularatne, email
 Supervisors
 Dr. Isuru Nawinne, email
 Prof. Roshan Ragel, email
 Table of content
 Abstract
 Related works
 Methodology
 Experiment Setup and Implementation
 Results and Analysis
 Conclusion
 Publications
 Links
 Abstract
 Near-Infrared spectroscopy is used for better vein visualization to make the venipuncture process more efficient. While there exist a few models which use the said mechanism, these models are costly, have accuracy issues, and are limited only to certain types of skin tones. Some of the available devices use image-guided venipuncture technique and the others use projection.
 We propose a low-cost mechanism of obtaining near-infrared spectroscopy by using the image-guided technique. We decided on using this technique after assessing both the available techniques. The low-cost is achieved by optimizing the image processing algorithms and adjusting the illumination method. We have tested and optimized the algorithms accordingly.
 We use near-infrared LEDs as the source of illumination, and a CMOS camera for image acquisition. Images are processed using OpenCV, and Histogram equalization and CLAHE algorithms are used in preprocessing. Initially, we processed the still images and later on developed the model to process the live video stream and display the processed video footage that visualizes the veins in real-time. We display the vein map on a 7 inch IPS LCD screen.
 We have tested the prototype using different combinations of light sources with different intensities and have analyzed the results. We have also analyzed how the results vary based on body fat. In order to quantitatively analyze, we have obtained a count of the number of visible veins and depicted the comparison in a graph. We have concluded that a higher intensity does not always increase the visibility of veins. Our plan is to conduct a clinical trial and test the device on human subjects and get the feedback from both the patients and phlebotomists and improve the model so that those final users are satisfied.
 Methodology
 3.1 Conceptual design
 Our task is to design and develop a device capable of detecting veins belonging to a particular region and display them on a portable screen accurately. The entire process – detection and display occur in real-time. The overall procedure can be divided into a few basic sub-components.
 3.1.1 Identifying the optimum wave length for lighting and obtain the appropriate light sources
 We obtained a variety of different sources of the near IR region. Here, we determined parameters such as the optimum wavelength, intensity,
 and lighting conditions, thereby the environment where image capturing should occur and the placement of the object. This step is a prerequisite for image capturing.
 3.1.2 Image capturing
 Once the light sources were selected, we proceeded with the capturing of images. For this purpose, we use a modified CMOS camera and, with the help of the light source, illuminate the object of interest and capture it.
 Later on, we progress to capturing a live stream instead of a still image.
 3.1.3 Preprocessing of the images
 Once an image is acquired, preprocessing steps are applied. These steps utilize image processing techniques to enhance the region of interest in a captured image (for example, the vein pattern in hand).
 The main objective is to create a visible contrast between the Region of Interest and its surroundings. The same preprocessing procedures are implemented for video capturing as well.
 3.1.4 Applying a formulated algorithm to clearly visualize and extract the vein patterns
 After preprocessing, we focus on extracting the region of interest from the image. Here, we isolate only the vein pattern from the body part captured in the image and that will be used as the image that is displayed on the screen. This task is a bit complex as we need to develop an algorithm that is capable of performing this task both efficiently and accurately. The algorithm contains several fundamental steps that are discussed under the methodological approach and extensively under the Implementation section.
 3.1.5 Designing a interface to display the live stream obtained from the camera
 A user interface is created in order to display the live video stream captured from the image sensor. It is capable of showing the video footage that has gone through image processing to clearly portray the veins real time. More details regarding this is stated under the implementation segment.
 3.2 Methodological approach
 We were able to assemble pieces of hardware to form a fully functioning hardware setup which captures the image and feeds it to a microprocessor where all the image processing is handled, and is displayed on the screen in real-time. As a final step we planned to do a clinical trial and get the feedback from the actual final users.
 3.2.1 Hardware setup, modifications and assembly
 NIR light source – These light sources fall under 760 – 940 nm waveband and the emitted light is absorbed by deoxygenated hemoglobin that circulates within the peripheral veins.
 Attention is given to the arrangement of the NIR LEDs, the number of LEDs used, their positioning and distance. In our setup they are fixed in concentric circles with the camera
 located at the center of the circle. The wavelength of a NIR source can be precisely measured with the use of a spectrometer which we were able to build and calibrate.
 Image sensor – This part of the setup captures the image that is illuminated by the light source. The image sensor that we are using currently is a NOIR image sensor that has no IR filter attached to it. The sensor captures an image of the object that is illuminated by the NIR sources and the resulting image displays highlighted (mildly) vein patterns. A high-pass filter is used to cutoff the radiation below 750 nm.
 Microprocessor – This is where the processing of the acquired image takes place. A raspberry pi 3 is used, and all the hardware components are interfaced with it. The image processing is executed by the microprocessor and the result is sent to be streamed live that will be displayed on the screen.
 Image display – The Image that is acquired and processed, is displayed on a 7 inch IPS LCD panel so that the phlebotomist can be guided by showing the positions of the veins accurately.
 3.2.2 Image processing
 Image processing is performing operations on an image/set of images to get an enhanced image or to extract some useful information from it. Our objective is to use image processing techniques to identify the vein patterns of an image and its extraction. The Open CV python package along with some basic applications of machine learning (classification and clustering algorithms) are used for vein detection and extraction. The entire process comprises of 3 important phases.
 • Background removal : This is the initial phase. For our application, we need only the region of the hand (or some specific body part) where veins are located, and the rest can be considered as noise and therefore eliminated. More details on how this is implemented is discussed under implementation section.
 • Preprocessing : This is the initial phase where the raw image is enhanced to show the vein patterns clearly with comparison to the tissue. More details on how this is implemented is discussed under implementation section.
 • Algorithm Development and Optimization : This is the most crucial phase of the entire process. It involves developing an algorithm to distinguish and derive the vein pattern from the image with a good accuracy. The general functionality of the algorithm can be expressed by following elementary steps.
 Eliminate the background of the image and isolate only the region of interest
 Increase the contrast of the image with the use of histogram equalization and CLAHE algorithm
 Use morphological transformations on the resulting image to tone down lines and edges.
 Smoothen and noise reduction using techniques such as median blur, bilateral filters.
 Segregate the pixels of the image into clusters based on the color of each pixel. Colors are assigned to each cluster to help identify and distinguish the components of the image from one another.
 Use Edge detection to separate the vein patterns - canny edge detection, Laplacian gradients, Sobel combined.
 Use adaptive thresholding to get the processed image.
 Separate the vein from the processed image, color them, and overlay the pattern onto the original image to see a clear and contrasting vein pattern.
 3.3 Assumptions and constraints
 Some problems arose when trying to find/verify the wavelength of the IR sources with
 the spectrometer and we were not able buy a variety of sources with different wave
 lengths due to the prevailing situation of the country. Therefore, we assumed that the
 wavelength of the sources that we currently have are accurate.
 Since the NOIR image sensor has no IR filter, it acquires all bands of light instead
 of just IR which affects the contrast of the image that is due to undergo processing.
 Therefore, we use a high-pass filter so that only the radiation above 750 nm is allowed to
 pass through.
 When trying to remove the background of an image, the current implementation relies
 on the color of each pixel to identify possible edges from the changes in the hsv values
 (hue, saturation, value). This approach might result in part of the region of interest being
 removed along with the background. Also it is quite difficult to automate this process
 where the program implicitly detects the region of interest and detaches the remaining
 part.
 During the pre-processing stage of an image CLAHE contrasting algorithm is used to
 increase the contrast. This results in spots or color marks on the image which are not
 veins, to be enhanced as well. Therefore, we are trying to increase the contrast of only
 the vein pattern while the rest of the image remains as it is.
 Experimental Setup and Implementation
 This section is covered under three main parts: hardware design and implementation, Image processing, and interface design.
 Hardware design and implementation
 Near-Infrared Spectroscopy to identify veins and display on a screen.
 Veins contain oxygenated hemoglobin-rich blood that almost completely absorbs light at near-infrared wavelengths (750 nm–950 nm) up to several millimeters. Using this phenomenon, we can illuminate the skin using infrared-emitting light sources within the specified spectrum and capture the images using an infrared-sensitive image sensor. By further processing of such acquired images, we can extract the vein pattern. We also display the live stream obtained from the camera in order to guide the phlebotomist. He will be shown the detected vein map of the puncture site. He will also be guided by the live stream and will not lose the normal vision on the puncture site by this mechanism. This can resolve many issues faced by venipuncture.
 However, depending on the region, skin color, weight the effectiveness of the selected wavelength can vary. Therefore, our study is about coming up with the best wavelength combination and effective image processing algorithm to identify the vein map irrespective of the above conditions.
 Initially, we got 3 light sources emitting at 3 different wavelengths. Namely 750nm, 850nm, and 950nm. Controlling the intensity of these light sources is not a concern at this stage. To verify the light sources that they emit such wavelengths we needed an infrared light spectrometer.
 However, such a device was not available to us at that moment. So we planned to build a spectrometer to do our study.
 Optical spectrometer is an instrument used to measure properties of light over a specific portion of the electromagnetic spectrum, typically used in spectroscopic analysis to identify materials. The independent variable is usually the wavelength of the light or a unit directly proportional to the photon energy, such as reciprocal centimeters or electron volts, which has a reciprocal relationship to wavelength.
 A monochromatic light beam that is incident on a grating gives rise to a transmitted beam and various diffracted beams, at angles that depend on the ratio between the distance between the lines of the grating and the wavelength of the light. So, if the light beam is composed of multiple wavelengths, the decomposition of the beam into its components is obtained.
 The light with a longer wavelength is deflected to a larger angle with respect to the incident direction (angle of diffraction ) . For each wavelength more rows can be observed. The number of rows that are counted from the middle line, which is not skewed with respect to the incident beam and is taken as a reference , it is said “order” and is often denoted by the letter m.
 The goals of the build were as follows:
 item Having at least 5 nm spectral resolution.
 item Covering the entire visible spectral region and the near-infrared spectral region.
 item Low cost and convenience to calibrate.
 The initial prototype for the spectrometer was designed using CAD software with the required dimensions and built by using 3D printed parts and was designed using CAD software.
 Initial prototype for the spectrometer
 The modal overview is as follows.
 The key optical principle in a spectrometer is diffraction. The slit was constructed with thin sheets of aluminum with sharpened edges. This allows a very limited amount of light to the sensor as a large amount can overwhelm the sensor and lead to inaccurate readings. To collimate the light, a biconvex lens was kept at its focal length away from the slit. This will ensure the light will enter the diffraction grating in a parallel manner. The diffraction grating is really important in this setup. Initially, we were not able to get ahold of quality diffraction grating. So we carefully removed the diffraction material in the DVD disk and used it in our first attempt.
 However, our first attempt was not successful because the DVD did not serve well as a true diffraction grating inside the first prototype. Moreover, a collimating biconvex lens was not included in the first prototype.
 We were able to find high quality 1000 lines per mm grating from the Faculty of Science and rebuilt the spectrometer. Therefore, we redesigned the spectrometer with the true diffraction grating and a collimating lens fixed at its proper focal length. The second prototype proved to be much more accurate.
 To analyze the light, we used a Logitech C270 image sensor (Web Camera) with its inbuilt IR Filter removed (In order to let the IR light enter the sensor). The software was an open-source software and to calibrate we used a Mercury Lamp which emits specific peak wavelengths at 4 different points and used 2 points to calibrate. The two points used for calibration were Mercury 2: 436 nm and Mercury 3: 546 nm.
 After calibrating the spectrometer, we analyzed our two IR sources in a controlled environment to avoid unnecessary light entering the spectrometer. The first source used was had a wavelength of 950 nm. The analyzed result gave us a reading of 945 nm and it was satisfactory.
 The second source had a nominal wavelength of 850 nm and it was read as 840 nm. This was also acceptable since these light sources were not high-quality ones.
 Initial prototype to capture images
 The prototype was designed to be composed of the following components.
 Light source setup
 IR Sensitive Camera
 Single-Board computer
 We designed the prototype to be able to hold 3 light sources at a time where each light source can be turned on and off. The light sources were powered with a 12V power supply which provides 5Amps on load. The single-board computer is a Raspberry Pi Model 3. Raspberry Pi is running a Lightweight version of Raspberry Pi OS which is a Debian-based operating system for Raspberry Pi. OpenCV was compiled and setup on the computer, in order to handle the image processing part of the study. The IR sensitive camera is a product from the same manufacturers of the Raspberry Pi, named NoIR camera which uses a Sony IMX219 image sensor with no inbuilt IR Filters. This is a CMOS sensor. However, CCD works best with Infrared imaging, but, at this point, we could not get ahold of a CCD Sensor.
 The prototype is designed so as the height of the camera and light sources are manually adjustable so that we are able to adjust the focus and determine the best distance for proper illumination and capturing images. The camera comes with a fixed focal distance. However, it sits within the region we require.
 The light sources can be turned on either individually or as a combination during the image acquiring procedure. Following figure displays the actual prototype with 2 IR light sources namely 940 nm and 850 nm. The images were taken using an IR sensitive camera to see the observe the functionality of IR sources.
 https://user-images.githubusercontent.com/62101605/115156034-1441cc00-a0a0-11eb-9768-5ec665f267ee.png
 Second prototype to detect and display veins
 The second prototype consists of the following components.
 Light source setup
 IR Sensitive Camera
 Single-Board computer
 Display screen
 As the light source, we used 48 LEDs emitting at wavelength of 940nm. We arranged IR LEDs in a circular array. This illumination setup uses a total power of 18W. The intensity of the LEDs is controlled by Pulse Width Modulation (PWM).
 We use the same camera we used in the initial prototype with an IR sensitive image sensor - SONY IMX219. A high-pass filter is used so that the radiation above 750 nm is let to pass through and those below that are cut off.
 image processing is done on a single board computer raspberry pi 3b which has a Quad Core 1.2GHz Broadcom BCM2837 64bit CPU with a 1GB ram.
 We use a 7 inch IPS LCD panel with a resolution of 1024x600 for the display of veins. It is powered by a set of li-ion cells with a capacity of 37Wh.
 Following figures give few different views of the second prototype.
 Final prototype
 The final prototype was implemented with suggestions from the medical personnel and considering ease of use and accessibility. The image capturing module was separated from the previous inbuilt design to make it easier for accessing difficult and sensitive venipuncture locations.
 Following figure shows an overall view of the final prototype including the image capturing module and display module.
 The final prototype consists of the following components and an updated IR source with higher intensity.
 IR light source
 IR Sensitive Camera
 Optical High Pass filter
 Battery System
 Single-Board computer
 Display screen
 The IR source consists of 96 LEDs emitting infrared light at 850 nm with a total power of 18 Watts. If we compare it with the second prototype, it consists of 48 LEDs of 940 nm. After analyzing the intensity and determining the wavelength we decided to go with 96 LEDs of 850 nm.
 Thefinal prototype can be seen in the pictures below.
 Image processing
 Several image processing techniques are utilized to get the vein map as the final output, but they can be broadly categorized into 3 main parts as described in the methodology section. In this section, we will explore how each phase can be explored and implemented in a programming environment.
 Background removal
 This stage removes the background of a particular image and isolates only the region of interest. This is accomplished by considering the HSV (hue, saturation, value) values of the image and thereby create an upper and a lower matrix to form the image free without its background. The values of the two matrices are found by using a track bar that executes in the runtime.
 Original image with the track bar is shown below.
 Image after background elimination is as follows —
 Preprocessing
 The primary constituents that we consider as preprocessing for our application are contrasting / equalization and smoothening. The sequence in which these techniques are implemented is: contrasting first, then morphological transformation, followed by smoothening. Contrasting is implemented using two widely used techniques - Histogram equalization and CLAHE.
 Contrasting using Histogram equalization
 A histogram is a graphical representation of the intensity distribution of an image. In simple terms, it represents the number of pixels for each intensity value considered. Histogram Equalization is a computer image processing technique used to improve contrast in images. It accomplishes this by effectively spreading out the most frequent intensity values, i.e. stretching out the intensity range of the image. This method usually increases the global contrast of images when its user data is represented by close contrast values. This allows for areas of lower local contrast to gain a higher contrast. Following image shows the equalized image compared with the original image.
 We can observe that the vein pattern of the hand is enhanced in the second image. If
 we plot a histogram for each of the images the outputs would be as follows.
 Contrasting using CLAHE (Contrast Limited Adaptive Histogram Equalization)
 CLAHE is an improvement on histogram equalization. It limits the contrast amplification to reduce amplified noise by distributing that part of the histogram that exceeds the clip limit equally across all histograms. In general, there are some features in the near-infrared superficial vein images such as high noise and low contrast. CLAHE comes in handy for a situation where the edges of veins in the image are blurred and the vascular lines are not obvious. We can use the CLAHE algorithm iteratively even though it is not ideal in every situation.
 We can see that the image on which CLAHE is done provides more contrast than the equalized image. The veins appear to be highlighted better. The problem is that the veins seem to be a little bit thicker than that in the original image. These are rectified in the latter stages of preprocessing when morphing and smoothening are done.
 Morphological transformation
 Under morphological transformation, we have only used “dilation” and “closing” so far. Additionally, there are few others namely, “closing”, “opening”, and “top-hat gradients”. Opening and closing were used to make the contrasted image from CLAHE appear closer to the original image in terms of the size of the veins.
 Since the CLAHE algorithm appears to thicken the veins, we need to narrow them down. The technique that is proved to be most useful in this case is “dilation”.
 The below images show the input (CLAHE image of the vein pattern) to the dilation method and output generated from it. Following figure depicts the comparison of the above approach and for ease of comparison, the same image is used.
 Smoothening
 This is the final stage of image preprocessing. As we saw previously the dilated CLAHE image still contains a small amount of noise and is slightly pixelated. We’ve tried out 3 types of blurring (smoothening) techniques – Gaussian Blur, Median Blur, and Bilateral Filtering.
 We saw that all the operations smoothen the image while maintaining the same level of detail. Also, we observed that bilateral filtering provides the most smoothened image. Therefore, we opted to go for bilateral filtering. Following figure depicts the comparison of these blurring techniques along with the dilated CLAHE image.
 We can see that all the operations smoothen the image while maintaining the same
 level of detail. Also, we see that bilateral filtering provides the most smoothened image.
 Therefore, we opted to go for bilateral filtering
 Algorithm Development
 This is the most complicated part of image processing that we have currently encountered. The entire algorithm can be summarized into 8 steps which were stated previously.
 The first 4 steps were discussed under background removal and preprocessing. The next involves a procedure called clustering. It is an Unsupervised Machine Learning technique which we can apply to find new patterns in our data. What’s interesting about this algorithm is that we can also use it for image processing tasks as well. In the same manner as with other types of data, we can find pixel patterns in our images that will allow us to process them in a faster and more efficient way. Though there are many clustering methods, we used K-means clustering which is quite straightforward and efficient. How this algorithm works is explained below.
 K-Means is a data clustering algorithm that tries to assign every data point in a data set to exactly one of K possible clusters which is defined according to the users requirement. The main idea here is that the algorithm tries to build the clusters in such way that two data points from the same cluster are as similar as possible, while two data points from two different clusters are as different as possible. The algorithm will iterate through the data set many times and we need to find a proper condition for it to stop.
 An important notion related to Kmeans is the term “centeroid”. The centroid of a cluster is the mean value of all the values present in that cluster. The algorithm operates in a way that it makes sure that the sum of squared distance between the data points in a cluster and the centroid of that cluster is minimum. The centroid of the cluster is the mean value of all the values in the cluster. To elaborate even further the algorithm can be broken down into few basic steps. They are as follows.
 Assign the K number of clusters (this can be found using the elbow method) \item Shuffle the data and randomly assign each data point to one of the K clusters and assign initial random centroids.
 Calculate the squared sum between each data point and all centroids.
 Reassign each data point to the closest centroid based on the computation for step 3.
 Reassign the centroid by calculating the mean value for every cluster.
 Repeat steps 3, 4, 5 until we no longer have to change anything in the clusters.
 In our application we use K-means clustering to group the pixels in the image so that more pixels with similar colors are in the same cluster, while pixels with different colors are placed in different clusters. Ultimately since the veins belong to a particular color range, this enables us to separate the pixels that belong to veins as a collection from the rest of the image. We tested the K-means algorithm on the smoothened image using different number of clusters on different images to determine the optimum value. The results are shown below in the figure.
 We saw that when the number of clusters increase, the image more and more similar to the original image. Since we need to separate the veins from the rest, resorting to small number of clusters seems ideal but it must not be too small because if that is the case, it could end up eliminating part of the veins that are slightly less visible. For example, here we can see a clear distinction between the veins and the rest when the number of clusters = 2. But we see that part of the veins which have low intensity have disappeared since the program interpolates those pixels to the cluster which holds the light coloured pixels. The output when the number of clusters = 5 provides a good balance and seems to be the best result.
 In the sixth step, we perform edge detection on the clustered image to obtain the edges of the vein patterns in the segmented image. Open CV offers several edge detection methods such as laplacian gradient, Sobel combined, and Canny edge detection. Laplacian gradient method proved to be ineffective as it showed no visible edges of veins but only managed to show the edge along with the hand (outline of the hand). Sobel combined method was able to separate the veins but also showed a lot of unwanted detail in the process. Canny edge detection was satisfactory as well. But it wasn’t capable of showing all the edges of the visible vein pattern but was able to provide a portion of it. The following figure depicts the comparison between the smoothened image, Sobel combined, and Canny edge detection.
 The next step of the algorithms uses thresholding. In conventional binary thresholding, each pixel of the image is considered and based on a threshold value the color of the pixel is converted to either black or white. When considering the colors of the pixel, the RGB values are the key aspects. In a grayscale image, the RGB values are scaled down to one particular value, so the thresholding algorithm checks that value with the threshold and based on that decide which to color to assign. For example, if the threshold is 127, then every pixel that has the value below 127 is converted to black, and the rest become white.
 But this approach didn’t yield good results, and therefore we had to resort to a more refined method of thresholding named adaptive thresholding.
 Here instead of one global threshold value, the function splits the image into segments of equal size, and each segment assigns a threshold value depending on the pixel color values which belong to that particular segment. This provides much better results as shown in the figure below.
 The final step is the coloring process. The processed image shows the vein patterns in black whereas the surrounding features are shown in white. This makes it possible to color only the veins by changing the RGB values of the pixels that are colored in black. After the colored image has been obtained it is then overlayed on top of the original image to form the final output as shown below.
 This algorithm works on a satisfactory level, but for some captured images the results were not as good. Therefore, we are working on how to improve the algorithm, what changes we need to make, how to optimize in terms of computational speed, and to increase the overall accuracy for any type of image that includes vein patterns.
 Algorithm optimization using multi-threading
 Multiple threads are used for parallel processing of the frame. The raspberry pi has 4 cores, and so we created 3 separate threads which are executed in parallel. The parent thread captures a frame, and displays the final output in the user interface. The 3 threads process 3 individual frame segments allowing them to be processed simultaneously without any dependency. The parent process splits the captured frame into 3 segments and feeds them so that each thread has only 1 segment to process at a given instance. Then the parent thread waits for all 3 threads to return their respective frames which have undergone the processing steps specified by the algorithm. Once all the threads have returned, the parent process continues from where it had paused. The frames are joined together in their original order to compose the output frame which is displayed in the user interface. An important thing to note is that all of this (from capturing a frame, dissecting it to segments, processing each segment in each thread, returning them to main thread for display) takes place real time. With this approach the latency that was present earlier without threads, is substantially reduced even when there is a lot of movement.
 Interface design
 After designing the algorithm the next step is to actually create a user interface to display the processed output to the user on a real time basis. This is still in a development stage, but the process occurs according to two phases.
 Designing the interface to display still images.
 Enable to interface to display a processed live stream.
 Designing the interface to display still images
 Initially the GUI was designed to be able to show still images while providing few other features like rotation, zooming, transition from base image to processed image and then to the colored image, as well as full screen viewing.
 These features are shown in the figure below.
 The python tkinter package along with openCV is used to create the widgets and the overall functionality of the graphical user interface.
 Configuring the interface to display a processed live stream
 This is the most significant phase of the GUI. We were able to get a live stream up on the GUI and displaying the processed stream with the use of multi-threading. The latency issues which we faced previously were resolved and we are able to produce a processed live stream that is crisp and clear. The veins are displayed vividly and the contrast difference and the smoothness enables the user to identify the veins precisely.
 Results and Analysis
 Testing the light intensities
 As mentioned before we used 2 separate NIR LED arrays corresponding to two intensities(18W and 60W) as the light sources and obtained their outputs. The base images and the outputs gained
 from each light source are shown in the figures below.
 The output obtained from the 18 watts source is quite accurate at visualizing the veins even managing to show very thin subcutaneous veins. The contrast is high and the image is smooth as well. We’ve obtained a lot of images using this lighting source and most of them were able to display clearly visible vein patterns. When looking at the output of the 60 watts LED source we can observe the vein patterns up to some extent. But the very thin veins are barely visible. Also the image has a glare which is caused by the light reflecting from the tissue.
 This was the case for most of the images that we took. The output from the 18 watts LEDs gave the better results in terms of accuracy, contrast and clarity.
 If the subject has a lot of body fat then the light emitted from the 18 watts source may not be able to penetrate through the subcutaneous fat and reach the underlying veins. This may cause veins in some regions to not be visible in the captured image. We hoped that the 60 watts light source would provide sufficient penetration in order pass through the layer of fat within the Hypodermis and resolve this issue. But the results were not as we expected. Even though the intensity is high, it was not enough to penetrate the tissue and the fat probably due to a large portion of the light being deflected from the epidermis and dermis. Thus we observed that having a higher intensity does not guarantee better results. In fact between the 2 sources, the 18 watts light source proved to be the better one. For further analysis lets take a look at the histograms generated for the processed images corresponding to each intensity.
 The above figure shows the differences between the two outputs. The histogram of the low
 intensity output is uniformly distributed unlike the histogram of the high intensity output.
 There are spikes in histogram of the high intensity output for some gray scale values.
 This indicates the presence of color spots in certain regions. If we look at corresponding
 image, we can see patches of white in the hand and dark patches in the background. To
 compare the two intensities even further, we’ve taken set of images and took the count
 of the clearly visible veins for a set of images obtained using both intensities. The results
 are shown in the figure below.
 Testing the algorithm
 We tested several sample images that were captured by the prototype we built, with the algorithm that we developed and got their outputs.
 Shown below are sets of images pertaining to different regions of the hand. Each figure illustrates the outputs of each step in the algorithm starting from the raw base image to the final processed image.
 Looking at each set we can see that the final image highlights the veins which are not so clearly visible in the raw image. An issue that can be noted here is background elimination. For some images, the background is properly eliminated, but for some, the entire background is not eliminated. Also in the final image, we see some dark spots which are not veins. This is due to the contrasting effect and noise created by the CLAHE algorithm. Also, the algorithm falls short when detecting narrow veins but only gives a good depiction of the larger and more prominent veins.
 Histogram analysis is a good method of identifying how the colors of an image are distributed based on their RGB values. We want the distribution to have concentrated clusters rather than a even distribution which makes is difficult to differentiate between colors. The following figure shows the histograms throughout each processing stage of the base image.
 The most recent addition to the image processing algorithm is K-means clustering, which was discussed in length in the previous chapter. In this case, the number of peaks in the histogram indicates the number different clusters. Therefore, all of the pixels fall under one of the peaks which means all the pixels pertaining to veins fall under one peak/cluster.
 When the number of clusters = 2, we can assume from the histogram that the pixels that form veins fall under the lower cluster () whereas the rest fall into the upper cluster. While this is convenient it doesn’t give the best result as some of lightly colored veins end up in the upper cluster and so some of the veins will not be present in the final image. When the number of clusters are high (8, 10), the image becomes too detailed, with many contrasting colors and if we look closely at the histograms the peaks located quite close to one another. This creates the issue of multiple clusters having similar colors. This is not ideal as we need the cluster which contains pixels of the veins to be separated from the rest. The ideal situation is when one cluster (to which the veins belong) is further as possible from the rest and the number of clusters need to be moderate. Hence we came to the conclusion that the amount of clusters to use for a given image, is a number between 4 to 6.
 Performance testing after multi-threading
 As mentioned under implementation the addition of multiple threads to achieve parallel processing of a frame increases the performance of the algorithm significantly. To explain the improvement we’ve obtained the processing times of frames in both (single threaded and multi threaded) implementations.
 There is a clear difference between the processing times of the two methods. Based on these values, the mean processing time for a frame while using multiple threads is around 27ms (37FPS) having a clear margin over the mean time of the basic implementation which is around 130ms (8FPS). It is apparent that the multi-threaded application is almost 5 times faster that the basic one. This made a huge difference in the overall delivery of the output and enhanced the performance of the algorithm substantially. If the number of cores in the microprocessor had been larger, then the results would be even greater.
 Video demonstration can be seen below.
 Conclusion and Future Work
 We have created a prototype for image capturing and perform preprocessing on the image.We have constructed an algorithm to obtain an output that shows the vein patterns and have optimized the algorithm to provide a clearer and more accurate depiction of the vein map. Considering the hardware implementation, we have developed a prototype by embedding the camera module, near-infrared light sources, single-board computer, and the display screen. The device is ready for clinical testing that unfortunately could not be completed due to the Ethical clearance process that took longer than expected given the pandemic situation of the country.
 In the future, when the external parties permit, we plan to carry out the planned clinical testing and evaluation and thereby further improve the algorithms and fine-tune the device according to the feedback we get.
 We also will extend the research further, to identify Peripheral Vascular Diseases (PVD) based on the data collected through clinical trials, in the future.
 Publications
 Semester 7 report
 Semester 7 slides
 Semester 8 report
 Semester 8 slides
 Author 1, Author 2 and Author 3 “Research paper title” (2021). PDF.
 Links
 Project Repository
 Project Page
 Demo Video
 Department of Computer Engineering
 University of Peradeniya
 Back to top"," Near-Infrared spectroscopy is used for better vein visualization to make the venipuncture process more efficient. It is usually a painful and time-consuming process and its difficulty increases with the dark skin tones, obesity of patients, and several other health and physical conditions.  Although near-IR spectroscopy is identified as a good solution, its usage is limited to availability, high cost, and doubts about accuracy. The cost of each of these products is very high and not affordable especially in countries like ours. We have implemented a mechanism for better accuracy and lower risk, using an illumination system that favors Asian skin types, while reducing the cost of the device so that it is more affordable especially for common clinical use in countries like ours.",E15,Undergraduate Research Projects,https://projects.ce.pdn.ac.lk/4yp/e15/nearIR-spectroscopy/,https://github.com/cepdnaclk/e15-4yp-nearIR-spectroscopy,https://cepdnaclk.github.io/e15-4yp-nearIR-spectroscopy,https://cepdnaclk.github.io/e15-4yp-nearIR-spectroscopy/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/4yp/E15/nearIR-spectroscopy/
99,online proctoring system,"Projects | Department of Computer Engineering
 Projects - Department of Computer Engineering
 University of Peradeniya, Sri Lanka
 Visit Department Site
 Online Exam Proctoring System
 Team
 E/15/138, Mohamed Irfan, irfanmm96@gmail.com
 E/15/021, Mohamed Aslam, e15021@ce.pdn.ac.lk
 Supervisors
 Dr. Ziyan Maraikar, ziyanm@eng.pdn.ac.lk
 Dr. Upul Jayasinghe, upuljm@eng.pdn.ac.lk
 Mr. Mohamed Fawzan, fawzanm@gmail.com
 Project Repository
 Project Page
 Department of Computer Engineering
 University of Peradeniya
 Back to top","Online examinations require human invigilation either by live monitoring or inspection of recorded video to ensure academic integrity. This process is not feasible always since exams can be taken at any time and it involves high cost.  This project proposes an Online Proctoring System to automate invigilation processes by making use of inputs from a web browser, without using any external hardware or standalone application. As a result, several misconducts during the examination can be identified and labelled as suspicious.",E15,Undergraduate Research Projects,https://projects.ce.pdn.ac.lk/4yp/e15/online-proctoring-system/,https://github.com/cepdnaclk/e15-4yp-online-proctoring-system,https://cepdnaclk.github.io/e15-4yp-online-proctoring-system,https://cepdnaclk.github.io/e15-4yp-online-proctoring-system/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/4yp/E15/online-proctoring-system/
100,sports action recognition,"Projects | Department of Computer Engineering
 Projects - Department of Computer Engineering
 University of Peradeniya, Sri Lanka
 Objectively Measure Player Performanceon Olympic
 Weightlifting
 Team
 E/15/154, Chamin Jayasooriya, email
 E/15/179, Anandi Karunaratne, email
 Supervisors
 Dr. Rajitha Navarathna, email
 Mr. Sampath Deegalla, email
 Table of content
 Abstract
 Related works
 Methodology
 Experiment Setup and Implementation
 Results and Analysis
 Conclusion and Future Work
 Links
 Abstract
 This project introduces a novel method to measure the quality of the actions performed in Olympic weightlifting using human action recognition in videos. Human action recognition is a well-studied problem in computer vision and on the other hand action quality assessment is researched and experimented comparatively low. This is due to the lack of datasets that can be used to assess the quality of actions. In this research, we introduce a method to assess player techniques in weightlifting by using skeleton-based human action recognition. Furthermore, we introduce a new video dataset for action recognition in weightlifting which is annotated to frame level. We intended to develop a viable automated scoring system through action recognition that would be beneficial in the sports industry.
 Related works
 Action Quality Analysis in Weightlifting
 A very limited amount of work has been conducted on action quality analysis in weightlifting. Chatzitofis et al. (2013) has analyzed the biomechanics of human body and how it is applied to weightlifting. They have also extracted the data that is important to a weightlifting coach. These data includes the position, the angle, and the velocity of the weightlifting bar, the initial angle of the athlete’s knee, and the start time and the end time of the lift. For this, they have used a kinetic sensor to get depth data.
 Skeleton based Action Classification
 Classifying Actions with a Skeleton based Approach has captured the attention excessively in the past years.
 Li et al. (2019) introduces a new module named, A-link inference module which is an encoder-decoder structure. It can capture richer dependencies that current models might miss since they only capture local physical dependencies among joints. A-link inference model captures two main dependencies. 1. Action links, which are action-specific dependencies; and 2. Structural links. Combining these two, actional-structural graph convolution networks (AS-GCN) is proposed.
 Skeleton based action recognition has difficulties such as limited expressive power and generalization. Yan et al. (2018) addresses this problem by introducing a novel model of dynamic skeletons called Spatial-Temporal Graph Convolutional Networks (ST-GCN). This model is able to learn spatio-temporal patterns automatically and capable of generalization. This is validated on Kinetics and NTU-RGBD datasets.
 Methodology
 Dataset
 Since there are no existing weightlifting datasets, we have created our own dataset consisting of weightlifting video clips.
 For our dataset, videos were collected from the official Olympic YouTube channel. The selected videos include both men and women categories and both snatch and clean and jerk categories. The long video footages were trimmed down so that they only contain weightlifting events where only the player and the barbell is seen. Then we annotated the actions in a frequency of 5 fps.
 Action Classes
 As shown in Table I we can identify 11 action classes in weightlifting. There are 1570 of total video frames in the dataset containing all the actions as described in the Table II.
 Testing Data
 As for the testing purposes, we have used videos specifically with the medalists of different weight categories in 2016 Olympics as shown in the Table III.
 By using these data we have assumed that their techniques are more accurate. Hence we have used them for deriving optimal values for initial knee angle, bar angle
 which are used for our scoring model.
 Approach
 As we discussed earlier, we used the skeleton based approach to recognize the actions.
 The following figure shows the basic workflow of extracting and identifying actions done by humans in a video. For this, we have identified a suitable framework which estimates the poses of humans and represent using a skeleton showing joints of the human body. This framework is Openpose (references: Cao et al. (2019), Simon et al. (2017), Cao et al. (2019), Wei et al. (2016), Feiyu Chen). It was initially developed by researchers at Carnegie Mellon University. We have trained this model with our dataset in order to identify and classify actions.
 Fig.1 - Action Recognition Workflow
 After pose estimation is done, the features are extracted from the skeleton.
 We have identified that the position, the angle, and the velocity of the weightlifting bar, the initial angle of the athlete’s knee as well as the start time and the end time of the attempt are important for a weightlifting coach (references: Weightlifting Equipment and History - Olympic Sport History, Chatzitofis et al. (2013)). We have used the coordinates of the joints in the 2D image plane that are obtained from our model to calculate those information. The joints that are identifiable are shown in the below figure.
 Fig.2 - Joints identified using Openpose
 In simple terms, a weightlifting attempt is considered as a successful lift if a lifter can lift the bar above him and keep it balanced for a time. Therefore tracking the position, angle, and the velocity is important for a coach.
 Usually a player balances the bar symmetrically when he is lifting. Therefore we can assume that the player’s hands are at the same length from the center of the bar. Therefore, weightlifting bar position is calculated using joint 4 and joint 7.
 The angle between the bar and horizontal axis is important, because if the angle is too wide, it is harder for the player to balance the bar and it is likely the attempt will be unsuccessful. For this calculation we will be using the same joints we used for position calculation; 4 and 7.
 Velocity of the bar is another important factor for a coach. When a player lifts a bar, there is work (W) being done. (W) equals to F.d, where F is force given, and d is the distance moved. And as we know power (P) equals W/t where t is the time. Which gives us the following,
 where v is the velocity. While lifting, a player must give power as needed, not more or less. For velocity calculation, we will consider the bar positions of the current frame and the previous frame.
 The knee angle in the initial set up position is crucially important in weightlifting, because the success of the lift will depend on that. To calculate the initial knee angle, we will be using the coordinates of 11, 12, and 13 joints.
 Here, a is the vector passing through joint 11 and 12, and b is the vector passing though joint 12 and 13.
 We propose a scoring model where we can get a proper understanding of the accuracy of the techniques of the player. We assigned weights for the features we have extracted based on their importance and evaluate the accuracy of their action.
 For this, we followed the pseudo code described below in Algorithm.
 And the expected structure of the result is as follows.
 Overall score for the action
 Individual scores relevant to sub actions
 Details of the score calculation (which and how features affected to score
 Implementation
 We set up the environment on a high-performance server that has an Intel Xeon E5-1630 v3 CPU and an NVIDIA GK180GL [Tesla K40c] GPU
 to train the action recognition model.
 As we discussed earlier, for skeleton detection, Openpose framework was used, which is the Tensorflow variant of the initial Openpose framework. As mentioned, the skeleton detection library is built upon Tensorflow. For drawing skeletons and bounding boxes around human figures, OpenCV and Matplotlib libraries were used.
 The Openpose framework provides two models for skeleton detection; CMU and Mobilenet-thin. CMU model detects skeletons more accurately but at a slower speed when it is compared to Mobilenet-thin. For our experiments, we used the CMU model. The image (pre-processed video frames) resolution is 656x368 pixels. (Higher the input images’ resolution, higher the accuracy of the skeleton detection.)
 Then the features were extracted and pre-processed, and then the classification was done by MLPClassifier from Scikit-learn which is a neural network model. After this step, we have recognized actions and skeleton data of the player for each frame. Using these two information, we calculated angles and other required features for the technique evaluation algorithm.
 With those required parameters calculated, the action quality assessment can be done.
 Experiment Setup and Implementation
 For our final experiments and demonstrations, we used Openpose framework. In the first setup, Openpose is used to get the positions of the joints. Each person is tracked by calculating the Euclidean distance between the joints of two skeletons and matching two skeletons. Missing joints are filled using the joints’ relative position in the previous frame. Noise was added to the joint positions to try to augment data. A window size of 0.5s (5 frames) was used to extract features. The features of Body velocity, Normalized joint positions, and joint velocities were extracted. Then PCA was applied to reduce the feature dimension to 80. After that, it was classified by DNN of 3 layers of 50x50x50.
 After that, recognized actions for each frame is used to evaluate player techniques. Python, OpenCV and other helper libraries can be used to achieve this. Once the action for each frame of the video is identified, we evaluated techniques according to those identified actions. For example, if it is the initial position then we do not want to evaluate the player’s arms angles. In that position, we would rather need to know about knee angles. Therefore, we programmed our scoring algorithm in such a way where player’s technique is evaluated relevant to the current action. As mentioned in the Algorithm, we estimated a score for the quality of the action. If the player performs actions accurately in pre-defined techniquewise, then the player will get a higher score. At the end of the video, the player will get an overall score which is the weighted average of the scores earned for each sub-action.
 Data manipulation and Testing
 The video datasets were manipulated mainly by FFMPEG tool and OpenCV framework. For the training purposes, videos were collected from YouTube and some of the data were already collected by previous authors. The video data is preprocessed by trimming the videos (removing unnecessary parts of the video that are irrelevant to the target action) by using FFMPEG.
 Then the trimmed videos were converted into sequences of images using OpenCV and other python libraries such as NumPy, itertools, simplejson, etc. After that, for skeleton detection, Openpose framework was used, which is the Tensorflow variant of the initial Openpose framework. As mentioned, the skeleton detection library is built upon Tensorflow. For drawing skeletons and bounding boxes around human figures, OpenCV and Matplotlib libraries were used.
 The Openpose framework provides two models for skeleton detection; CMU and Mobilenet-thin. CMU model detects skeletons more accurately but at a slower speed when it is compared to Mobilenet-thin. For our experiments, we used the CMU model. The image (preprocessed video frames) resolution is 656x368 pixels. (Higher the input images’ resolution, higher the accuracy of the skeleton detection.)
 Then the features were extracted using Numpy and then the classification was done by MLPClassifier from Scikit-learn which is a neural network model. Then again the sequence of images was converted into the output video with labeled actions by OpenCV, Numpy, and other Python libraries.
 Pitfalls and workarounds
 Since we have been working on a CLI environment, getting video inputs and outputs to the local GUI cannot be done directly. Because of this, we could not test these frameworks using a webcam for live-action recognition. To overcome this issue, we have used Google cloud APIs (Drive API and YouTube Data API) and SSH, SCP protocols to manipulate data on the server.
 Further, the OpenPose framework does not perform well with the videos that include truncated and occluded human figures, rapid variations of camera angles, scale of the viewpoint and colour spectrum, brightness etc. Therefore to avoid erroneous results that would happen due to such reasons, we created our Weightlifting Dataset in which the video data does not have such properties.
 Since we do not estimate the angles of body joints in 3-dimensional space, the calculated angles may vary from the real values. But what we have done is using those angles to estimate a score which is a reasonable interpretation of the estimated angles. People can use that scores to evaluate players rather than raw angle values. To calculate the scores, we followed an experimental approach where we had to change parameters of the scoring algorithm many times to obtain an acceptable values for the scores.
 Results and Analysis
 Action Recognition Results
 Results of the action recognition model are shown in below Figure 3 and Table IV. We can see that the accuracy on test data is over 93\%. This is due to the video dataset we chose for this task, which contains favourable properties for our action recognition model. By choosing training data in which human body occlusion, truncation are very minimum and other properties such as uniform scale, uniform brightness/contrast distribution, the human skeleton detection accuracy must have been improved. The dataset contains single person video frames and the background clutter is almost none.
 Fig.3 - Confusion Matrix for the Results of the Action Recognition Model
 Table IV - ACCURACY REPORT OF THE ACTION RECOGNITION MODEL
 Action Quality Assessment Results
 According to the algorithm we described in the above algorithm we assessed the whole lift and each sub-action. To get optimal values for comparison, we considered a video set of Olympic champions of the 2016 Olympics as the baseline data as shown in Table VI. Table V
 includes the player names with respect to the video sample number. Results obtained from those videos are shown in Figure .
 Table V - Sample Video Number and Relevant Player
 Table VI - Optimal values for Scoring Algorithm
 Fig.4 - Distribution of Angles in Olympic Medalists’ Videos Which Were Used to Determine Optimal Values for Scoring Algorithm
 Scoring Algorithm
 Using the values in Table VI we updated our scoring algorithm, and we applied it to calculate the scores of our previous dataset.
 Fig.5 - Initial knee angle and Obtained score distribution
 Fig.6 - Bar angle and Obtained score distribution
 In Figures 5 and 6 we can see how the score decreases with the deviation from the optimal value.
 The comparison of the scores of Olympic medalists’ videos and our dataset is given in Figure 7 and Figure 8.
 Fig.7 - Leg, Arm, Knee angle, Bar angle, and Overall scores of Olympic medalists
 Fig.8 - Leg, Arm, Knee angle, Bar angle, and Overall scores of the dataset
 By analyzing the overall scores for the weightlifting videos, we can see that our scoring algorithm has assigned scores for players’ actions as we expected with defined optimal values and weights for each sub-action. One can always re-configure the scoring algorithm values to determine what is best for their own evaluation. Here we have attempted to demonstrate the capability of our proposed method to evaluate players based on their action quality.
 As we have already mentioned, for a coach the velocity of lifting the bar of each player is important. We have used a graphical way to show the coach how the bar moves vertically, with respect to the frames in the video footage. (Figure 9 and 10)
 Fig.9 - Bar movement - Snatch
 Fig.10 - Bar movement - Clean and Jerk
 Figure 11 shows the differences between the bar movements of the snatch lift and the clean and jerk lift.
 Figure 12 compares the bar movements of two different Olympic players. The idea here is for a coach to get a general idea about the player. Therefore we did not focus on converting the data into metric units. Sport-persons can compare the bar movement with other players or with their previous lifts to improve their lifts. Hence this can be used as an assistive tool for training.
 Fig.11 - Comparison of bar movement - Snatch and Clean and Jerk
 Fig.12 - Comparison of bar movement - Snatch Men 94 kg and Snatch women 75 kg
 Conclusion and Future Work
 We have introduced a way to analyze the quality of a weightlifting player performance using just a video footage as the input. For this purpose we have identified the actions performed, extracted the required features for the relevant actions, and evaluated the quality of the action using those features.
 We have used Openpose, which is a 2D pose estimation model using skeleton based approach as our model. The model was trained using a weightlifting dataset created by us.
 The parameters for automatic action quality assessment algorithm can be further fine-tuned by following an empirical approach by involving sportspeople.
 Our intention was to develop a solution that can be used by anyone with a digital camera. Hence, the calculated angles were not accurate for direct usage. Another approach to address this problem is to use a 3D pose estimation model or to simultaneously process two videos obtained from the front angle and side angle, but that would limit the usage from the regular users.
 Links
 Project Repository
 Project Page
 Department of Computer Engineering
 University of Peradeniya
 Back to top","This includes a novel method to measure the quality of the actions performed in Olympic weightlifting using human action recognition in videos. Human action recognition is a well-studied problem in computer vision and on the other hand action quality assessment is researched and experimented comparatively low. This is due to the lack of datasets that can be used to assess the quality of actions. In this research, we introduce a method to assess player techniques in weightlifting by using skeleton-based human action recognition. Furthermore, we introduce a new video dataset for action recognition in weightlifting which is annotated to frame level. We intended to develop a viable automated scoring system through action recognition that would be beneficial in the sports industry. ",E15,Undergraduate Research Projects,https://projects.ce.pdn.ac.lk/4yp/e15/sports-action-recognition/,https://github.com/cepdnaclk/e15-4yp-sports-action-recognition,https://cepdnaclk.github.io/e15-4yp-sports-action-recognition,https://cepdnaclk.github.io/e15-4yp-sports-action-recognition/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/4yp/E15/sports-action-recognition/
101,CUDA_Fast_ICA,No project page,CUDA implementation of Fast-ICA in C++,E10,Undergraduate Research Projects,https://projects.ce.pdn.ac.lk/4yp/e10/CUDA_Fast_ICA/,https://github.com/cepdnaclk/e10-4yp-CUDA_Fast_ICA,#,#,http://api.ce.pdn.ac.lk/projects/v1/4yp/E10/CUDA_Fast_ICA/
102,building a testbed for power analysis attacks,No project page,The A to Z of Building a Testbed for Power Analysis Attacks,E10,Undergraduate Research Projects,https://projects.ce.pdn.ac.lk/4yp/e10/building-a-testbed-for-power-analysis-attacks/,https://github.com/cepdnaclk/e10-4yp-building-a-testbed-for-power-analysis-attacks,#,#,http://api.ce.pdn.ac.lk/projects/v1/4yp/E10/building-a-testbed-for-power-analysis-attacks/
103,HBFT,No project page,A new Bredth First Traversal algorithm for large graphs ,E11,Undergraduate Research Projects,https://projects.ce.pdn.ac.lk/4yp/e11/HBFT/,https://github.com/cepdnaclk/e11-4yp-HBFT,#,#,http://api.ce.pdn.ac.lk/projects/v1/4yp/E11/HBFT/
104,Temporal Attention Based MARL,No project page,,E14,Undergraduate Research Projects,https://projects.ce.pdn.ac.lk/4yp/e14/Temporal-Attention-Based-MARL/,https://github.com/cepdnaclk/e14-4yp-Temporal-Attention-Based-MARL,#,#,http://api.ce.pdn.ac.lk/projects/v1/4yp/E14/Temporal-Attention-Based-MARL/
105,dark arts algorithms for low light image enhancement and interpretation,No project page,Dark arts: Algorithms for enhancement and interpretation of low light images,E14,Undergraduate Research Projects,https://projects.ce.pdn.ac.lk/4yp/e14/dark-arts-algorithms-for-low-light-image-enhancement-and-interpretation/,https://github.com/cepdnaclk/e14-4yp-dark-arts-algorithms-for-low-light-image-enhancement-and-interpretation,#,#,http://api.ce.pdn.ac.lk/projects/v1/4yp/E14/dark-arts-algorithms-for-low-light-image-enhancement-and-interpretation/
106,f5n,No project page,Genopo a.k.a. F5N - A nanopore sequencing analysis toolkit for Android smartphones https://nanoporetech.com,E14,Undergraduate Research Projects,https://projects.ce.pdn.ac.lk/4yp/e14/f5n/,https://github.com/cepdnaclk/e14-4yp-f5n,#,#,http://api.ce.pdn.ac.lk/projects/v1/4yp/E14/f5n/
107,f5n_server,No project page,This is a job server to distribute and keep track of f5n pipeline jobs to connected f5n clients,E14,Undergraduate Research Projects,https://projects.ce.pdn.ac.lk/4yp/e14/f5n_server/,https://github.com/cepdnaclk/e14-4yp-f5n_server,#,#,http://api.ce.pdn.ac.lk/projects/v1/4yp/E14/f5n_server/
108,ipb,"IPB
 Automation of Intelligence Preparation of the Battleﬁeld
 August 2019 – July 2020
 Final Year research project - Group
 Python
 Flask
 Algorithm Development
 JavaScript
 Web Application Development
 Download Project Report With Full Details
 Abstarct
 In military operations armed forces have to get a better idea of the area in
 which they have to operate including terrain features, threats and avenues of
 approach.
 So they gather intelligence on the location, enemy, weather, vegetation,
 infrastructure and many such factors before making decisions. Intelligence
 Preparation of the Battleﬁeld (IPB) is the name given for that process of
 analyzing the situation and making decisions based on predictions. Usually this
 process happen manually by ofﬁcers using hard copy maps and it have number of
 inconveniences. In this paper, we implement and present a set of algorithms,
 tools and approaches for automating terrain analysis and decision making in
 Intelligence Preparation of the Battleﬁeld process and compare their results
 with a manual analysis.
 Team
 Hishan Indrajith
 Sathyanga Tharana
 Jayamal Jayamaha
 Project Supervisors
 Dr. Isuru Nawinne - Department of Computer
 Engineering, University of Peradeniya
 Dr. Janaka Alawatugoda - Department of Computer
 Engineering, University of Peradeniya
 Introduction
 IPB is a process that starts in advance of operations and continues during
 operations planning and execution. It provides guidelines for the gathering,
 analysis, and organization of intelligence. The purpose of this intelligence is
 to inform a commander’s decision process during the preparation for, and
 execution of a mission. Therefore IPB is a Command and staff tool which allows
 systematic and continuous analysis of the enemy and the battleﬁeld environment.
 It presents the results of the process in a graphical format. It is an
 integrated method of analysing Enemy, Ground and Friendly Forces factors in the
 Estimate.
 Basically there are four steps in IPB process. They are,
 Deﬁne the battleﬁeld environment
 Describe the battleﬁeld’s effects
 Evaluate the threat
 Determine threat COAs
 The resulting product of IPB is identiﬁcation of various areas of the battleﬁeld
 that affect Courses of Action (COAs). The four distinctive courses
 of action are,
 engagement areas
 battle positions
 inﬁltration lanes
 avenue of approach
 Any force that has the control of the key terrain has the military advantage.
 Key terrain areas cannot be deﬁned by geographical features alone. The
 evaluation of terrain features must be fused with information about weather,
 enemy asset types, friendly and enemy range of ﬁre, enemy doctrine and type of
 operation.
 The problem with current process is that IPB is done manually by intelligence
 ofﬁcers using hard copy maps on which they anotate various signiﬁcant areas,
 such as key terrain or defensible terrain. This manual process suffers from a
 number of inefﬁciencies as described below.
 No variable zooming in and out to obtain desired level of
 detail
 Annotating the maps is time consuming.
 Notations on maps get cluttered with the risk of being
 misread.
 Information could be disregarded or not used effectively in
 the process of the IPB.
 What We Did
 The research was basically spllited in to two major sections such that each
 section contain three milestones. The two sections was,
 Visual Support for Automating the Intelligence Preparation
 for Battlefield (IPB) Process
 Implement Automation of Intelligence Preparation for
 Battlefield
 So the six milestones for the project was,
 Web-based platform to display overlays on a map.
 Infrastructure to efficiently store data for overlays.
 Integrating the data storing mechanism with graphical user
 interface.
 A grid based combined obstacle overlay by collecting the
 vector overlays to a grid
 Generating the potential mobility corridors in the
 terrain
 Risk evaluation of corridors to predict the avenues of
 approach and key areas.
 Web-based platform to display overlays on a map
 As the IPB need a visual tool that allows military staff to add battlefield data
 in to the system and also visualize them as overlays, we needed to firstly
 develop a web based platform to add overlays and visualize them. So we firstly
 researched about a framework that we can use to do the map based functions.
 Simply from front-end side the application should work like a GIS software.
 Following technologies were chosen by us to be used fro the web platform.
 Leafletjs – Leaflet is the
 leading open-source JavaScript library for mobile-friendly interactive maps.
 Open
 street Maps – OpenStreetMap is a free editable map of the whole world
 that is being built by volunteers largely from scratch and released with an
 open-content license.
 Infrastructure to efficiently store data for overlays
 We needed to find a data storing mechanism and also a data format to store
 the overlay data. As the data in overlay are spatial data with attributes,
 We researched about the available methods to store such data.
 So the available options to store those data were using a vector
 format or a raster format.
 So as our web application was JS based, we choose GeoJSON
 which is a format for encoding a variety of geographic data structures.
 To store and provide the required overlay information relevant to
 battlefields, there should be a back-end application. As our future
 algorithms and models are based on python, we used Python
 Flask as the web framework for our back-end and the we decided to use
 REST architecture to build the back-end web service.
 Following were the attributes we defined for our overlays
 Building
 No of occupants
 Status
 Material
 Building Type
 No of stories
 Vegetation
 Vegetation Type (grassland, shrubland, woodland, medium
 density forest, high denisty forest, unknown)
 Water
 Water body type (water, river, reservoir, dock,
 wetland, unknown)
 Mark known points of shallow or deep
 Roads
 Road type (tertiary, track, unclassified, secondary,
 trunk, primary, motorway_link, trunk_link, primary_link, road,
 secondary_link, tertiary_link, motorway)
 Elevation
 Elevation : float value
 Integrating the data storing mechanism with graphical user interface
 Finally we had to integrate the back-end we developed using the data storing
 mechanism and data retrieving mechanisms with the front-end developed with
 map overlays
 So in our first section of the project, we implemented the web application
 tool to perform following major tasks.
 Create and save multiple battlefields(maps).
 Automatically generate the buildings, water, roads,
 elevation, vegetation overlays when a new battlefield is created.
 View a battlefield on user interface graphically with a
 map (Satellite or Topographical)
 View the overlays generated for the battlefield
 graphically on the map separately.
 Add new buildings, water bodies, vegetation areas,
 roads on the battlefield using a drawing tool
 Add values for the defined attributes of the newly
 drawn shape.
 Edit values of attributes of automatically generated
 geographical features.
 Remove geographical features of overlays.
 Save changes to be able to access later.
 All the information are stored in the backend.
 Following images are few screenshots from the tool.
 Creating the battlefield
 Generated overlays added on the map
 Adding data to an geographic feature
 Save the data insertion
 The architecture implemented for the system was basically a 3-Tier
 Architecture. Presentation layer being our web tool using LeafletJS,
 Application layer being the python web application using Flask and use REST
 web services to communicate with Presentation layer.
 Data layer is the filesystem which stores GeoJSON files in a hierarchical
 structure.
 Following diagram is the system architectural diagram.
 The auto generation of overlays happen in IPB Service Layer, where the
 available geographical data for Sri Lanka stored in the server are processed
 in order to produce the overlays of the given boundaries.
 We have obtained relevant digital geographical data for Sri Lanka and
 pre-processed them to suit the overlays we are considering.
 The Elevation data for Sri Lanka have been obtained from highest-resolution
 topographic data generated from NASA's Shuttle Radar
 Topography Mission (SRTM). We generated the island wide 25m contour
 lines using that DEM data and that is used for creating elevation overlay.
 Also we stored the raster DEM file in server for some other functions
 including trafficability calculation.
 OpenStreetMap data for Sri Lanka were obtained from https://download.geofabrik.de/asia/sri-lanka.html and
 processed to obtain overlay data for Sri Lanka.
 OSM Land Use data was used to obtain vegetation overlay
 by filtering vegetation and mapping their properties to our defined
 attributes.
 OSM Building data was processed to get building overlay
 such that their properties mapped into our defined building
 attributes.
 OSM water data was coverted into water overlay
 OSM road data was converted in to road overlay.
 A grid based combined obstacle overlay by collecting the vector overlays to
 a grid
 As we have built the overlays using a vector format with properties, we
 needed to convert those data overlays to grids of their properties as grid
 based analysis is used for the processing. We started from the elevation
 raster file of Sri Lanka obtained from SRTM dataset. In our program to get
 the combined obstacle overlay first step was to get the elevation grid. So
 our program was added the functionality to clip the Sri Lanka elevation
 raster file to the size of the battlefield firstly.
 The NASA’s Space Shuttle Radar Topography Mission (SRTM) DEM data's
 resolution is about 30 meters. It has pixels (cells) of grid approximately
 30m containing elevation data. See the image below.
 We needed to map these elevation data to a grid of cells of size 10 times
 smaller than SRTM data resolution for better accuracy as 30m is not a good
 resolution for ﬁnding mobility. So elevation data graph was resampled using
 bi-linear interpolation in order to reduce the resolution of the overlay
 grid size to about 3 meters. The elevation data raster overlay after
 resampling is shown in below.
 So the other overlay grids was also to be built to the same shape of the
 elevation grid obtained, such that they can be put one on other.
 So next from the elevation grid, an additional grid of slope was derived.
 The slope grid is produced such that slope at grid cell (x,y) is assigned
 the mean of the slope between (x,y) and each of the surrounding grid cells.
 Following Figure shows the generated slope overlay for above elevation
 example.
 Rasterization techniques were used to get the rater images of the building,
 water, road and vegetation overlays preserving their properties and those
 raster images of the overlays were converted to a numpy array for our
 processing. Following images will show the original map, building grid,
 water grid, vegetation grid and road grid obtained using our program
 respectively.
 Our target in this milestone was to obtain combined obstacle overlay by
 combining all these overlays in a suitable way to achieve our goal. So we
 constructed an overlay called trafficability grid combinning all those
 overlays (elevation, slope, building, vegetation, water, roads)
 Trafficability grid is a grid witch has cells representing squares on land,
 where each grid cell represent the trafficability of the cell. In another
 way each cell give a value defining how much it is difficult to troop
 maneuver withing that cell.
 We considered the electric flow model as a foundation of our algorithm to
 get trafficability grid. In electric current point of view, the electric
 current or the flow of electrons is determined by the resistance of the
 medium. The resistance is determined by the resistivity of the materials
 used in the medium.
 If the resistance per unit length is k, the resistance of
 l length medium becomes k x l.
 So for each property that we consider that would effect trafficability from
 the overlays, we defined a value denoting resistance per distance for troop
 maneuver. So the total resistance per distance for a given grid cell is the
 sum of all resistances per length of properties that belong to that cell.
 So the pseudocode for algorithm used in obtaining the trafficability using
 the resistance
 model is given below.
 function trafficability(coo):
 create empty grid trafficability
 elevation_min = minimum(coo.elevation)
 for each cell in coo:
 slope = cell.getSlope()
 isBuilding = cell.isBuildingHere()
 isWater = cell.isWaterHere()
 isRoad = cell.isRoadHere()
 vegetationLevel= cell.vegetation()
 relative_elevation = cell.getElevation() - elevation_min
 isBridge = isWater and isRoad
 resistivity_of_cell = relative_elevation
 if slope > max_slope_threshold:
 resistivity_of_cell = resistivity_of_cell + resistivity_heavy_slope
 if isRoad:
 resistivity_of_cell = resistivity_of_cell + resistivity_road
 else if isBridge:
 resistivity_of_cell = elevation + resistivity_bridge
 else if isBuilding:
 resistivity_of_cell = resistivity_of_cell + resistivity_building
 else if isWater:
 resistivity_of_cell = resistivity_of_cell + resistivity_water
 else if vegetationLevel == grassland
 resistivity_of_cell = resistivity_of_cell + resistivity_vegetation_grassland
 else if vegetationLevel == shrubland
 resistivity_of_cell = resistivity_of_cell + resistivity_vegetation_shrubland
 else if vegetationLevel == woodland
 resistivity_of_cell = resistivity_of_cell + resistivity_vegetation_woodland
 else if vegetationLevel == medium density forest
 resistivity_of_cell = resistivity_of_cell + resistivity_vegetation_medium_density_forest
 else if vegetationLevel == high density forest
 resistivity_of_cell = resistivity_of_cell + resistivity_vegetation_high_density_forest
 else if vegetationLevel == unknown
 resistivity_of_cell = resistivity_of_cell + resistivity_vegetation_unknown
 else:
 resistivity_of_cell = resistivity_of_cell + resistivity_vegetation_empty
 update corresponding cell in trafficability grid with resistivity_of_cell
 return trafficability
 So for the operation of this algorithm, we defined few attributes that
 describe the resistivity per length for different terrain features as below.
 max_slope_threshold = 0.4
 resistivity_vegetation_grassland = 30
 resistivity_vegetation_shrubland = 100
 resistivity_vegetation_woodland = 200
 resistivity_vegetation_medium_density_forest = 400
 resistivity_vegetation_high_density_forest = 600
 resistivity_vegetation_unknown = 200
 resistivity_vegetation_empty = 65
 resistivity_building = 1000
 resistivity_road = 1
 resistivity_bridge = 1
 resistivity_water = 10000
 resistivity_heavy_slope = 800
 These attributes were given assumed values based on the mobility in each
 situation.
 Generating the potential mobility corridors in the terrain
 So next we moved to generating potential mobility corridors that troops can
 move from a given starting point to an destination. The trafficability grid
 that was generated in last milestone, was used in determining the mobility
 corridors, or the avenues of approach. Trafficabilty grid represent a
 relative cost or a resistance of moving per a unit length, for each cell in
 grid. Here unit refer to width of a cell in the grid.
 To generate the potential mobility corridors, we experimented three
 approaches. Those were,
 Generalized Voronoi Diagram Method
 k-shortest paths algorithm
 Dijkstra's based path removing algorithm
 Generalized Voronoi Diagram Method
 The voronoi diagram method was to get mobility corridors from a voronoi
 diagram drawn for a GO-NO terrain map generated from trafficability
 grid.
 an example voronoi diagram
 Let P = {p1,p2,…,pn} be a set of n
 distinct points or sites in the plane. The Voronoi diagram of P is the
 subdivision of the plane into n cells, one for each site in P, with the
 property that a point q lies in the cell corresponding to a site pi if and
 only if dist(q, pi) < dist(q, pj) for each
 pj ∈ P with j ≠ i. If the sites are replaced
 with polygons, the above deﬁnition holds true with a more complex distance
 function that represents the minimum distance between a point and a polygon
 in the plane. Such a diagram for polygons instead of points is called the
 Generalized Voronoi Diagram (GVD). This can help to ﬁnd avenues of approach,
 and other important tactical features of terrain.
 Though the optimized algorithm for voronoi diagram is Fortune's algorithm with time complexity O(n log
 n), as we need to get the Generalized Voronoi Diagram for polygons, we
 used the basic algorithm with O(n2) for that.
 Following is the pseudocode for the generation of generalized voronoi
 diagram from GO NO-GO terrain grid.
 function voronoi(go_no_go_grid):
 create new grid border_grid
 for each no_go cell in go_no_go gird:
 if any neighbor cell is a go cell:
 mark cell as a border in border_grid
 create an array of array of cells (say cell_families) to store connected cells separately
 using a connected cell algorithm add connected cells to cell_families
 depth_map = grid of size go_no_go_grid
 color_map = grid of size go_no_go_grid
 put infinity to all cells in depth_map
 put zero to all cells in color_map
 family_id = 0
 for each family in cell_families:
 increment family_id by 1
 create a go_no_go grid sized grid which contain minimum geometric distance of each cell from the cells in the family, say it distance_map
 update the color_map, with family_id, only the cells where distance_map value < depth_map value
 update the depth_map , with distance_map value, only the cells where distance_map value < depth_map value
 create new grid voronoi_grid
 for each cell in color_map:
 if any neighbor cell is not equal to cell value:
 mark cell as a voronoi grid in voronoi_grid
 return voronoi_grid
 Following is the voronoi diagram resulted for a given battlefield.
 GVD drawn to the battlefield without restricted terrain(left) and with restricted terrain(right)
 So in this diagram is a
 network of paths, which gives many paths that avoids restricted NO-GO
 areas. Each edge of the voronoi graph corresponds to a path between two
 restricted NO-GO features. So basically voroni diagram gives an abstract
 set of paths that one can go avoiding only NO-GO areas. So we can
 select set of routes that join two positions from the network as below.
 set of paths selected using GVD
 But the problem in this
 method is that only the restricted terrain is considered for path
 generation. the other costs of mobility like cost from elevation,
 vegetation, roads, slope is not considered as the trafficability grid is
 mapped to a binary grid of GO, NO-Go and used here. So the accuracy is
 low as many data are not used.
 Considering the time
 complexity of the algorithm, the algorithm we used for generating this
 generalized voronoi algorithm has time complexity O(n2), assuming the NO-GO feature density is linearly proportional to number of cells(n) of a battlefield grid.
 We created 6 sample
 battlefields with different sizes in the same location, where it can be
 assumed as a uniform restricted terrain is there. Then we used the
 algorithm to compare time take for each.
 Following are the 6 sample battlefields created.
 See the times taken for the algorithm for different size grids below.
 The graphical representation
 of time taken for voronoi diagram vs number of cells is below.
 k-shortest paths algorithm
 The k shortest path routing
 problem is a generalization of the shortest path routing problem in a
 given network. It asks not only about a shortest path but also about
 next k−1 shortest paths (which may be longer than the shortest path).
 Our approach of finding k shortest paths in trafficability grid was
 actually finding the lowest cost paths, as the grid contain the cost
 values. We approached the k shortest paths problem by extending Dijkstra
 algorithm. We have to give start and end locations to find paths here
 first.
 Following is the pseudocode
 for the generation of paths using k-shortest paths algorithm from
 trafficability terrain grid.
 function kshortest(trafficability_grid, start_cell, end_cell):
 count = grid of zeros of size trafficability grid
 temp_path_list = queue to store temporary paths
 final_path_list = queue to store final paths
 add start cell to temp_path_list with cost 0
 while temp_path_list is not empty and count value for end_cell < k:
 current_shortest_path = get shortest path from temp_path_list
 remove current_shortest from temp_path_list
 current_destination = destination cell of current_shortest_path
 increment count value of current_destination by 1
 if current_destination == end_cell:
 add current_shortest to final_path_list
 if count value of current_destination <= k:
 for all neighbor cells of current_destination:
 current_path_copy = get a copy of current_shortest_path
 new_path = get_new_path(current_path_copy, r_change, c_change)
 let new_path be a new path formed by concatenating neighbor cell to current_shortest_path
 update cost of new_path by adding the cost of new part
 temp_path_list.append(new_path)
 return final_path_list
 So we generated 10 least
 cost paths taking k as 10, for the 6 sample battlefields marking their
 execution times.
 Following is an image of
 output paths obtained for battlefield 5.
 The problem in this result
 is that though there are several paths given as output in the result,
 they are actually represent a single path, just few small changes at few
 points are there. So those few changes make them the next least cost
 path. but there are not more different than the earlier path. But what
 we need is a set of paths that are different actually and go through a
 different area. So it is clear that k-shortest path algorithm doesn't
 give the best fit answer and we need not the next least cost paths, but
 the paths that are really different from others.
 The k shortest path problem
 is a problem where the complexity increases with k, number of cells, as
 well as distance between the two points given. For our time measurements
 we gave the start and end points as a corner to the middle of the
 battlefield approximately.
 Following is the table of times taken for the algorithm for different size grids.
 The graphical representation
 of time taken for k-shortest paths algorithm vs number of cells is
 below.
 Dijkstra's based path removing algorithm
 Our third approach was a
 dijkstra's algorithm based approach that include path segment removing
 and path correcting functions. Dijkstra's algorithm is a least cost or
 least distance finding algorithm between nodes in a graph, conceived by
 computer scientist Edsger W. Dijkstra in 1956.
 Basically for the
 trafficability grid, when performed Dijkstra's algorithm giving two
 points as start and end, outputs a path with the minimum cost, that one
 can go. But it just give one path and we need a set of different paths.
 After obtaining a shortest path, if we remove that path completely
 defining it as restricted, dijkstra's algorithm will next find another
 path that is completely independent from earlier path. They will not
 have common edges. But they can cross each other in places where both
 routes are in diagonal directions as in below figure.
 Then the set of paths give
 much different avenues of approaches as we need, but in cases where the
 route lie on common areas that both can use same path, that doesn't give
 the correct path and have multiple parallel paths. That happen as the
 paths coming next after the first path cannot use the same edges used by
 earlier paths. More importantly when the first path use a already
 available road in it, next path that need to use the road to some extent
 cannot use it and it will go in other areas close and parallel to road.
 See the below image that show the close and parallel path issue in this
 approach.
 Issue of having close and parallel paths (Left) and parts that need that issue to be corrected marked (Right)
 So a correction must be done
 to the close and parallel paths issue in common sections in routes. In
 the above image those places are circled with a red marker. So we
 developed a correction algorithm to correct that issue.
 When the paths generated
 were added to a grid, where cells belonging to paths have the cost
 defined in trafficability grid and other non path areas have a very high
 cost, the rasterized grid looks like below image.
 So in that grid view, the
 close and parallel, unwanted paths can be seen merged together as they
 are closer cells.
 Therefore in our correction
 algorithm, we made this grid and used dijkstra's algorithm again to this
 new grid to get least cost paths out of this faulty path set. Also this
 correction algorithm has a path section removing mechanism as well as a
 mechanism to identify which path section has to be removed before
 applying dijkstra's algorithm again to avoid resulting same path.
 In paths thickness is
 obtained for each cells using the number of surrounding path cells. When
 removing sections from the least cost path generated from new grid,
 first the segments with different thicknesses are splitted and we give
 priority to segments where, the path is thin (lowest wisth) and long
 (length with same thickness). Out of same width segments one with
 maximum length is chosen. Further the paths get splitted based on
 crossings as well, because in a crossing the number of surrounding path
 cells increase, hence taken as an increase of thickness.
 So following are the
 pseudo-codes for getting independent paths, path correction algorithm
 and obtaining sections to remove from paths respectively.
 Getting Independent Paths
 function independent_paths(trafficability_grid)
 max_factor = 5 // no paths of cost more than 5 times of initial path will be resulted
 create paths array to store paths
 lc_path = get least cost path for trafficability_grid using dijkstra's algorithm
 add lc_path to paths
 limit = max_factor * cost of lc_path // this is the cost limit for paths
 while true:
 mark cell of lc path as restricted in trafficability_grid except start and end cell
 lc_path = get least cost path for trafficability_grid using dijkstra's algorithm
 if cost of lc_path > limit:
 break while loop
 add path to path
 return correct_paths(paths, trafficability_grid) // correct the paths and return
 Path Correction Algorithm
 function correct_paths(paths, trafficability_grid)
 k = a very large value
 create array new_paths to store corrected paths
 create new grid of shape trafficability_grid and change all cell values to k (say new_grid)
 for each path in paths
 for all cells of the path:
 update new_grid with value from trafficability_grid
 for each path in paths
 lc_path = get least cost path for new_grid using dijkstra's algorithm
 add lc_path to new_paths
 section_to_remove = find_section_to_remove(lc_path, new_grid)
 mark cells of section_to_remove with k in new_grid
 return new_paths
 Obtaining Sections to remove from paths respectively
 find_section_to_remove(path, new_grid)
 k = very large value used in new_grid
 initially consider whole path as section to remove
 create array sections to store spllited sections
 threshold = 6 // initially cells with 6 or more neighboring non-path cells are considered as they are possible thinnest paths
 while sections is empty and threshold >= 0:
 inside_section = false
 length_of_section
 = 0
 start_cell_of_section = path[0]
 for cell in path:
 empty_count = number of neighboring cell with k // number of non-path neighbors
 if not inside_section and empty_count >= threshold:
 inside_section = True
 start_cell_of_section = cell
 length = 0
 if inside_section and empty_count >= threshold:
 length = length + 1
 if inside_section and empty_count < threshold:
 inside_section = False
 store start_cell_of_section as start, cell as end and length in sections array
 find the maximum length section from sections and assign to max_section
 threshold = threshold - 2
 return max_section
 So after applying the
 correction the paths for above example looked like following. Paths are
 shown in orange color.
 So this approach could be
 identified as a successful one, as the more unique and different avenues
 of approaches could be given as output. The routes that were given as
 output were basically similar paths that a person who is familiar with
 this area would choose.
 Limitation at choke points
 In this approach, a problem
 that we identified was, there is only a single path would be given as
 output through a choke point. Choke points are the places where troops
 have to maneuver through a very narrow area, where both the left and
 right sides are restricted areas. As example bridges, mountain passes,
 narrow areas between buildings etc.
 When generating paths, at
 initial state, when a path is there through a such choke point, though
 there can be another path which is not exactly similar to this path and
 has another approach but need to pass this choke point, it will not be
 given as output. The reason is to happen such a thing there should be
 close and parallel path segments for those two where there is should be
 common path. But that cannot happen as parallel and close paths cannot
 pass restricted part at choke point and the only pass through choke
 point has been occupied by first path. So to resolve this issue, if
 there is a choke point in the path generated, before generating the next
 independent path the pass through choke point must be unoccupied.
 Following is an image where
 paths are generated between two locations in Unversity of Peradeniya and
 the limitation of paths can be seen as there are two choke points here
 (Akbar bridge and Peradeniya bridge) and hence the potential avenues
 marked in light green color are not given in output. Paths in blue color
 are the computer generated paths.
 As described in above to
 resolve this issue, an modification was done to the 'Getting Independent
 Paths' algorithm. The identified choke points were made unoccupied
 after obtaining a path and before generating next independent path. For
 that instead of making the whole generated path restricted, the cells
 excluding choke points in the path were made restricted. So to find all
 choke points and remove path without choke points, an algorithm was
 developed and it's pseudo-code is as below.
 function non_choke_points(restricted_grid, path):
 front_cell = None
 back_cell = None
 create array non_chokes_point_set = to store cells which are not choke points
 for each cell in path:
 back_cell = front_cell
 front_cell = cell
 if back_cell is not None and front_cell is not None:
 v_d = front_cell[0] - back_cell[0]
 // get vertical displacement
 h_d = front_cell[1] - back_cell[1]
 // get horizontal displacement
 directions = None
 is_diagonal = False
 if (abs(h_d) - abs(v_d)) == 1:
 directions = (1, 0, -1, 0)
 else if (abs(h_d) - abs(v_d)) == -1:
 directions = (0, 1, 0, -1)
 else if(h_d * v_d) == 1:
 directions = (1, -1, -1, 1)
 is_diagonal = True
 else if(h_d * v_d) == -1:
 directions = (1, 1, -1, -1)
 is_diagonal = True
 is_choke = scan(restricted_grid, back_cell, directions, is_diagonal) // start scanning two sides
 if not is_choke:
 non_chokes_point_set.append(back_cell)
 return non_chokes_point_set
 function scan(restricted_grid, cell, directions, is_diagonal):
 choke_threshold = minimum distance to an obstacle for a cell to be a choke point
 restricted_1_found = False
 restricted_2_found = False
 distance_traveled = 0
 distance_step = 1
 if is_diagonal:
 distance_step = square root of 2
 while distance_traveled <= choke_threshold:
 distance_traveled = distance_traveled + distance_step
 current_cell1 = (cell[0] + directions[0], cell[1] + directions[1])
 current_cell2 = (cell[0] + directions[2], cell[1] + directions[3])
 if not restricted_1_found and restricted_grid[current_cell1] == 1:
 restricted_1_found = True
 if not restricted_2_found and restricted_grid[current_cell2] == 1:
 restricted_2_found = True
 if restricted_1_found and restricted_2_found:
 return True
 return False
 The scan function scan each
 cell in path in left and right directions up to the distance defined as
 choke_threshold to find a obstacle, it there are obstacles in both
 directions withing that limit, that cell is a choke point.
 We defined choke threshold
 as 4 units, that will approximately equal to 12 meters.
 So see the below image of
 computer generated output of above scenario after applying the fix to
 the 'Getting Independent Paths' algorithm.
 So now the result seems very similar to a human generated output.
 To do a time complexity
 comparison as in earlier approaches, we executed these algorithms for
 the 6 sample battlefields. As done in k-shortest path time comparison
 when choosing start and end, they were chosen such that they are one at a
 corner and other in center. Following is the table of times taken for
 the algorithm for different size grids.
 The graphical representation
 of time taken for k-shortest paths algorithm vs number of cells is
 below.
 Comparison of Three Approaches
 Below chart compares times
 taken by all three algorithms tested using six sample battlefields.
 The chart suggests that
 compared to time consumption, Dijkstra's based path removing algorithm
 is much time efficient than other two approaches. k-shortest path
 approach is not good as it's time consumption is much high as well as
 increase exponentially with number of cells.
 Following is a qualitative comparison between outputs of the three approaches.
 Generalized Voronoi Diagram Method
 k-shortest paths algorithm
 Dijkstra's based path removing algorithm
 Only GO,NO-GO terrain is used
 Trafficability grid is used with all features
 Trafficability grid is used with all features
 Paths does not depend on cost of traveling
 Paths depend on cost of traveling
 Paths depend on cost of traveling
 Different possible paths are resulted, but some mismatch is with paths paths are not spread, mostly same path with small differences is resulted
 Much spread can be seen in paths, actually different possible paths are resulted
 Time taken for algorithm is low (not the lowest)
 Heavy time consuption
 Very low time taken (it is the lowest out of three approaches)
 Considering all factors, we decide to use Dijkstra's based path removing algorithm.
 Risk evaluation of corridors to predict the avenues of approach and key
 areas
 As we get set of distinct
 easiest avenues that can be used for troop maneuver, there might be
 some risks in using the
 paths due to enemy locations.
 So in this milestone
 wedeveloped an algorithm to define the range of threats for
 the enemy locations
 annotatedby user and then use that range of threats to find
 threat for routes
 generated.The general approach to get a range of threat was,
 the threat decreasing a
 uniformamount when going away from the enemy location or building.
 So as we can definebuildings
 as enemy ones in our tool, the value must start decreasing from
 the wall ofthe building to
 outside. As our representation of terrain was using a grid of cells,
 wedefined two 2d arrays of
 the shape of terrain grid called Enemy threat grid and Threat
 decrement grid. In here
 Threat decrement grid has a value for each cell defining how much threat
 will loss in this cell. The amount is the loss of threat per grid cell
 unit.Using that we created the enemy threat grid that will finally have a
 value of threat foreach cell in the terrain. The value is between 0 and
 10.In here, what would decrease threat was only distance from the enemy
 building. Sothe threat decrement grid has uniform values. So the threat
 range that is resulted is a circular area around the building where
 threat last only to a maximum fixed distance from borders of the
 building.When a same cell in threat array get values from two threat
 locations, the maximum out of the threats at the cell due to all threat
 locations is kept.So the rasterized image of threat variation from an
 enemy building is as Following figure when only distance is considered.
 Following figure is the flow of the code we developed to get threat grid
 So the pseudo-code of the
 algorithm used to get the threat grid from a building whengiven the
 border_cell_list, which is the list of cells in the buildings border of
 it is given below
 function threat_grid(border_cell_list, threat_decrement_array)
 define starting threat for this building as T
 create new grid threat_range
 for each cell in border_cell_list:
 visited = new grid to store whether the cell visited or not
 q = new queue to store scanned cells with threat
 add cell to the q with threat T
 mark threat of cell in threat_range as T
 while q is not empty:
 current_cell = get cell with maximum threat cell from q
 remove current_cell from q
 mark current_cell as visited in visited
 let d is current_cell_threat_decrement
 d = threat_decrement_array_cell[current_cell]
 threat = threat_range[current_cell]
 for each unvisited neighbor cell of current_cell:
 let n is neighbor_cell_decrement
 n = threat_decrement_array_cell[neighbor]
 if neighbor is in diagonal direction:
 threat_decrement = square_root(2) * (d + n) /2
 else
 threat_decrement = d + n) /2
 neighbor_cell_threat = threat - threat_decrement
 if neighbor_cell_threat > threat_range[neighbor]:
 update threat_range with neighbor_cell_threat
 add neighbor to q
 return threat_range
 Then we studied how the
 terrain features would effect the threat and how to introduce those
 effects to our automated tool. From the features we have for terrain
 grid we identified following features would effect threat range of an
 enemy building.
 Enemy building height
 Height of surrounding buildings
 Level of vegetation
 High elevation than enemy building height in surround area
 Low elevation than enemy building ground
 So to add enemy building
 height to the code functionality we defined the starting threat T of the
 algorithm according to the enemy building height. As the threats will
 be mapped to range between 10 and 0 at the end, increasing starting
 threat at enemy building is not an issue.
 The best way to add the
 effect of surrounding features to the threat grid, we automatically
 change the Threat_decrement_array according to the features. As example,
 in the cells where there is a building, the threat decrement will be
 higher than normal cell.
 So we defined following
 attributes that affect threat decrement array and assingned some sample
 values for them and fine tuned the variables until a good result come.
 In a normal flat terrain
 with no features like building or vegetation given, threat from 1
 storied enemy building decrease uniformly up to 100m from border of the
 building.
 So the average width of a
 cell in our grid is approximately 3m, So the range is about 33 units.
 threat_decrement_building_max, this is the threat decrement at places
 where a building blocks visibility, normally it is a building with same
 number of stories or more than the enemy building.
 threat_decrement_grassland, this is the threat decrement at places where there is a grassland
 threat_decrement_shrubland, this is the threat decrement at places where there is a shrubland
 threat_decrement_medium_forest, this is the threat decrement at places where there is a medium_forest
 threat_decrement_high_forest, this is the threat decrement at places where there is a high_forest
 threat_decrement_elevation_increment, this is the value which the
 available threat decrement increase when the elevation is higher than
 the estimated building height
 threat_decrement_elevation_decrement , this is the value which the
 available threat decrement decrease when the elevation is lower than the
 enemy ground level.
 building_floor_height_average, this is the multiplying factor to get
 estimated building height from the number of stories of it.
 range_increment_per_floor, normally range of the single storied building
 is 33 units, but it increase when the number of stories of enemy
 building increase. This is the value in which the range increase per
 single storey.
 following Figures are images
 of the threat array generated in enemy buildings by changing the
 terrain features.
 When all buildings are
 single floor(LEFT), when top enemy building was made two story (MIDDLE) ,
 when a close building of it also made two story (RIGHT)
 When all buildings are two
 story, the threat range of enemy building is blocked by surrounding
 buildings as in first image. So when the enemy building is made a two
 stroy one, it's range of threat doesn't blocked by single floored
 buildings around. that is why the range has not changed in middle image.
 So when a nearby surround building also made two story as in right
 image, the range of enemy building will get effected from that.
 Below figure shows the
 variation of threat with elevation right side of the building, the
 elevation is high, it is higher than the height of the building, so the
 threat from building has been limited to right side. Also to left of the
 building, you can see there is an increase of threat. that is because
 the ground level to that side is lower than building ground level as
 well as the vegetation is grassland, that cause more spread of threat
 towards that side.
 Variation of threat with elevation
 Basically vegetation level
 effect threat range, in below figure to left side of the building, there
 is a heavy density forest, so the threat have been limited towards that
 side.
 Variation of threat with vegetation
 Finally obtaining the threat
 grid for the whole battlefield, we decided threats for the routes
 generated between given coordinates. So the paths were colored in IPB
 tool using the threats obtained for each routes as below. In the threat
 representation of the map, the colors change from green to red to
 represent threat from 0 to 10 respectively.
 Below is an image from IPB
 tool when potential mobility corridors were generated and paths are
 colored according to threat level due to the enemy locations marked in
 red.
 Comparison with Available Systems
 Google map directions
 Basically the platform
 normally used to find paths to travel from one place to another place is
 Google Map directions.
 Following figure shows the
 comparison of the avenues of approaches generated between two positions
 separated by a river and the Google direction result for those two
 positions.
 Comparison with Google
 direction, Our System generated paths (LEFT) Google Directions for
 vehicles(MIDDLE) and Google directions for walking(RIGHT)
 Basically direction API
 consider only available routes to generate the paths. Some times they
 give multiple paths possible but not to much deep level. So it doesn't
 consider the terrain features or any additional information we give on
 terrain in generation paths. Also it does not suggest paths to maneuver
 through non road areas. So in case of avenues of approaches our
 implementation is much successful towards obtaining avenues of
 approaches for troop maneuver.
 Comparison with result from a related works
 In [3] the researchers have
 developed algorithms to generate avenues of approaches for a small map
 using a trafficability array and generalized voronoi diagram. Also they
 have evaluated the avenues of approaches of that map by an subject
 matter experts (SME). So we recreated the map they have used in the
 research drawing similar terrain data.Then obtained avenues of
 approaches for the two locations they have used and then compared it
 with the result generated by their system and manual result by SME.
 Following figure shows the
 avenues of approaches for that map given by algorithms used by the
 researchers and the SME.
 Avenues of approaches by
 researcher's algorithms (LEFT) and subject matter expert(RIGHT), (C.
 Grindle, M. Lewis, R. Glinton, J. Giampapa, and K. Owens 2004, Fig. 5
 and 6, p. 4)
 Then following figure shows
 the our IPB tool generated avenues of approaches for the same map
 created by us on our tool.
 Avenues of approaches by our IPB tool
 So the avenues of approaches
 generated by our tool seems much similar to the avenues drawn by
 subject matter experts in the given research. There are basic three
 avenues suggested by the SME as well as our system. In the referenced
 research's output, only two avenues are suggested. Also our result
 contain risk estimation values for the avenues as well if enemy
 locations were annotated.
 Also the algorithm used by
 the referenced research is based on a voronoi diagram method. So that
 approach is giving a complex scenario when comes to larger maps as well
 as much time as concluded in \autoref{f:label27}. So for larger maps
 with more details like buildings, water bodies the voronoi diagram
 become complex and give very high number of paths. So removing unwanted
 paths is difficult. So for each larger and smaller maps with any amount
 of features the approach taken by our algorithms is good.
 Conclusions and Future Works
 Building a tool for
 battlefield area evaluation, storing and visualizing information, and
 supporting decision making for troop maneuver planning is the primary
 objective of this project. In the IPB process also the objective is to
 build the combined obstacle overlay to use for troop locating and
 maneuver planning. Avenues of approach, Engagement areas, Defensible
 terrain are some final high level information obtained through the
 combined obstacle overlay. In this research we could develop algorithms
 and the tool to generate and display avenues of approach successfully.
 we looked at the low level
 environmental factors such as ground, and environment data. We developed
 a database of predefined terrain features data like building,
 elevation, vegetation, water and roads for any location. In this project
 we just included data for only Sri Lanka. So any default terrain data
 for a battlefield are automatically obtained by the tool. Then we build a
 mechanism to display in on the map as overlays. Also implemented method
 to put user defined features to overlays. We could developed a backend
 to store, edit and give the battlefield data separately. Using a REST
 API, we connected the backend with frontend IPB tool to enable
 operations on overlays.
 We obtained trafficability
 grid using a grid based model for processing overlay data. In the
 decision support development part we explored three different approaches
 to use trafficbilty grid to generate avenues of approach, which was a
 main requirement in IPB process. Finally we compared and further
 developed the best and more practical approach from those three. we
 finally developed the algorithms for the avenues of approach generation.
 Then We developed algorithms to find threat level for paths due to
 enemy. Finally we compared our output avenues with available paths
 generating platforms like Google maps and the avenues suggested by
 subject matter experts in related researches.
 As planned in milestones we
 implemented only avenues of approaches finding using trafficability
 grid. So there are few other high-level terrain information such as key
 terrain, defensible terrain and engagement areas. So as a future work
 those goals must be accomplished.
 Also there are few other
 terrain information that need to be fused with terrain data like weather
 and soil type. So we couldn't combine those data due to lack of those
 data. If those data also got fused, the we could obtain more accurate
 results. So that need to be done as a future target.
 Also when considering the
 threat from enemy we developed the algorithm to change the enemy range
 of threat according to elevation, vegetation, surrounding buildings and
 height of enemy building. So in future works, enemy must be more
 customized like several types of enemy like snipers, normal ones, scouts
 so on. Then that type also will effect the enemy range.
 Also currently we are
 obtaining the threat from enemy to the avenues. so in future version
 when there is a considerable high threat from a enemy location to a
 path, that path should be minimized to avoid that threat making a new
 path.
 References
 P.Skalický and
 T.Palasiewicz, “Intelligence preparation of the battlefield as a partof
 knowledge development,” 2017
 R. Glinton, S.
 Owens, J. Giampapa, K. Sycara, C. Grindle, and M. Lewis, “Terrain-based
 information fusion and inference,”Proc. Seventh Int. Conf. Inf.
 Fusion,FUSION 2004, vol. 1, pp. 338–345, 2004
 C. Grindle, M.
 Lewis, R. Glinton, J. Giampapa, and K. Owens, Sean Sycara, “Au-tomating
 Terrain Analysis: Algorithms for Intelligence Preparation of the
 Battlefield,”Proceedings of the Human Factors and Ergonomics Society
 Annual Meeting, vol. 48,no. 3, pp. 533–537, 2004
 C. J. James Donlon
 and K. D. Forbus, “Using a Geographic Information System forQualitative
 Spatial Reasoning about Trafficability,”Proc. QR99, pp. 1–11, 1999
 K. K. Rowel and H.
 Ranasinghe, “Impact of gis modelling in military operationalplanning..”
 E. D. Porter, “An overview of the army gis research program,” 1987
 W. Headquarters,
 Department of the Army, “Terrain analysis,”Encyclopedia ofGeographic
 Information Science, 1990.
 M. C. A. Agee,
 “INTELLIGENCE PREPARATION OF THE BATTLEFIELD(IPB),”Society, vol. 1387,
 no. 22, pp. 1383–1387, 1987
 U. W. Mark Meeder,
 Tobias Aebi, “The influence of slope on walking activity andthe
 pedestrian modal share,”20th EURO Working Group on Transportation
 Meeting,2017.
 D. E. Sidran,
 “TIGER: AN UNSUPERVISED MACHINE LEARNING TACTICALINFERENCE GENERATOR,”
 2009
 P. Svenson and H.
 Sidenbladh, “Determining possible avenues of approach usingANT,” 2003.
 © Team IPB","Intelligence Preparation of Battlefield or IPB as it is more commonly known is a Command and staff tool that allows systematic, continuous analysis of the enemy and the battlefield environment to be carried out and which presents the results in a graphical form",E14,Undergraduate Research Projects,https://projects.ce.pdn.ac.lk/4yp/e14/ipb/,https://github.com/cepdnaclk/e14-4yp-ipb,https://cepdnaclk.github.io/e14-4yp-ipb,https://cepdnaclk.github.io/e14-4yp-ipb/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/4yp/E14/ipb/
109,Agriculture Monitoring and Control System,No project page,,E16,Industrial Automation Projects (CO326),https://projects.ce.pdn.ac.lk/co326/e16/Agriculture-Monitoring-and-Control-System/,https://github.com/cepdnaclk/e16-co326-Agriculture-Monitoring-and-Control-System,#,#,http://api.ce.pdn.ac.lk/projects/v1/co326/E16/Agriculture-Monitoring-and-Control-System/
110,Remotely Controlled CNC Robot,"CNC base 3-axis remotely controlled pick and place robot
 Projects - Department of Computer Engineering
 University of Peradeniya, Sri Lanka
 CNC base 3-axis remotely controlled pick and place robot
 Team
 Jaliyagoda A.J.N.M. (E/15/140)
 Karunarathna S.D.D.D. (E/15/173)
 Tennakoon T.M.P.B. (E/15/350)
 Table of Contents
 Introduction
 Specifications
 Methodology
 Design
 Links
 Introduction
 Over the past few decades technology has been evolving rapidly in so many aspects and fields to reach out to the human population in a more user-friendly manner. With these technological developments, new terms like “distance learning”, “remote laboratories”, “virtual learning environments” etc have emerged. However, labs have the greatest potential to overcome the bottleneck in distance education. The goal of Remote Laboratory implementation is to grant students access to laboratory equipment and cover physical laboratory exercises remotely.
 A programmable logic controller (PLC) is a special purpose industrial computer/controller that has been adapted to control manufacturing processes. Hence, PLC is widely used in manufacturing to organize complex tasks like security monitoring, automatic control production lines, and management of energy consumption. As a result, there is a great need for engineers with knowledge of PLC. High cost, limited equipment, and limited access to this equipment make it difficult to access everyone.
 Hence, connecting PLC with the remote laboratory enables educational institutions to offer programs to a much broader target group of potential students who under no circumstances are able to travel to and attend on-site sessions.
 In this project, we hope to develop a CNC (Computer Numerical Control) based pick and place arm, which will help to try different wiring arrangements in a PLC rig and try experiments one it remotely.
 Figure 1: An example of a Pick and Place CNC machine
 Figure 2: PLC Test Rig of the department of computer engineering
 Specifications
 Software Layer
 Web HMI with Javascript
 MQTT broker for communication
 Hardware Software Interface
 Using standard G-code language for machine control
 A firmware like Merlin/Gerber will be supposed to use
 Communication is done by USART protocol, with a baud rate of 115000bps
 Hardware Layer
 Nema17 standard stepper motors for motion control
 DRV8825 for stepper motor control
 Arduino Mega 2560 for motion planning
 Methodology
 It is constructed using the CNC technology, using Stepper motors with a combination of Screw and Bolt mechanism (for Y and Z axises) and Belt and Pulley mechanism (For X axis). Stepper motors are driven by DRV8825 Step stick drivers with microstepping of 1/16 mode. The microcontroller for the machine is Arduino Mega 2560, with the support of RAMPS 1.4 shield for additional hardware.
 The firmware we used is known as GRBL, a well known firmware which drives CNC machines using AVR family microcontrollers. We used GCODE to instruct the machine about movements in 3d space. The software named ‘Candle’ is used to control the machine from PC, and send machine commands as well as configuration commands.
 Communication protocol between HMI and the server needed careful consideration. The messages should be exchanged through the public internet and it should be lightweight since the messages need to arrive with minimum latency which could potentially disrupt the operation of the CNC machine. Therefore, the protocol should be able to handle unreliable networks. Also the messages only contain small commands that need to be passed to the hardware controller. Therefore, the message overhead should be minimum. And most importantly it should be able to handle multiple users. So students can work in groups controlling the PLC rig remotely. Considering all these requirements we used MQTT (Message Queue Telemetry Transport) as the message passing protocol. Because, MQTT is a lightweight message protocol that is based on a subscription-publishing model, in which publishers send messages to a server and this is who forwards messages to subscribers avoiding point-to-point connections between subscribers and publishers.
 Students can log into the remote platform using the HMI developed using JavaScript (along with HTML and CSS). Then they can use the HMI to send signals to control the CNC machine. The control messages get published to a MQTT broker. A server physically connected to the hardware controller subscribed to the MQTT broker receives the control messages. The control messages contain GCODE commands needed to control
 the machine. The server sends the G-CODE commands through a serial interface to the ATMEGA 2560 which runs the GRBL. GRBL converts the control signals to electronic signals and sends to the motor driver, which controls the three server motors. This moves the 3-axis of the CNC machine.
 Design
 Funding
 This project was funded by the Prof. Suhada Jayasuriya Project Support Fund through PEFAA https://pefaa.net/.
 Links
 Project Repository
 Project Page
 Department of Computer Engineering
 University of Peradeniya
 Back to top","In this project, we developed a CNC (Computer Numerical Control) based pick and place arm, which will help to try different wiring arrangements in a PLC rig and try experiments on it remotely.",E15,Industrial Automation Projects (CO326),https://projects.ce.pdn.ac.lk/co326/e15/Remotely-Controlled-CNC-Robot/,https://github.com/cepdnaclk/e15-co326-Remotely-Controlled-CNC-Robot,https://cepdnaclk.github.io/e15-co326-Remotely-Controlled-CNC-Robot,https://cepdnaclk.github.io/e15-co326-Remotely-Controlled-CNC-Robot/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/co326/E15/Remotely-Controlled-CNC-Robot/
111,Analysis Tool for Industrial Images,"Analysis Tool for Industrial Images
 Projects - Department of Computer Engineering
 University of Peradeniya, Sri Lanka
 Analysis Tool for Industrial Images
 Team
 E/17/154, KARUNANAYAKE A.I, e17154@eng.pdn.ac.lk
 E/17/072, DISSANAYAKE D.M.D.R, e17072@eng.pdn.ac.lk
 E/17/380, WEERASOORIYA S.S, e17380@eng.pdn.ac.lk
 Table of Contents
 Problem Overview
 Existing System
 Proposed System
 Links
 Problem Overview
 The injection molding manufacturing process is used for producing parts by injecting molten material into a mold.In our specific use case, we focus on plastic injection molding.One of the common problems in this process is when plastic residue gets stuck or left behind in the mold.Thus leading to damages to the mold , defects in products and ultimately resulting in unwanted business costs.
 Current Implementation
 The above issue is mainly handled by checking the molds manually on each iteration. This has become a heavy burden as it costs time and money to supply this labor at a high frequency . These machines are highly capable of working on full automatic but this issue has caused a significant overhead in the manufacturing process.As a solution, a device was built to capture images of the mold in near IR frequency to check for stuck particles. Currently, this is being implemented to mitigate this issue.The implementation is purely run to check for defects using an image processing algorithm with a given set of parameters.The problem here is that although this implementation has proven to be a better alternative there is a limitation at which how effective or how true the results from this device is Currently, the built device only gives out a binary output as positive a negative. Since there is a limitation for the flexibility of the used algorithm is these results do not always turn out to be true. Hence our focus is to elevate the performance of the implemented device to produce better results.
 Proposed System
 As a solution, we aim to provide a tool to provide statistical data representing to be used to show the effectiveness of the algorithm.Here we would change parameters such as threshold values for identifying particle sizes. For this we will be using the images captured from the device and manually labelled image sets. As a second part of the implementation, a dashboard to centrally view all the statistical data pertaining to different machines will be created. So that a better management of the facility can be maintained. With the proposed solution we will be able to improve the performance of the current implementation and as a result, improve the overall benefits. We will be able to have better confirmation of the errors / defects with higher assurance. We wil be able to minimize the cost of labour as the frequency of mold checks/ cleaning gets minimized and increase the overall output of the machines. The minimal maintenance cost wil be significantly lower than the high price to be paid for damaged molds. With the added improvement s to the device, the whole operation will ultimately be able to work on its maximum rated speed without any interference.
 Links
 Project Repository
 Project Page
 Department of Computer Engineering
 University of Peradeniya
 Back to top",An analysis tool for increasing the performance of an injection mold maintenance device which uses image processing techniques to identify stuck plastic particles on molds. ,E17,Software Engineering Projects (CO328),https://projects.ce.pdn.ac.lk/co328/e17/Analysis-Tool-for-Industrial-Images/,https://github.com/cepdnaclk/e17-co328-Analysis-Tool-for-Industrial-Images,https://cepdnaclk.github.io/e17-co328-Analysis-Tool-for-Industrial-Images,https://cepdnaclk.github.io/e17-co328-Analysis-Tool-for-Industrial-Images/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/co328/E17/Analysis-Tool-for-Industrial-Images/
112,Document Tag Generator,No project page,Our project is to generate tags for the projects on the department website.,E17,Software Engineering Projects (CO328),https://projects.ce.pdn.ac.lk/co328/e17/Document-Tag-Generator/,https://github.com/cepdnaclk/e17-co328-Document-Tag-Generator,#,#,http://api.ce.pdn.ac.lk/projects/v1/co328/E17/Document-Tag-Generator/
113,Flood Forecasting System,"Flood Forecasting System
 Projects - Department of Computer Engineering
 University of Peradeniya, Sri Lanka
 Flood Forecasting System
 Team
 E/17/006, ALAHAKOON A.M.H.H, e17006@eng.pdn.ac.lk
 E/17/176, KUMARA W.M.E.S.K, e17176@eng.pdn.ac.lk
 E/17/338, SRIMAL R.M.L.C, e17338@eng.pdn.ac.lk
 Table of Contents
 Introduction
 Solution
 Web Interface Overview
 Links
 Introduction
 Floods are the most destructive form of natural hazards in both local and global context.
 This is true in terms of both loss of life and property damage. Early flood forecasting can
 be used to identify potential areas of flooding in order to develop mitigatory planning and
 evacuation programs to remove people from such areas during flooding
 and also to implement suitable preventive measures to avoid damage to properties.
 In this project, our main objective is to build a flood forecasting system for Mi Oya river
 basin(Sri Lanka).Mi Oya Basin is heavily affected by seasonal flooding and droughts.
 As per the available data, floods in the Mi Oya basin are unleashed due to river overflow
 and reservoir spilling. Out of the several reservoirs located in the basin, Tabbowa and
 Inginimitiya are crucial in worsening the flood impacts as these two reservoirs are
 frequently spilling under adverse weather conditions. As such, the prevalence of a
 real-time flood forecasting model with the incorporation of the reservoir operations
 for the entire basin is essential to alleviate the flood induced impacts while
 preserving the optimum volume of water in the major reservoirs in the basin.
 Solution
 The proposed solution is to implement a simple data-driven or machine learning model to
 identify a direct mapping between the inputs(e.g., precipitation(P), temperature(T),
 potential evapotranspiration(PET), etc.) and outputs(In-flow level) without detailed
 consideration of the internal structure of the physical process. Also the system is
 consist of a web interface. The interface consists of three major modules as community
 view,control panel, and report. The warning messages are displayed with a map depicting
 the inundation extent in the community view, which is the interface for the public. Further,
 the community members are allowed to register their mobile numbers to receive warnings
 as SMS when such warnings are broadcasting at the disaster management center. A summary
 of past floods can be generated from the Report module for a required time period. Control
 panel provides access to the simulation module, gate operation module, flood warning
 dissemination module, access control module, etc. Only privileged users have access to the
 control panel. The figure below outlines the highlevel structure of the web application.
 Web Interface Overview
 Links
 Project Repository
 Project Page
 Department of Computer Engineering
 University of Peradeniya
 Back to top","Floods are the most destructive form of natural hazards in both local and global context. This is true in terms of both loss of life and property damage. Early flood forecasting can be used to identify potential areas of flooding in order to develop mitigatory planning and evacuation programs to remove people from such areas during flooding and also to implement suitable preventive measures to avoid damage to properties. In this project, our main objective is to build a flood forecasting system for Mi Oya river basin(Sri Lanka). Mi Oya Basin is heavily affected by seasonal flooding and droughts. As per the available data, floods in the Mi Oya basin are unleashed due to river overflow and reservoir spilling. Out of the several reservoirs located in the basin, Tabbowa and Inginimitiya are crucial in worsening the flood impacts as these two reservoirs are frequently spilling under adverse weather conditions. As such, the prevalence of a real-time flood forecasting model with the incorporation of the reservoir operations for the entire basin is essential to alleviate the flood induced impacts while preserving the optimum volume of water in the major reservoirs in the basin.",E17,Software Engineering Projects (CO328),https://projects.ce.pdn.ac.lk/co328/e17/Flood-Forecasting-System/,https://github.com/cepdnaclk/e17-co328-Flood-Forecasting-System,https://cepdnaclk.github.io/e17-co328-Flood-Forecasting-System,https://cepdnaclk.github.io/e17-co328-Flood-Forecasting-System/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/co328/E17/Flood-Forecasting-System/
114,Greenhouse Monitoring System,"Greenhouse Monitoring System
 Projects - Department of Computer Engineering
 University of Peradeniya, Sri Lanka
 Greenhouse Monitoring System Based on Image Spectral Data
 Team
 E/17/297, Rupasinghe T.T.V.N., e17297@eng.pdn.ac.lk
 E/17/206, Manohara H.T., e17206@eng.pdn.ac.lk
 E/17/148, Kalpana M. W. V., e17148@eng.pdn.ac.lk
 Table of Contents
 Overview
 Problem Statement
 Solution
 Links
 Overview
 Greenhouse Monitoring System provides a platform to manage the Greenhouse by tracking the phases of plant harvest, identifying any plant disorder and tracking the plant growth by using the image spectral data of plants. The system is basically considered the key problems plant diseases, huge harvest wastage and unnecessary expensive maintenance in a greenhouse. So this system will make a high positive impact on maximizing the harvest and reduce maintenance cost in Greenhouses.
 Problem Statement
 Although the environmental conditions of plants are controlled, the temporal effects like temperature, humidity may not evenly balanced for each crop. Therefore, plants respond differently under those unbalanced environmental conditions. And, plants can have different kind of disorders. It leads to production failures in greenhouses. As well, the crop yield may not be harvested at the right moment. Because of that there would be a huge harvest wastage.
 In current greenhouses, the workers continuously observe the plant growth. In that case, workers will be tired and labor system would be inefficient. So, there would be an unnecessary higher cost for maintenance.
 Solution
 Plant diseases, huge harvest wastage and unnecessary expensive maintenance have been observed as the major issues in current Greenhouse Systems. These key problems simulated the development of image analysis and computer vision methods. That’s how the “greenhouse monitoring system based on image spectral data” came to the stage.
 Links
 Project Repository
 Project Page
 Department of Computer Engineering
 University of Peradeniya
 Back to top","Greenhouse Monitoring System provides a platform to manage the Greenhouse by tracking the phases of plant harvest, identifying any plant disorder and tracking the plant growth by using the image spectral data of plants. The system is basically considered the key problems plant diseases, huge harvest wastage and unnecessary expensive maintenance in a greenhouse. So this system will make a high positive impact on maximizing the harvest and reduce maintenance cost in Greenhouses.",E17,Software Engineering Projects (CO328),https://projects.ce.pdn.ac.lk/co328/e17/Greenhouse-Monitoring-System/,https://github.com/cepdnaclk/e17-co328-Greenhouse-Monitoring-System,https://cepdnaclk.github.io/e17-co328-Greenhouse-Monitoring-System,https://cepdnaclk.github.io/e17-co328-Greenhouse-Monitoring-System/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/co328/E17/Greenhouse-Monitoring-System/
115,Host Pathogen Interaction,"BioWeb | Analazing microbiome data
 BioWeb
 Home
 About
 Pricing
 Team
 Contact
 Pages
 About Page
 Pricing Page
 Contact Page
 Blog Grid Page
 Blog Details Page
 Sign Up Page
 Sign In Page
 404 Page
 Sign In
 Sign Up
 Inference of host-microbe associations based on metagenomic data
 Toole provided the feature to analyze given microbiome data with the help of Machine learning and GUI .
 Web App Demo
 Star on Github
 Features
 Main Features Of Play
 There are many variations of passages of Lorem Ipsum available but the majority have suffered alteration in some form.
 Free and Open-Source
 Lorem Ipsum is simply dummy text of the printing and industry.
 Learn More
 Multipurpose Template
 Lorem Ipsum is simply dummy text of the printing and industry.
 Learn More
 High-quality Design
 Lorem Ipsum is simply dummy text of the printing and industry.
 Learn More
 All Essential Elements
 Lorem Ipsum is simply dummy text of the printing and industry.
 Learn More
 About Us
 Brilliant Toolkit to Build Nextgen Website Faster.
 The main ‘thrust' is to focus on educating attendees on how to best protect highly vulnerable business applications with interactive panel discussions and roundtables led by subject matter experts.
 The main ‘thrust' is to focus on educating attendees on how to best protect highly vulnerable business applications with interactive panel.
 Learn More
 Pricing Table
 Our Pricing Plan
 There are many variations of passages of Lorem Ipsum available but the majority have suffered alteration in some form.
 STARTING FROM
 $ 19.99/mo
 1 User
 All UI components
 Lifetime access
 Free updates
 Use on 1 (one) project
 3 Months support
 Purchase Now
 POPULAR
 STARTING FROM
 $ 19.99/mo
 5 User
 All UI components
 Lifetime access
 Free updates
 Use on 1 (one) project
 4 Months support
 Purchase Now
 STARTING FROM
 $ 70.99/mo
 1 User
 All UI components
 Lifetime access
 Free updates
 Use on unlimited project
 4 Months support
 Purchase Now
 FAQ
 Any Questions? Answered
 There are many variations of passages of Lorem Ipsum available but the majority have suffered alteration in some form.
 How to use TailGrids?
 It takes 2-3 weeks to get your first blog post ready. That includes the in-depth research & creation of your monthly content marketing strategy that we do before writing your first blog post, Ipsum available .
 How to download icons from LineIcons?
 It takes 2-3 weeks to get your first blog post ready. That includes the in-depth research & creation of your monthly content marketing strategy that we do before writing your first blog post, Ipsum available .
 Is GrayGrids part of UIdeck?
 It takes 2-3 weeks to get your first blog post ready. That includes the in-depth research & creation of your monthly content marketing strategy that we do before writing your first blog post, Ipsum available .
 Can I use this template for commercial project?
 It takes 2-3 weeks to get your first blog post ready. That includes the in-depth research & creation of your monthly content marketing strategy that we do before writing your first blog post, Ipsum available .
 Do you have plan to releasing Play Pro?
 It takes 2-3 weeks to get your first blog post ready. That includes the in-depth research & creation of your monthly content marketing strategy that we do before writing your first blog post, Ipsum available .
 Where and how to host this template?
 It takes 2-3 weeks to get your first blog post ready. That includes the in-depth research & creation of your monthly content marketing strategy that we do before writing your first blog post, Ipsum available .
 Testimonials
 What our Client Say
 There are many variations of passages of Lorem Ipsum available but the majority have suffered alteration in some form.
 “Our members are so impressed. It's intuitive. It's clean. It's distraction free. If you're building a community.
 Sabo Masties
 Founder @ Rolex
 “Our members are so impressed. It's intuitive. It's clean. It's distraction free. If you're building a community.
 Margin Gesmu
 Founder @ UI Hunter
 “Our members are so impressed. It's intuitive. It's clean. It's distraction free. If you're building a community.
 William Smith
 Founder @ Trorex
 Some Of Our Clients
 Our Team
 Meet Our Team
 There are many variations of passages of Lorem Ipsum available but the majority have suffered alteration in some form.
 Adveen Desuza
 UI Designer
 Jezmin uniya
 Product Designer
 Andrieo Gloree
 App Developer
 Jackie Sanders
 Content Writer
 CONTACT US
 Let's talk about
 Love to hear from you!
 Our Location
 401 Broadway, 24th Floor, Orchard Cloud View, London
 How Can We Help?
 info@yourdomain.com
 contact@yourdomain.com
 Send us a Message
 Full Name*
 Email*
 Phone*
 Message*
 Send Message
 We create digital experiences for brands and companies by using
 technology.
 About Us
 Home
 Features
 About
 Testimonial
 Features
 How it works
 Privacy policy
 Terms of Service
 Refund policy
 Our Products
 LineIcons
 Ecommerce HTML
 Ayro UI
 PlainAdmin
 Partners
 Privacy policy
 Legal notice
 Terms of service
 Designed and Developed by
 TailGrids and UIdeck",Building the software for analyzing the microbiome data using machine learning technics. the application provides the GUI for the user they can easily analyze the data without any other programming knowledge developed by  @piriyaraj @varnaraj @thanujan @rilwan292,E17,Software Engineering Projects (CO328),https://projects.ce.pdn.ac.lk/co328/e17/Host-Pathogen-Interaction/,https://github.com/cepdnaclk/e17-co328-Host-Pathogen-Interaction,https://cepdnaclk.github.io/e17-co328-Host-Pathogen-Interaction,https://cepdnaclk.github.io/e17-co328-Host-Pathogen-Interaction/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/co328/E17/Host-Pathogen-Interaction/
116,Oral Cavity Region Detection,"Oral Cavity Region Detection Tool
 Projects - Department of Computer Engineering
 University of Peradeniya, Sri Lanka
 Oral Cavity Region Detection Tool
 Team
 e17058, Devindi G.A.I, e17058@eng.pdn.ac.lk
 e17090, Francis F.B.A.H, e17090@eng.pdn.ac.lk
 e17190, Liyanage S.N, e17190@eng.pdn.ac.lk
 Table of Contents
 Introduction
 Solution Architecture
 Software Design
 Testing
 Conclusion
 Links
 Introduction
 This project contains a web-based application that can be used to upload images of the oral cavity and identify the known regions which are normal. For example: The tool will process an image uploaded by the clinician and apply masks to easily recognize a specific region of the oral cavity which does not indicate any abnormality.
 Why
 If known regions are quickly detected using a methodology, without patient having to endure prolonged invasions to the oral cavity, the dentists can easily identify the abnormal regions and pay more attention to the undetected oral lesions/ suspected regions in a matter of seconds.
 On the other hand, AI detection systems that are used to detect oral cancers require oral cavity images with only the lesion component. Therefore, the output masks of our tool can be used to filter out the lesion part and feed it to the cancer detection tools.
 Solution Architecture
 Software Design
 User Interface
 See the prototype
 of the web interface. (in progess)
 Use Case Diagram
 Testing
 Conclusion
 Links
 Project Repository
 Project Page
 Department of Computer Engineering
 University of Peradeniya
 Back to top",,E17,Software Engineering Projects (CO328),https://projects.ce.pdn.ac.lk/co328/e17/Oral-Cavity-Region-Detection/,https://github.com/cepdnaclk/e17-co328-Oral-Cavity-Region-Detection,#,#,http://api.ce.pdn.ac.lk/projects/v1/co328/E17/Oral-Cavity-Region-Detection/
117,Prediction of risks associated with mass corona vaccination,"Projects | Department of Computer Engineering
 Projects - Department of Computer Engineering
 University of Peradeniya, Sri Lanka
 Project Title
 This is a sample image, to show how to add images to your page. To learn more options, please refer this
 Team
 eNumber, Name, email
 eNumber, Name, email
 eNumber, Name, email
 Table of Contents
 Introduction
 Other Sub Topics
 Links
 Introduction
 description of the real world problem and solution, impact
 Other Sub Topics
 …..
 Links
 Project Repository
 Project Page
 Department of Computer Engineering
 University of Peradeniya
 Back to top","There is a trend among people not to get Covid 19 vaccinations. Because of the differing viewpoints of the various parties involved, society is skeptical of obtaining it. However, there is no research being done into the vaccination's side effects or the causes of illnesses and deaths. This project will look into the specific criteria or risks that come with vaccinations. So, this will help people to get an idea to check whether to take the vaccines or not. ",E17,Software Engineering Projects (CO328),https://projects.ce.pdn.ac.lk/co328/e17/Prediction-of-risks-associated-with-mass-corona-vaccination/,https://github.com/cepdnaclk/e17-co328-Prediction-of-risks-associated-with-mass-corona-vaccination,https://cepdnaclk.github.io/e17-co328-Prediction-of-risks-associated-with-mass-corona-vaccination,https://cepdnaclk.github.io/e17-co328-Prediction-of-risks-associated-with-mass-corona-vaccination/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/co328/E17/Prediction-of-risks-associated-with-mass-corona-vaccination/
118,Skim Sequencing Analysis,"Projects | Department of Computer Engineering
 Projects - Department of Computer Engineering
 University of Peradeniya, Sri Lanka
 Analysis Software for Next Generation Skim Sequencing
 This is a sample image, to show how to add images to your page. To learn more options, please refer this
 Team
 E/17/018, Imesh Balasuriya, email
 E/17/194, Madhushan Ramalingam, email
 E/17/296, Ravisha Rupasinghe, email
 Table of Contents
 Analysis Software for Next Generation Skim Sequencing
 Team
 Table of Contents
 Introduction
 Other Sub Topics
 Links
 Introduction
 description of the real world problem and solution, impact
 Other Sub Topics
 …..
 Links
 Project Repository
 Project Page
 Department of Computer Engineering
 University of Peradeniya
 Back to top",An Analytical Software for Next Generation Skim Sequencing Data.,E17,Software Engineering Projects (CO328),https://projects.ce.pdn.ac.lk/co328/e17/Skim-Sequencing-Analysis/,https://github.com/cepdnaclk/e17-co328-Skim-Sequencing-Analysis,#,#,http://api.ce.pdn.ac.lk/projects/v1/co328/E17/Skim-Sequencing-Analysis/
119,Sobriety Detection Using Mobile Phone Gyroscope Data,No project page,An intelligent smartphone application that will help minimize risks of drunk-driving accidents by tracking gyroscope data to identify sobriety levels in real-time and continuously authenticating users.,E17,Software Engineering Projects (CO328),https://projects.ce.pdn.ac.lk/co328/e17/Sobriety-Detection-Using-Mobile-Phone-Gyroscope-Data/,https://github.com/cepdnaclk/e17-co328-Sobriety-Detection-Using-Mobile-Phone-Gyroscope-Data,#,#,http://api.ce.pdn.ac.lk/projects/v1/co328/E17/Sobriety-Detection-Using-Mobile-Phone-Gyroscope-Data/
120,Visualization of Teacher Student Activities,No project page,This a graphical tool for represent teacher-student activities based on Moodle log data,E17,Software Engineering Projects (CO328),https://projects.ce.pdn.ac.lk/co328/e17/Visualization-of-Teacher-Student-Activities/,https://github.com/cepdnaclk/e17-co328-Visualization-of-Teacher-Student-Activities,#,#,http://api.ce.pdn.ac.lk/projects/v1/co328/E17/Visualization-of-Teacher-Student-Activities/
121,apparel industry performance analyser,No project page,A tool for data visualization and performance optimization of the production process of apparel industry.,E17,Software Engineering Projects (CO328),https://projects.ce.pdn.ac.lk/co328/e17/apparel-industry-performance-analyser/,https://github.com/cepdnaclk/e17-co328-apparel-industry-performance-analyser,#,#,http://api.ce.pdn.ac.lk/projects/v1/co328/E17/apparel-industry-performance-analyser/
122,Footwear Shop System,No project page,E16_3rd year - Software Engineering Project (CO328),E16,Software Engineering Projects (CO328),https://projects.ce.pdn.ac.lk/co328/e16/Footwear-Shop-System/,https://github.com/cepdnaclk/e16-co328-Footwear-Shop-System,#,#,http://api.ce.pdn.ac.lk/projects/v1/co328/E16/Footwear-Shop-System/
123,Learning Management Systerm,No project page,CO328 Software Engineering Group Project. ,E16,Software Engineering Projects (CO328),https://projects.ce.pdn.ac.lk/co328/e16/Learning-Management-Systerm/,https://github.com/cepdnaclk/e16-co328-Learning-Management-Systerm,#,#,http://api.ce.pdn.ac.lk/projects/v1/co328/E16/Learning-Management-Systerm/
124,Movie Review System,"Projects | Department of Computer Engineering
 Projects - Department of Computer Engineering
 University of Peradeniya, Sri Lanka
 Project Title
 This is a sample image, to show how to add images to your page. To learn more options, please refer this
 Team
 eNumber, Name, email
 eNumber, Name, email
 eNumber, Name, email
 Table of Contents
 Introduction
 Other Sub Topics
 Links
 Introduction
 description of the real world problem and solution, impact
 Other Sub Topics
 …..
 Links
 Project Repository
 Project Page
 Department of Computer Engineering
 University of Peradeniya
 Back to top",,E16,Software Engineering Projects (CO328),https://projects.ce.pdn.ac.lk/co328/e16/Movie-Review-System/,https://github.com/cepdnaclk/e16-co328-Movie-Review-System,https://cepdnaclk.github.io/e16-co328-Movie-Review-System,https://cepdnaclk.github.io/e16-co328-Movie-Review-System/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/co328/E16/Movie-Review-System/
125,Pharmacy Management System,No project page,,E16,Software Engineering Projects (CO328),https://projects.ce.pdn.ac.lk/co328/e16/Pharmacy-Management-System/,https://github.com/cepdnaclk/e16-co328-Pharmacy-Management-System,#,#,http://api.ce.pdn.ac.lk/projects/v1/co328/E16/Pharmacy-Management-System/
126,Project Publication Platform,"Projects | Department of Computer Engineering
 Projects - Department of Computer Engineering
 University of Peradeniya, Sri Lanka
 Project Publication Platform
 Team
 E/16/078, Thilini Deshika, email
 E/16/168, Sudam Kalpage, email
 E/16/275, Hashan Eranga, email
 Table of Contents
 Introduction
 Other Sub Topics
 Links
 Introduction
 Website based platform for inventors and students to publish their projects and attract investment to continue their projects to the production stage.
 …..
 Links
 Project Repository
 Project Page
 Department of Computer Engineering
 University of Peradeniya
 Back to top",Website based platform for inventors and students to publish their projects and attract investment to continue their projects to the production stage.,E16,Software Engineering Projects (CO328),https://projects.ce.pdn.ac.lk/co328/e16/Project-Publication-Platform/,https://github.com/cepdnaclk/e16-co328-Project-Publication-Platform,https://cepdnaclk.github.io/e16-co328-Project-Publication-Platform,https://cepdnaclk.github.io/e16-co328-Project-Publication-Platform/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/co328/E16/Project-Publication-Platform/
127,Student Management System,"Student Management System | e16-co328-Student-Management-System
 e16-co328-Student-Management-System
 Student Management System
 Student App
 Desging diagrams
 Interaction perspective - use case diagram
 left - teachers app
 right - students app
 Structural perspective - class diagram
 left - teachers app
 right - students app
 Behavioral
 perspective - Activity diagram
 Databse design - ER diagram",,E16,Software Engineering Projects (CO328),https://projects.ce.pdn.ac.lk/co328/e16/Student-Management-System/,https://github.com/cepdnaclk/e16-co328-Student-Management-System,https://cepdnaclk.github.io/e16-co328-Student-Management-System,https://cepdnaclk.github.io/e16-co328-Student-Management-System/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/co328/E16/Student-Management-System/
128,mystore,No project page,,E16,Software Engineering Projects (CO328),https://projects.ce.pdn.ac.lk/co328/e16/mystore/,https://github.com/cepdnaclk/e16-co328-mystore,#,#,http://api.ce.pdn.ac.lk/projects/v1/co328/E16/mystore/
129,GPSARSA gaussian process state action reward state actios based SITNSHOP,No project page,An web application(+mobile application) to advertise any kinds of shops and more additional features,E14,Software Engineering Projects (CO328),https://projects.ce.pdn.ac.lk/co328/e14/GPSARSA-gaussian-process-state-action-reward-state-actios-based-SITNSHOP/,https://github.com/cepdnaclk/e14-co328-GPSARSA-gaussian-process-state-action-reward-state-actios-based-SITNSHOP,#,#,http://api.ce.pdn.ac.lk/projects/v1/co328/E14/GPSARSA-gaussian-process-state-action-reward-state-actios-based-SITNSHOP/
130,Zero Trash,"Projects | Department of Computer Engineering
 Projects - Department of Computer Engineering
 University of Peradeniya, Sri Lanka
 Project Zero Trash
 Team
 E/15/140, Jaliyagoda A.J.N.M., nuwanjaliyagoda@eng.pdn.ac.lk
 E/15/173, Karunarathne S.D.D.D, dinelkadilshani95@gmail.com
 E/15/350, Tennakoon T.M.P.B., pasan96tennakoon@gmail.com
 Table of Contents
 Introduction
 Solution Architecture
 Design
 Deployments
 Testing
 Links
 Introduction
 In today’s increasingly congested world, it is difficult to imagine the absence of waste. Waste generation levels are rising. In 2019 alone, the world’s cities produced 2 billion tons of solid waste. With increasing urbanization and population growth, annual waste generation is expected to grow by 70% in 2050.
 Although richer nations like the U.S.A and Japan produce more waste than countries like Sri Lanka, the problems of waste management are different in the developing world. Unlike developed nations, we do not have a well-organized means of controlling waste. Garbage is rarely collected on a regular basis, as municipalities are often underfunded. The lack of status and poor salaries associated with the job of garbage collection also creates a system where employees are not trained or able to manage an effective system.
 Zero Trash provides a communication platform between its users and garbage collectors which did not exist (at least not successfully) in Sri Lanka. This could have huge implications for both lessening the cost of collecting garbage and aiding in the recycling process, as well as ensuring that materials that would otherwise end up in a landfill are transported to the appropriate recycling centers. This will increase the income for garbage collectors while providing householders with an additional income.
 Solution Architecture
 Frontend Web Applications for Clients
 For Users and Collectors. Users should be able to sign in to it and place pickup requests for trash collections and maintain their contact information.
 RESTful API Server
 This will provide data and services to the Frontend application using HTTP GET and POST requests. Every request is authenticated using a bearer authentication token, which is issued to users when login into the system.
 Web Application for Management
 For management and monitoring purposes. System owners can sign in and see the progress of the system, accept and manage trash collector activities, communicate with customers, and see the summary of trash collected between certain periods.
 Design
 ER Diagram:
 A User Case Diagram:
 Deployments
 Frontend component is based on client side processing and it can be stored as a static web page on a server. React framework provides an option for build and develop the content using Development mode and after the component is ready for Deployment, it allows to collect all the HTML, JS and CSS files, bundle them and export the site as a package for Live Production.
 We used git with Github for development and it allows build workflows for automation of the project deployment. So we used the Workflow for React applications and once a pull request or commit happens to the master branch, it can automatically build the code for the production environment.
 Since we have not commercialized our project yet, we used the feature provided by GitHub for static web page hosts, named GitHub Pages. Our workflow can automatically upload the latest production build into the GitHub page we have created. This will help us to follow the DevOps practice known as CI/CD
 Testing
 Unit Testing was mainly focused on front end web application which is exposed to the customers. Unit tests were done using Jest and Enzyme libraries. Following areas were considered when writing unit tests
 Object Rendering
 API endpoint handling
 Styling
 Input validation with states
 Transferring of props
 Links
 Project Repository
 Project Report
 Department of Computer Engineering
 University of Peradeniya
 Back to top","Zero Trash provides a communication platform between its users and garbage collectors which did not exist (at least not successfully) in Sri Lanka. This could have huge implications for both lessening the cost of collecting garbage and aiding in the recycling process, as well as ensuring that materials that would otherwise end up in a landfill are transported to the appropriate recycling centers. This will increase the income for garbage collectors while providing householders with an additional income.",E15,Software Engineering Projects (CO328),https://projects.ce.pdn.ac.lk/co328/e15/Zero-Trash/,https://github.com/cepdnaclk/e15-co328-Zero-Trash,https://cepdnaclk.github.io/e15-co328-Zero-Trash,https://cepdnaclk.github.io/e15-co328-Zero-Trash/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/co328/E15/Zero-Trash/
131,Brain Tumor Detection,No project page,,E15,Image Processing Projects (CO543),https://projects.ce.pdn.ac.lk/co543/e15/Brain-Tumor-Detection/,https://github.com/cepdnaclk/e15-co543-Brain-Tumor-Detection,#,#,http://api.ce.pdn.ac.lk/projects/v1/co543/E15/Brain-Tumor-Detection/
132,Swarm Robot Monitoring System group A,No project page,"A swarm robotics systems consists of automation robots with local sensing and communication capabilities, lacking centralized control or access to global information, situated in a possibly unknown environment performing a collective action. Therefore, having centralized monitoring in addition to the distributed local sensing yields to a more robust system as a centralized monitoring system capable of performing global collective actions under a wide range of group sizes (scalability), despite the possible sudden loss of multiple agents (robustness), and under unknown and dynamic circumstances (flexibility).",E15,Image Processing Projects (CO543),https://projects.ce.pdn.ac.lk/co543/e15/Swarm-Robot-Monitoring-System-group-A/,https://github.com/cepdnaclk/e15-co543-Swarm-Robot-Monitoring-System-group-A,#,#,http://api.ce.pdn.ac.lk/projects/v1/co543/E15/Swarm-Robot-Monitoring-System-group-A/
133,Swarm Robot Monitoring System group B,No project page,"localization of robots, identifying  the boundary of arena and distance calculation with image processing(openCV python)",E15,Image Processing Projects (CO543),https://projects.ce.pdn.ac.lk/co543/e15/Swarm-Robot-Monitoring-System-group-B/,https://github.com/cepdnaclk/e15-co543-Swarm-Robot-Monitoring-System-group-B,#,#,http://api.ce.pdn.ac.lk/projects/v1/co543/E15/Swarm-Robot-Monitoring-System-group-B/
134,Swarm Robot Monitoring System group C,No project page,"Localization of multiple robots in real time, Video restoration at the presence of noise due to lighting conditions and degradation due to motion blur.",E15,Image Processing Projects (CO543),https://projects.ce.pdn.ac.lk/co543/e15/Swarm-Robot-Monitoring-System-group-C/,https://github.com/cepdnaclk/e15-co543-Swarm-Robot-Monitoring-System-group-C,#,#,http://api.ce.pdn.ac.lk/projects/v1/co543/E15/Swarm-Robot-Monitoring-System-group-C/
135,Pera Knowledge Portal,"Projects | Department of Computer Engineering
 Projects - Department of Computer Engineering
 University of Peradeniya, Sri Lanka
 Pera Knowledge Portal
 Team
 E/15/140, Jaliyagoda A.J.N.M., nuwanjaliyagoda@eng.pdn.ac.lk
 E/15/173, Karunarathne S.D.D.D, dinelkadilshani95@gmail.com
 E/15/350, Tennakoon T.M.P.B., pasan96tennakoon@gmail.com
 Introduction
 Our project aims to create a database of Research papers, Project Reports, Thesis’ done in University of Peradeniya. Since there is no current database where we keep the project reports other than the hard copies themselves, it is not possible for a person outside the university to access these reports. We created a platform where anyone in the university can upload their respective reports into a database which can be used by anyone; inside or outside the university, to access and read through. Anyone who is a member of the University of Peradeniya can use the CMS platform we have created to register on Pera Knowledge Portal. Members of our website can upload a pdf document of the documentation (Research Paper, Project Report, Thesis) they wish to be published. Knowledge Portal platform extracts the texts from pdf documents and indexed them in our database. Which gives the documentation to be searched even by the content of pdf documents. Anyone inside or outside the university can use the search engine we have provided to search these documents. Pera knowledge Portal search engine is able to produce accurate search results and give the most relatable documents according to the search text by the user.
 Links
 Project Repository
 Project Page
 Department of Computer Engineering
 University of Peradeniya
 Back to top","Pera Knowledge Portal is a web site that will allow visitors to do a full-text search on student project reports and thesis, developed using PHP and ElasticSearch",E15,Software Systems Projects,https://projects.ce.pdn.ac.lk/2yp/e15/Pera-Knowledge-Portal/,https://github.com/cepdnaclk/e15-2yp-Pera-Knowledge-Portal,https://cepdnaclk.github.io/e15-2yp-Pera-Knowledge-Portal,https://cepdnaclk.github.io/e15-2yp-Pera-Knowledge-Portal/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/2yp/E15/Pera-Knowledge-Portal/
136,finite element based structural non linear analysis,"finite-element-based-structural-non-linear-analysis | e15-2yp-finite-element-based-structural-non-linear-analysis
 e15-2yp-finite-element-based-structural-non-linear-analysis
 finite-element-based-structural-non-linear-analysis
 Introduction
 This is an attempt to implement a CLI tool for the following paper.
 M.C.M. Rajapakse, K.K. Wijesundara, R. Nascimbene, C.S. Bandara, R. Dissanayake, Accounting axial-moment-shear interaction for force-based fiber modeling of RC frames, Engineering Structures, Volume 184, 2019, Pages 15-36,ISSN 0141-0296,
 https://doi.org/10.1016/j.engstruct.2019.01.075.
 People: [Pubudu Premathilaka], [Suneth Samarasinghe]
 Advised by: Dr. Kushan Wijesundara and [Sameera Hippola)
 Project summary
 Civil engineering structure modelling
 Elementwise local stiffness matrix calculation generation.
 Structure’s global stiffnexx matrix calculation.
 Load matrix generation.
 Lieanr system oprations.
 Linear system representation : dense and sparse
 Linear system solving: Gauss ellimination on dense and sparse matrices, iterative numerical techniques.
 Backtracking to translate the linear system solution to civil engineering structure.
 The main objective of this project was to find the time and memroy efficient way of computing the results.
 Please note
 This work was done as a partial requirement for CO328 Software Engineering) course. **This implementation consists of the linear and non linear region analysis **",,E15,Software Systems Projects,https://projects.ce.pdn.ac.lk/2yp/e15/finite-element-based-structural-non-linear-analysis/,https://github.com/cepdnaclk/e15-2yp-finite-element-based-structural-non-linear-analysis,https://cepdnaclk.github.io/e15-2yp-finite-element-based-structural-non-linear-analysis,https://cepdnaclk.github.io/e15-2yp-finite-element-based-structural-non-linear-analysis/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/2yp/E15/finite-element-based-structural-non-linear-analysis/
137,dynamic background cancellation in videos,"Dynamic Background Cancellation | e14-2yp-dynamic-background-cancellation-in-videos
 e14-2yp-dynamic-background-cancellation-in-videos
 Dynamic Background Cancellation
 Dynamic background cancellation is a fundamental problem in video processing. Eventhough estimating the foreground is trivial in static background conditions, estimating the foreground can be tricky when the background is dynamic as well.
 In this project we try differrent approaches to detect the background and foreground in a video. We start with classical approaches and try to evaluate their pros and cons. Finally, we propose the most suitable combination of video processing operations to get the most accurate results.
 Algorithms
 PBAS Pixel based adaptive segmentation
 GMM Gaussian mixture model
 EM Expectatation maximization algorithm
 AGMM Adaptive gaussian mixture model
 FCMM Free Cylindrical mixture model
 AFCMM Adaptive free cylinder mixture model
 HVS Hierarchial video segmentation
 RPCA Robust Principle Component Analysis
 Morphological filtering
 SCC Strongly connected component analysis
 Technologies
 Python (Numpy, Scipy, Matplotlib)
 Matlab
 OpenCV
 People
 This project was done by Gihan Jayatilaka, Harshana Weligampola and Suren Sritharan as a course project for CO227 (Computer Engineering Project). The project was supervised by Dr. Dhammika Elkaduwe, Dr. Roshan Godaliyadda, Dr. Parakrama Ekanayeka and Dr. Vijitha Herath.
 Gihan, Harshana, Suren and Dr.Elkaduwa are from Department of Computer Engineering, Faculty of Engineering, University of Peradeniya. Dr.Godaliyadda, Dr.Ekanayeka, and Dr.Herath are from Department of Electrical and Electronics Engineering, Faculty of Engineering, University of Peradeniya
 Links
 Teambitecode.com page
 Report",Foreground estimation in dynamic background conditions using unsupervised learning techniques.,E14,Software Systems Projects,https://projects.ce.pdn.ac.lk/2yp/e14/dynamic-background-cancellation-in-videos/,https://github.com/cepdnaclk/e14-2yp-dynamic-background-cancellation-in-videos,https://cepdnaclk.github.io/e14-2yp-dynamic-background-cancellation-in-videos,https://cepdnaclk.github.io/e14-2yp-dynamic-background-cancellation-in-videos/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/2yp/E14/dynamic-background-cancellation-in-videos/
138,mabandit,No project page,A python based library which includes multi_arm_bandit and Bayesian_optimization_algorithms. The PYPI repository  can be found as mabandit 1.3,E14,Software Systems Projects,https://projects.ce.pdn.ac.lk/2yp/e14/mabandit/,https://github.com/cepdnaclk/e14-2yp-mabandit,#,#,http://api.ce.pdn.ac.lk/projects/v1/2yp/E14/mabandit/
139,packet forwarding simulator,No project page,Packet forwarding network simulator,E14,Software Systems Projects,https://projects.ce.pdn.ac.lk/2yp/e14/packet-forwarding-simulator/,https://github.com/cepdnaclk/e14-2yp-packet-forwarding-simulator,#,#,http://api.ce.pdn.ac.lk/projects/v1/2yp/E14/packet-forwarding-simulator/
140,RISCV Pipeline CPU Implimentation Group2,"RISC V Pipeline Processor
 Jun03
 Welcome to Our Risc V Pipeline Processor!
 Categories
 Memory systems and cache
 (1)
 Control Unit (4)
 Arithmatic and logic unit
 (1)
 Register file (4)
 Pipeline stages (1)
 Hazards (4)
 Calendar
 July 2021
 M
 T
 W
 T
 F
 S
 S
 « Jun
 Aug »
 1
 2
 3
 4
 5
 6
 7
 8
 9
 10
 11
 12
 13
 14
 15
 16
 17
 18
 19
 20
 21
 22
 23
 24
 25
 26
 27
 28
 29
 30
 31
 Instruction Implementation
 R Type
 S Type
 J Type
 B Type
 I Type
 U Type
 ©2011 Butterfly . All Rights Reserved.
 •
 Design by TEMPLATED   •
 Icons by FAMFAMFAM. Valid XHTML   •   Valid CSS",This is the RISC-V ISA implementation by Group 2,E16,Advanced Computer Architecture (CO502),https://projects.ce.pdn.ac.lk/co502/e16/RISCV-Pipeline-CPU-Implimentation-Group2/,https://github.com/cepdnaclk/e16-co502-RISCV-Pipeline-CPU-Implimentation-Group2,https://cepdnaclk.github.io/e16-co502-RISCV-Pipeline-CPU-Implimentation-Group2,https://cepdnaclk.github.io/e16-co502-RISCV-Pipeline-CPU-Implimentation-Group2/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/co502/E16/RISCV-Pipeline-CPU-Implimentation-Group2/
141,RISCV pipeline cpu implementation group04,"e16-co502-RISCV-pipeline-cpu-implementation-group04 | This is Advance Computer Architecture project of implementing Piplined Proccesor according to the RISC-V Instruction set
 e16-co502-RISCV-pipeline-cpu-implementation-group04
 RISCV Pipeline Proccesor Impementation
 This is a sample image, to show how to add images to your page. To learn more options, please refer this
 Team
 E/16/319, Vindula Rathnayke, email
 E/16/320, Subhash Rathnayke, email
 Table of Contents
 Introduction
 Pipeline Diagram
 Instruction Encoding System
 Links
 Introduction
 This is Advance Computer Architecture project of implementing Piplined Proccesor according to the 32bit
 RISC-V Instruction set. There containing all type of instructions.
 Pipeline Diagram with Datapath
 ### Control Signals
 Register Read Flag
 Register write Flag
 Memory to register Flag
 Memory write Flag
 Branch Flag
 ALU opcode
 Register destination Flag
 ALU source Flag
 Instruction Encoding System
 …..
 Links
 Project Repository
 Project Page
 Department of Computer Engineering
 University of Peradeniya",This is Advance Computer Architecture project of implementing Piplined Proccesor according to the RISC-V Instruction set,E16,Advanced Computer Architecture (CO502),https://projects.ce.pdn.ac.lk/co502/e16/RISCV-pipeline-cpu-implementation-group04/,https://github.com/cepdnaclk/e16-co502-RISCV-pipeline-cpu-implementation-group04,https://cepdnaclk.github.io/e16-co502-RISCV-pipeline-cpu-implementation-group04,https://cepdnaclk.github.io/e16-co502-RISCV-pipeline-cpu-implementation-group04/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/co502/E16/RISCV-pipeline-cpu-implementation-group04/
142,RV32IM NoC implementation,"Home - RV32IM Network on Chip Design and Implementation
 Home | RV32IM Network on Chip Design and Implementation
 Link
 Search
 Menu
 Expand
 Document
 HomeNodesCommunicationRoutersMain Memory and Memory Controller
 This site uses Just the Docs, a documentation theme for Jekyll.
 Project Repository on GitHub
 RV32IM Network on Chip Design and Implementation
 Table of Contents
 Introduction Pipeline Datapath Team Supervisors Links
 Introduction
 Under extended features we designed a Network of Chip to interconnect 16 RV32IM CPU instances using a mesh network. Mesh network is used by the CPU instances to communicate with other CPU instances and to access the main memory through the memory controllers. Figure 1 shows the basic design of the NoC. As shown in Figure 1, the main hardware components of the NoC,
 Nodes Routers Main memory Memory Controller
 GitHub Repository
 Overview of the Network on Chip
 Team
 E/16/069, Damsy De Silve, email E/16/094, Shirly Ekanayake, email E/16/276, Buddhi Perera, email
 Supervisors
 Dr. Isuru Navinna Dr. Mahanama Wickramasinghe
 Links
 Project Repository Project Page Department of Computer Engineering University of Peradeniya",,E16,Advanced Computer Architecture (CO502),https://projects.ce.pdn.ac.lk/co502/e16/RV32IM-NoC-implementation/,https://github.com/cepdnaclk/e16-co502-RV32IM-NoC-implementation,https://cepdnaclk.github.io/e16-co502-RV32IM-NoC-implementation,https://cepdnaclk.github.io/e16-co502-RV32IM-NoC-implementation/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/co502/E16/RV32IM-NoC-implementation/
143,RV32IM pipeline implementation group1,"Home - RV32IM Pipeline Implementation
 Home | RV32IM Pipeline Implementation
 Link
 Search
 Menu
 Expand
 Document
 HomeHardware UnitsControl Unit Control Signals Generated by the Control Unit
 Control Unit Design
 Instructions and the Control Signals ALURegister FileBranch and Jump Detection UnitImmediate Value Generation UnitProgram Counter RegisterMultiplexersProgram Counter Incrimeting AdderPipeline RegisterMemory HierarchyData CacheData MemoryInstruction CacheInstruction MemoryTiming and Simulation DelaysSimulation Delays of Hardware UnitsClock Cycle PeriodHazard Detection and Handling HazardsTypes of HazardsHandling Data HazardsForwarding‌ ‌unit‌ ‌HardwareHandling Control HazardsPipeline Datapath with Forwarding MechanismIntegrated CPUTesting
 This site uses Just the Docs, a documentation theme for Jekyll.
 Project Repository on GitHub
 RV32IM Pipeline Implementation
 Table of Contents
 Introduction Pipeline Datapath Team Supervisors Links
 Introduction
 The objective of this project was to design and implement a 5 stage pipeline CPU to support the RISC-V instruction architecture. This pipeline CPU supports the entire RV32IM ISA which contains 45 instructions. The designed pipeline CPU was implemented using behavioral modeling in verilogHDL and icarus Verilog was used compile and simulate. gtkWave was used to observe the behavior. GitHub Repository
 Pipeline Datapath
 Team
 E/16/069, Damsy De Silve, email E/16/094, Shirly Ekanayake, email E/16/276, Buddhi Perera, email
 Supervisors
 Dr. Isuru Navinna Dr. Mahanama Wickramasinghe
 Links
 Project Repository Project Page Department of Computer Engineering University of Peradeniya",The objective of this project was to design and implement a 5 stage pipeline CPU to support the RISC-V instruction architecture. This pipeline CPU supports the entire RV32IM ISA which contains 45 instructions. The designed pipeline CPU was implemented using behavioral modeling in verilogHDL and icarus Verilog was used compile and simulate. gtkWave was used to observe the behavior.,E16,Advanced Computer Architecture (CO502),https://projects.ce.pdn.ac.lk/co502/e16/RV32IM-pipeline-implementation-group1/,https://github.com/cepdnaclk/e16-co502-RV32IM-pipeline-implementation-group1,https://cepdnaclk.github.io/e16-co502-RV32IM-pipeline-implementation-group1,https://cepdnaclk.github.io/e16-co502-RV32IM-pipeline-implementation-group1/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/co502/E16/RV32IM-pipeline-implementation-group1/
144,RV32IM pipeline implementation group3,"RISC-V 32 bit CPU
 RISC-V 32 bit CPU
 Home
 Blog
 planning
 Posted On July 2nd, 2021 .
 Instructions and encoding.
 Datapath and control, hardware units and signals.
 Continue Reading →
 ALU and Register file
 Posted On July 3rd, 2021 .
 ALU
 Register file.
 Other hardware units.
 Continue Reading →
 control unit
 Posted On July 7th, 2021 .
 Analysis of the controll unit behaviour to different instructions
 Finalizing the convention of the multi-bit control signals outputs.
 Continue Reading →
 Intergration & Testing
 Posted On July 24th, 2021 .
 Intergrating all the units together.
 Hazzard handling is done with assembler for now.
 Continue Reading →
 Hazard Hanndling
 Posted On August 24th, 2021 .
 Implemented two fowarding units in stage3 and stage4.
 Implemented a flushing unit to flush the data in pipeline registers in stage1 and stage2
 Continue Reading →
 Phase2: Shared Memory MPSoC
 Posted On October 22th, 2021 .
 Implemented a Shared MPSoC system that uses a snoop bus and snoop controller to handle data sharing
 Implemented a chache with a state machine based on the MESI protocol
 Continue Reading →",CPU and memory system including cache memory implementation according to the RISC-V Instruction set,E16,Advanced Computer Architecture (CO502),https://projects.ce.pdn.ac.lk/co502/e16/RV32IM-pipeline-implementation-group3/,https://github.com/cepdnaclk/e16-co502-RV32IM-pipeline-implementation-group3,https://cepdnaclk.github.io/e16-co502-RV32IM-pipeline-implementation-group3,https://cepdnaclk.github.io/e16-co502-RV32IM-pipeline-implementation-group3/data/index.json,http://api.ce.pdn.ac.lk/projects/v1/co502/E16/RV32IM-pipeline-implementation-group3/
145,balancing an inverted pendulum using a neural network,No project page,CO542 Project,E14,Neural Networks Projects (CO542),https://projects.ce.pdn.ac.lk/co542/e14/balancing-an-inverted-pendulum-using-a-neural-network/,https://github.com/cepdnaclk/e14-co542-balancing-an-inverted-pendulum-using-a-neural-network,#,#,http://api.ce.pdn.ac.lk/projects/v1/co542/E14/balancing-an-inverted-pendulum-using-a-neural-network/
146,business to business database,No project page,"CO226 Project - Relational Database for B2B Online Trade Community - by: @Isuri-Devindi, @KavinduJayas, @Dr-Madhushan",E17,Database Projects (CO226),https://projects.ce.pdn.ac.lk/co226/e17/business-to-business-database/,https://github.com/cepdnaclk/e17-co226-business-to-business-database,#,#,http://api.ce.pdn.ac.lk/projects/v1/co226/E17/business-to-business-database/
